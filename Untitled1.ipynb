{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90659d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff17a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44653e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.700297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.482855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reading_time\n",
       "count    337.000000\n",
       "mean       9.700297\n",
       "std        5.482855\n",
       "min        2.000000\n",
       "25%        6.000000\n",
       "50%        8.000000\n",
       "75%       13.000000\n",
       "max       31.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44adfd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7d4aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots were the next big thing: what happened? – The Startup – Medium'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d975a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'claps', 'reading_time', 'link', 'title', 'text'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1facac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author          0\n",
       "claps           0\n",
       "reading_time    0\n",
       "link            0\n",
       "title           0\n",
       "text            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76108b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['title'] = df['title'].apply(lambda x: ''.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbc8a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots were the next big thing: what happened? – The Startup – Medium'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62761b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda x:[i.replace(\" \",\"_\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e491cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].apply(lambda x:[i.replace(\" \",\"_\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0553b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df['title'] + df['author'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ce2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df.tags.apply(lambda x:''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d0f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df.title.apply(lambda x:''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5dac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots_were_the_next_big_thing:_what_happened?_–_The_Startup_–_MediumJustin_Lee'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "    v = ' '.join([ps.stem(i) for i in text.split()])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f44645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots_were_the_next_big_thing:_what_happened?_–_The_Startup_–_MediumJustin_Lee'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['title'] = df['title'].apply(lambda x: ','.join(map(str, x)))\n",
    "df.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae0330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-77b86c153cd3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags'] = new_df.tags.apply(lambda x:x.lower())\n",
      "<ipython-input-18-77b86c153cd3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['title'] = new_df.title.apply(lambda x:x.lower())\n"
     ]
    }
   ],
   "source": [
    "new_df = df[['author','title','tags','text','link']]\n",
    "new_df['tags'] = new_df.tags.apply(lambda x:x.lower())\n",
    "new_df['title'] = new_df.title.apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad860a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['title'][0]\n",
    "#new_df['title'] = np.asarray(new_df['title'])\n",
    "#new_df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ff7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      chatbots_were_the_next_big_thing:_what_happene...\n",
       "1      python_for_data_science:_8_concepts_you_may_ha...\n",
       "2      automated_feature_engineering_in_python_–_towa...\n",
       "3      machine_learning:_how_to_go_from_zero_to_hero_...\n",
       "4      reinforcement_learning_from_scratch_–_insight_...\n",
       "                             ...                        \n",
       "332    you_can_build_a_neural_network_in_javascript_e...\n",
       "333    artificial_intelligence,_ai_in_2018_and_beyond...\n",
       "334    spiking_neural_networks,_the_next_generation_o...\n",
       "335    surprise!_neurons_are_now_more_complex_than_we...\n",
       "336    “wth_does_a_neural_network_even_learn??”_—_a_n...\n",
       "Name: tags, Length: 337, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502997e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(max_features = 5000, stop_words = 'english')\n",
    "vectors = cvect.fit_transform(new_df['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac0a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45a4fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend(article):\n",
    "    article = article.lower()\n",
    "    article_index = new_df[new_df.title == article].index[0]\n",
    "    distances = similarity[article_index]\n",
    "    article_list = sorted(list(enumerate(distances)), reverse = True, key = lambda x:x[1])[1:8]\n",
    "    for i in article_list:\n",
    "        print(new_df.iloc[i[0]].title)\n",
    "    return article_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b18929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'building_a_smarter_home_feed_–_pinterest_engineering_–_medium'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['title'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6abeef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intuitively_understanding_convolutions_for_deep_learning\n",
      "intuitively_understanding_convolutions_for_deep_learning\n",
      "chatbots_were_the_next_big_thing:_what_happened?_–_the_startup_–_medium\n",
      "python_for_data_science:_8_concepts_you_may_have_forgotten\n",
      "automated_feature_engineering_in_python_–_towards_data_science\n",
      "machine_learning:_how_to_go_from_zero_to_hero_–_freecodecamp\n",
      "reinforcement_learning_from_scratch_–_insight_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(89, 1.0), (158, 1.0), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=recommend('intuitively_understanding_convolutions_for_deep_learning')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59d2d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    \n",
    "    article = file_name.split(\"\\n\")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(s1, s2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in s1]\n",
    "    sent2 = [w.lower() for w in s2]\n",
    " \n",
    "    all_words = list(set(s1 + s2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "  \n",
    "    for w in s1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "  \n",
    "    for w in s2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "   \n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: \n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "   \n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "   \n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    \n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "   \n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    \n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c26acb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Pinchak | Pinterest engineer, Discovery\n",
      "The home feed should be a reflection of what each user cares about. Content is sourced from inputs such as people and boards the user follows, interests, and recommendations. To ensure we maintain fast, reliable and personalized home feeds, we built the smart feed with the following design values in mind:\n",
      "1. Different sources of Pins should be mixed together at different rates.\n",
      "2. Some Pins should be selectively dropped or deferred until a later time. Some sources may produce Pins of poor quality for a user, so instead of showing everything available immediately, we can be selective about what to show and what to hold back for a future session.\n",
      "3. Pins should be arranged in the order of best-first rather than newest-first. For some sources, newer Pins are intuitively better, while for others, newness is less important.\n",
      "We shifted away from our previously time-ordered home feed system and onto a more flexible one. The core feature of the smart feed architecture is its separation of available, but unseen, content and content that’s already been presented to the user. We leverage knowledge of what the user hasn’t yet seen to our advantage when deciding how the feed evolves over time.\n",
      "Smart feed is a composition of three independent services, each of which has a specific role in the construction of a home feed.\n",
      "The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.\n",
      "Essentially, the worker manages Pins as they become newly available, such as those from the repins of the people the user follows. Pins have varying value to the receiving user, so the worker is tasked with deciding the magnitude of their subjective quality.\n",
      "Incoming Pins are currently obtained from three separate sources: repins made by followed users, related Pins, and Pins from followed interests. Each is scored by the worker and then inserted into a pool for that particular type of pin. Each pool is a priority queue sorted on score and belongs to a single user. Newly added Pins mix with those added before, allowing the highest quality Pins to be accessible over time at the front of the queue.\n",
      "Pools can be implemented in a variety of ways so long as the priority queue requirement is met. We choose to do this by exploiting the key-based sorting of HBase. Each key is a combination of user, score and Pin such that, for any user, we may scan a list of available Pins according to their score. Newly added triples will be inserted at their appropriate location to maintain the score order. This combination of user, score, and Pin into a key value can be used to create a priority queue in other storage systems aside from HBase, a property we may use in the future depending on evolving storage requirements.\n",
      "Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.\n",
      "The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.\n",
      "The content generator is generally free to perform any rearrangements it likes, but is bound to the priority queue nature of the pools. When the generator asks for n pins from a pool, it’ll get the n highest scoring (i.e., best) Pins available. Therefore, the generator doesn’t need to concern itself with finding the best available content, but instead with how the best available content should be presented.\n",
      "In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.\n",
      "The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.\n",
      "The smart feed service relies on the content generator to provide new Pins. If the generator experiences a degradation in performance, the service can gracefully handle the loss of its availability. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. In this instance, the feed will appear to the end user as unchanged from last time. Future feed views will produce chunks as large as, or larger than, the last so that eventually the user will see new Pins.\n",
      "By moving to smart feed, we achieved the goals of a highly flexible architecture and better control over the composition of home feeds. The home feed is now powered by three separate services, each with a well-defined role in its production and distribution. The individual services can be altered or replaced with components that serve the same general purpose. The use of pools to buffer Pins according to their quality allows us a greater amount of control over the composition of home feeds.\n",
      "Continuing with this project, we intend to better model users’ preferences with respect to Pins in their home feeds. Our accuracy of recommendation quality varies considerably over our user base, and we would benefit from using preference information gathered from recent interactions with the home feed. Knowledge of personal preference will also help us order home feeds so the Pins of most value can be discovered with the least amount of effort.\n",
      "If you’re interested in tackling challenges and making improvements like this, join our team!\n",
      "Chris Pinchak is a software engineer at Pinterest.\n",
      "Acknowledgements: This technology was built in collaboration with Dan Feng, Dmitry Chechik, Raghavendra Prabhu, Jeremy Carroll, Xun Liu, Varun Sharma, Joe Lau, Yuchen Liu, Tian-Ying Chang, and Yun Park. This team, as well as people from across the company, helped make this project a reality with their technical insights and invaluable feedback.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering\n",
      "\n",
      "Indexes of top ranked_sentence order are  [(0.07035587589313758, ['Distinct', 'from', 'the', 'smart', 'feed', 'worker,', 'the', 'smart', 'feed', 'content', 'generator', 'is', 'concerned', 'primarily', 'with', 'defining', 'what', '“new”', 'means', 'in', 'the', 'context', 'of', 'a', 'home', 'feed.', 'When', 'a', 'user', 'accesses', 'the', 'home', 'feed,', 'we', 'ask', 'the', 'content', 'generator', 'for', 'new', 'Pins', 'since', 'their', 'last', 'visit.', 'The', 'generator', 'decides', 'the', 'quantity,', 'composition,', 'and', 'arrangement', 'of', 'new', 'Pins', 'to', 'return', 'in', 'response', 'to', 'this', 'request.']), (0.06803332617614993, ['In', 'addition', 'to', 'providing', 'high', 'availability', 'of', 'the', 'home', 'feed,', 'the', 'smart', 'feed', 'service', 'is', 'responsible', 'for', 'combining', 'new', 'Pins', 'returned', 'by', 'the', 'content', 'generator', 'with', 'those', 'that', 'previously', 'appeared', 'in', 'the', 'home', 'feed.', 'We', 'can', 'separate', 'these', 'into', 'the', 'chunk', 'returned', 'by', 'the', 'content', 'generator', 'and', 'the', 'materialized', 'feed', 'managed', 'by', 'the', 'smart', 'feed', 'service.']), (0.06585684921566548, ['The', 'smart', 'feed', 'worker', 'is', 'the', 'first', 'to', 'process', 'Pins', 'and', 'has', 'two', 'primary', 'responsibilities', '—', 'to', 'accept', 'incoming', 'Pins', 'and', 'assign', 'some', 'score', 'proportional', 'to', 'their', 'quality', 'or', 'value', 'to', 'the', 'receiving', 'user,', 'and', 'to', 'remember', 'these', 'scored', 'Pins', 'in', 'some', 'storage', 'for', 'later', 'consumption.']), (0.06047125282541027, ['The', 'content', 'generator', 'assembles', 'available', 'Pins', 'into', 'chunks', 'for', 'consumption', 'by', 'the', 'user', 'as', 'part', 'of', 'their', 'home', 'feed.', 'The', 'generator', 'is', 'free', 'to', 'choose', 'any', 'arrangement', 'based', 'on', 'a', 'variety', 'of', 'input', 'signals,', 'and', 'may', 'elect', 'to', 'use', 'some', 'or', 'all', 'of', 'the', 'Pins', 'available', 'in', 'the', 'pools.', 'Pins', 'that', 'are', 'selected', 'for', 'inclusion', 'in', 'a', 'chunk', 'are', 'thereafter', 'removed', 'from', 'from', 'the', 'pools', 'so', 'they', 'cannot', 'be', 'returned', 'as', 'part', 'of', 'subsequent', 'chunks.']), (0.05584349930996346, ['The', 'materialized', 'feed', 'represents', 'a', 'frozen', 'view', 'of', 'the', 'feed', 'as', 'it', 'was', 'the', 'last', 'time', 'the', 'user', 'viewed', 'it.', 'To', 'the', 'materialized', 'Pins', 'we', 'add', 'the', 'Pins', 'from', 'the', 'content', 'generator', 'in', 'the', 'chunk.', 'The', 'service', 'makes', 'no', 'decisions', 'about', 'order,', 'instead', 'it', 'adds', 'the', 'Pins', 'in', 'exactly', 'the', 'order', 'given', 'by', 'the', 'chunk.', 'Because', 'it', 'has', 'a', 'fairly', 'low', 'rate', 'of', 'reading', 'and', 'writing,', 'the', 'materialized', 'feed', 'is', 'likely', 'to', 'suffer', 'from', 'fewer', 'availability', 'events.', 'In', 'addition,', 'feeds', 'can', 'be', 'trimmed', 'to', 'restrict', 'them', 'to', 'a', 'maximum', 'size.', 'The', 'need', 'for', 'less', 'storage', 'means', 'we', 'can', 'easily', 'increase', 'the', 'availability', 'and', 'reliability', 'of', 'the', 'materialized', 'feed', 'through', 'replication', 'and', 'the', 'use', 'of', 'faster', 'storage', 'hardware.']), (0.04943495385656368, ['The', 'smart', 'feed', 'service', 'relies', 'on', 'the', 'content', 'generator', 'to', 'provide', 'new', 'Pins.', 'If', 'the', 'generator', 'experiences', 'a', 'degradation', 'in', 'performance,', 'the', 'service', 'can', 'gracefully', 'handle', 'the', 'loss', 'of', 'its', 'availability.', 'In', 'the', 'event', 'the', 'content', 'generator', 'encounters', 'an', 'exception', 'while', 'generating', 'a', 'chunk,', 'or', 'if', 'it', 'simply', 'takes', 'too', 'long', 'to', 'produce', 'one,', 'the', 'smart', 'feed', 'service', 'will', 'return', 'the', 'content', 'contained', 'in', 'the', 'materialized', 'feed.', 'In', 'this', 'instance,', 'the', 'feed', 'will', 'appear', 'to', 'the', 'end', 'user', 'as', 'unchanged', 'from', 'last', 'time.', 'Future', 'feed', 'views', 'will', 'produce', 'chunks', 'as', 'large', 'as,', 'or', 'larger', 'than,', 'the', 'last', 'so', 'that', 'eventually', 'the', 'user', 'will', 'see', 'new', 'Pins.']), (0.04929100220823563, ['Chris', 'Pinchak', '|', 'Pinterest', 'engineer,', 'Discovery']), (0.04884016385389834, ['2.', 'Some', 'Pins', 'should', 'be', 'selectively', 'dropped', 'or', 'deferred', 'until', 'a', 'later', 'time.', 'Some', 'sources', 'may', 'produce', 'Pins', 'of', 'poor', 'quality', 'for', 'a', 'user,', 'so', 'instead', 'of', 'showing', 'everything', 'available', 'immediately,', 'we', 'can', 'be', 'selective', 'about', 'what', 'to', 'show', 'and', 'what', 'to', 'hold', 'back', 'for', 'a', 'future', 'session.']), (0.0475486232162542, ['Incoming', 'Pins', 'are', 'currently', 'obtained', 'from', 'three', 'separate', 'sources:', 'repins', 'made', 'by', 'followed', 'users,', 'related', 'Pins,', 'and', 'Pins', 'from', 'followed', 'interests.', 'Each', 'is', 'scored', 'by', 'the', 'worker', 'and', 'then', 'inserted', 'into', 'a', 'pool', 'for', 'that', 'particular', 'type', 'of', 'pin.', 'Each', 'pool', 'is', 'a', 'priority', 'queue', 'sorted', 'on', 'score', 'and', 'belongs', 'to', 'a', 'single', 'user.', 'Newly', 'added', 'Pins', 'mix', 'with', 'those', 'added', 'before,', 'allowing', 'the', 'highest', 'quality', 'Pins', 'to', 'be', 'accessible', 'over', 'time', 'at', 'the', 'front', 'of', 'the', 'queue.']), (0.047333905642470746, ['The', 'home', 'feed', 'should', 'be', 'a', 'reflection', 'of', 'what', 'each', 'user', 'cares', 'about.', 'Content', 'is', 'sourced', 'from', 'inputs', 'such', 'as', 'people', 'and', 'boards', 'the', 'user', 'follows,', 'interests,', 'and', 'recommendations.', 'To', 'ensure', 'we', 'maintain', 'fast,', 'reliable', 'and', 'personalized', 'home', 'feeds,', 'we', 'built', 'the', 'smart', 'feed', 'with', 'the', 'following', 'design', 'values', 'in', 'mind:']), (0.04638295646649186, ['Essentially,', 'the', 'worker', 'manages', 'Pins', 'as', 'they', 'become', 'newly', 'available,', 'such', 'as', 'those', 'from', 'the', 'repins', 'of', 'the', 'people', 'the', 'user', 'follows.', 'Pins', 'have', 'varying', 'value', 'to', 'the', 'receiving', 'user,', 'so', 'the', 'worker', 'is', 'tasked', 'with', 'deciding', 'the', 'magnitude', 'of', 'their', 'subjective', 'quality.']), (0.045744260820342206, ['Continuing', 'with', 'this', 'project,', 'we', 'intend', 'to', 'better', 'model', 'users’', 'preferences', 'with', 'respect', 'to', 'Pins', 'in', 'their', 'home', 'feeds.', 'Our', 'accuracy', 'of', 'recommendation', 'quality', 'varies', 'considerably', 'over', 'our', 'user', 'base,', 'and', 'we', 'would', 'benefit', 'from', 'using', 'preference', 'information', 'gathered', 'from', 'recent', 'interactions', 'with', 'the', 'home', 'feed.', 'Knowledge', 'of', 'personal', 'preference', 'will', 'also', 'help', 'us', 'order', 'home', 'feeds', 'so', 'the', 'Pins', 'of', 'most', 'value', 'can', 'be', 'discovered', 'with', 'the', 'least', 'amount', 'of', 'effort.']), (0.04451904237520356, ['By', 'moving', 'to', 'smart', 'feed,', 'we', 'achieved', 'the', 'goals', 'of', 'a', 'highly', 'flexible', 'architecture', 'and', 'better', 'control', 'over', 'the', 'composition', 'of', 'home', 'feeds.', 'The', 'home', 'feed', 'is', 'now', 'powered', 'by', 'three', 'separate', 'services,', 'each', 'with', 'a', 'well-defined', 'role', 'in', 'its', 'production', 'and', 'distribution.', 'The', 'individual', 'services', 'can', 'be', 'altered', 'or', 'replaced', 'with', 'components', 'that', 'serve', 'the', 'same', 'general', 'purpose.', 'The', 'use', 'of', 'pools', 'to', 'buffer', 'Pins', 'according', 'to', 'their', 'quality', 'allows', 'us', 'a', 'greater', 'amount', 'of', 'control', 'over', 'the', 'composition', 'of', 'home', 'feeds.']), (0.044403984725042873, ['We', 'shifted', 'away', 'from', 'our', 'previously', 'time-ordered', 'home', 'feed', 'system', 'and', 'onto', 'a', 'more', 'flexible', 'one.', 'The', 'core', 'feature', 'of', 'the', 'smart', 'feed', 'architecture', 'is', 'its', 'separation', 'of', 'available,', 'but', 'unseen,', 'content', 'and', 'content', 'that’s', 'already', 'been', 'presented', 'to', 'the', 'user.', 'We', 'leverage', 'knowledge', 'of', 'what', 'the', 'user', 'hasn’t', 'yet', 'seen', 'to', 'our', 'advantage', 'when', 'deciding', 'how', 'the', 'feed', 'evolves', 'over', 'time.']), (0.044129651772061074, ['The', 'content', 'generator', 'is', 'generally', 'free', 'to', 'perform', 'any', 'rearrangements', 'it', 'likes,', 'but', 'is', 'bound', 'to', 'the', 'priority', 'queue', 'nature', 'of', 'the', 'pools.', 'When', 'the', 'generator', 'asks', 'for', 'n', 'pins', 'from', 'a', 'pool,', 'it’ll', 'get', 'the', 'n', 'highest', 'scoring', '(i.e.,', 'best)', 'Pins', 'available.', 'Therefore,', 'the', 'generator', 'doesn’t', 'need', 'to', 'concern', 'itself', 'with', 'finding', 'the', 'best', 'available', 'content,', 'but', 'instead', 'with', 'how', 'the', 'best', 'available', 'content', 'should', 'be', 'presented.']), (0.039712569836571635, ['Smart', 'feed', 'is', 'a', 'composition', 'of', 'three', 'independent', 'services,', 'each', 'of', 'which', 'has', 'a', 'specific', 'role', 'in', 'the', 'construction', 'of', 'a', 'home', 'feed.']), (0.03816094637008598, ['Chris', 'Pinchak', 'is', 'a', 'software', 'engineer', 'at', 'Pinterest.']), (0.03753909409687661, ['3.', 'Pins', 'should', 'be', 'arranged', 'in', 'the', 'order', 'of', 'best-first', 'rather', 'than', 'newest-first.', 'For', 'some', 'sources,', 'newer', 'Pins', 'are', 'intuitively', 'better,', 'while', 'for', 'others,', 'newness', 'is', 'less', 'important.']), (0.028717219526289924, ['Pools', 'can', 'be', 'implemented', 'in', 'a', 'variety', 'of', 'ways', 'so', 'long', 'as', 'the', 'priority', 'queue', 'requirement', 'is', 'met.', 'We', 'choose', 'to', 'do', 'this', 'by', 'exploiting', 'the', 'key-based', 'sorting', 'of', 'HBase.', 'Each', 'key', 'is', 'a', 'combination', 'of', 'user,', 'score', 'and', 'Pin', 'such', 'that,', 'for', 'any', 'user,', 'we', 'may', 'scan', 'a', 'list', 'of', 'available', 'Pins', 'according', 'to', 'their', 'score.', 'Newly', 'added', 'triples', 'will', 'be', 'inserted', 'at', 'their', 'appropriate', 'location', 'to', 'maintain', 'the', 'score', 'order.', 'This', 'combination', 'of', 'user,', 'score,', 'and', 'Pin', 'into', 'a', 'key', 'value', 'can', 'be', 'used', 'to', 'create', 'a', 'priority', 'queue', 'in', 'other', 'storage', 'systems', 'aside', 'from', 'HBase,', 'a', 'property', 'we', 'may', 'use', 'in', 'the', 'future', 'depending', 'on', 'evolving', 'storage', 'requirements.']), (0.02824665173615034, ['1.', 'Different', 'sources', 'of', 'Pins', 'should', 'be', 'mixed', 'together', 'at', 'different', 'rates.']), (0.017503993473322897, ['Inventive', 'engineers', 'building', 'the', 'first', 'visual', 'discovery', 'engine,', '100', 'billion', 'ideas', 'and', 'counting.', 'https://careers.pinterest.com/careers/engineering']), (0.007847593876347553, ['Acknowledgements:', 'This', 'technology', 'was', 'built', 'in', 'collaboration', 'with', 'Dan', 'Feng,', 'Dmitry', 'Chechik,', 'Raghavendra', 'Prabhu,', 'Jeremy', 'Carroll,', 'Xun', 'Liu,', 'Varun', 'Sharma,', 'Joe', 'Lau,', 'Yuchen', 'Liu,', 'Tian-Ying', 'Chang,', 'and', 'Yun', 'Park.', 'This', 'team,', 'as', 'well', 'as', 'people', 'from', 'across', 'the', 'company,', 'helped', 'make', 'this', 'project', 'a', 'reality', 'with', 'their', 'technical', 'insights', 'and', 'invaluable', 'feedback.']), (0.007603101085995281, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.0064794816414686825, ['If', 'you’re', 'interested', 'in', 'tackling', 'challenges', 'and', 'making', 'improvements', 'like', 'this,', 'join', 'our', 'team!'])]\n",
      "Summarize Text: \n",
      " Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.. In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.. The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.. The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.. The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.\n",
      "chatbots_were_the_next_big_thing:_what_happened?_–_the_startup_–_medium\n",
      "automated_feature_engineering_in_python_–_towards_data_science\n",
      "machine_learning:_how_to_go_from_zero_to_hero_–_freecodecamp\n",
      "reinforcement_learning_from_scratch_–_insight_data\n",
      "intuitively_understanding_convolutions_for_deep_learning\n",
      "an_intro_to_machine_learning_for_designers_–_ux_collective\n",
      "the_big_list_of_ds/ml_interview_resources_–_towards_data_science\n",
      "Oh, how the headlines blared:\n",
      "Chatbots were The Next Big Thing.\n",
      "Our hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.\n",
      "And why wouldn’t they be? All the road signs pointed towards insane success.\n",
      "At the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an ‘overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots’.\n",
      "In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:\n",
      "One year on, we have an answer to that question.\n",
      "No.\n",
      "Because there isn’t even an ecosystem for a platform to dominate.\n",
      "Chatbots weren’t the first technological development to be talked up in grandiose terms and then slump spectacularly.\n",
      "The age-old hype cycle unfolded in familiar fashion...\n",
      "Expectations built, built, and then..... It all kind of fizzled out.\n",
      "The predicted paradim shift didn’t materialize.\n",
      "And apps are, tellingly, still alive and well.\n",
      "We look back at our breathless optimism and turn to each other, slightly baffled:\n",
      "“is that it? THAT was the chatbot revolution we were promised?”\n",
      "Digit’s Ethan Bloch sums up the general consensus:\n",
      "According to Dave Feldman, Vice President of Product Design at Heap, chatbots didn’t just take on one difficult problem and fail: they took on several and failed all of them.\n",
      "Bots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.\n",
      "Users had to type commands manually into a machine to get anything done.\n",
      "Then, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!\n",
      "Meanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.\n",
      "Another bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:\n",
      "The next item on the agenda was holding a two-way dialog with a machine. Here’s an example dialog (dating back to the 1990s) with VCR setup system:\n",
      "Pretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.\n",
      "It was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.\n",
      "Modern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.\n",
      "Basically, we’re still trying to achieve the same innovations we were 30 years ago.\n",
      "Here’s where I think we’re going wrong:\n",
      "An oversized assumption has been that apps are ‘over’, and would be replaced by bots.\n",
      "By pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.\n",
      "You might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?\n",
      "It’s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No — not yet, at least.\n",
      "Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.\n",
      "Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.\n",
      "A great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, there’s no competition.\n",
      "That’s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.\n",
      "Modern-day apps benefit from decades of research and experimentation. Why would we throw this away?\n",
      "But, if we swap the word ‘replace’ with ‘extend’, things get much more interesting.\n",
      "Today’s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.\n",
      "The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.\n",
      "Another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.\n",
      "For plenty of companies, bots just aren’t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they aren’t needed.\n",
      "Building a bot for the sake of it, letting it loose and hoping for the best will never end well:\n",
      "The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input.\n",
      "The advantage of this approach is that it’s pretty easy to list all the cases that they are designed to cover. And that’s precisely their disadvantage, too.\n",
      "That’s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.\n",
      "Problems arise when life refuses to fit into those boxes.\n",
      "According to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.\n",
      "When we were building GrowthBot, we decided to make it specific to sales and marketers: not an ‘all-rounder’, despite the temptation to get overexcited about potential capabilties.\n",
      "Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.\n",
      "A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.\n",
      "In an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.\n",
      "Some platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)\n",
      "As Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.\n",
      "And conversations are complex. They’re not linear. Topics spin around each other, take random turns, restart or abruptly finish.\n",
      "Today’s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isn’t available yet.\n",
      "And in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:\n",
      "Once upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate information\n",
      "There’s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, it’s easier and faster to click than it is to type.\n",
      "Tapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.\n",
      "We love optical displays of information because we are highly visual creatures. It’s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.\n",
      "Conversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, we’re swapping something simple for a more-complex alternative.\n",
      "Sure, there are some concepts that we can only express using language (“show me all the ways of getting to a museum that give me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.\n",
      "Aiming for a human dimension in business interactions makes sense.\n",
      "If there’s one thing that’s broken about sales and marketing, it’s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated ‘contact us’ forms.\n",
      "Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.\n",
      "A conversation encompasses so much more than just text.\n",
      "Humans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what they’re talking about, meaning it’s a bit like conversing with someone who has little or no short-term memory.\n",
      "As HubSpot team pinpointed:\n",
      "People aren’t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that you’re lying to your users).\n",
      "And even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.\n",
      "And here’s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans.\n",
      "But is that how humans prefer to interact with machines?\n",
      "Not necessarily.\n",
      "At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.\n",
      "In a way, those early-adopters weren’t entirely wrong.\n",
      "People are yelling at Google Home to play their favorite song, ordering pizza from the Domino’s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots haven’t lived up to the hype generated circa 2015/16.\n",
      "Not even close.\n",
      "Computers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.\n",
      "Computers aren’t good at understanding human emotion. The state of NLP means they still don’t ‘get’ what we’re asking them, never mind how we feel.\n",
      "That’s why it’s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.\n",
      "For now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.\n",
      "But that’s not the whole story.\n",
      "Yes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.\n",
      "As Bill Gates once said:\n",
      "The hype is over. And that’s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.\n",
      "I believe we’re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.\n",
      "Messaging will continue to gain traction. Chatbots aren’t going away. NLP and AI are becoming more sophisticated every day.\n",
      "Developers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.\n",
      "And I can’t wait to see what happens next.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Head of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot\n",
      "Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.029607798673486337, ['A', 'competent', 'developer', 'can', 'build', 'a', 'basic', 'bot', 'in', 'minutes', '—', 'but', 'one', 'that', 'can', 'hold', 'a', 'conversation?', 'That’s', 'another', 'story.', 'Despite', 'the', 'constant', 'hype', 'around', 'AI,', 'we’re', 'still', 'a', 'long', 'way', 'from', 'achieving', 'anything', 'remotely', 'human-like.']), (0.023833586394445196, ['Facebook’s', 'goal', 'is', 'that', 'their', 'bots', 'should', 'pass', 'the', 'so-called', 'Turing', 'Test,', 'meaning', 'you', 'can’t', 'tell', 'whether', 'you', 'are', 'talking', 'to', 'a', 'bot', 'or', 'a', 'human.', 'But', 'a', 'bot', 'isn’t', 'the', 'same', 'as', 'a', 'human.', 'It', 'never', 'will', 'be.']), (0.02232835645469477, ['Whether', 'they’re', '‘better’', 'is', 'subjective,', 'but', 'I', 'think', 'it’s', 'fair', 'to', 'say', 'that', 'today’s', 'best', 'bot', 'isn’t', 'comparable', 'to', 'today’s', 'best', 'app.']), (0.021356986452058385, ['Plus,', 'nobody', 'thinks', 'that', 'using', 'Lyft', 'is', 'too', 'complicated,', 'or', 'that', 'it’s', 'too', 'hard', 'to', 'order', 'food', 'or', 'buy', 'a', 'dress', 'on', 'an', 'app.', 'What', 'is', 'too', 'complicated', 'is', 'trying', 'to', 'complete', 'these', 'tasks', 'with', 'a', 'bot', '—', 'and', 'having', 'the', 'bot', 'fail.']), (0.021116527104886432, ['In', 'fact,', 'the', 'only', 'significant', 'question', 'around', 'chatbots', 'was', 'who', 'would', 'monopolize', 'the', 'field,', 'not', 'whether', 'chatbots', 'would', 'take', 'off', 'in', 'the', 'first', 'place:']), (0.02110429997976109, ['Remember:', 'a', 'bot', 'that', 'does', 'ONE', 'thing', 'well', 'is', 'infinitely', 'more', 'helpful', 'than', 'a', 'bot', 'that', 'does', 'multiple', 'things', 'poorly.']), (0.018561065241709973, ['And', 'here’s', 'the', 'other', 'thing.', 'Conversational', 'UIs', 'are', 'built', 'to', 'replicate', 'the', 'way', 'humans', 'prefer', 'to', 'communicate', '—', 'with', 'other', 'humans.']), (0.01792922261953376, ['Computers', 'aren’t', 'good', 'at', 'understanding', 'human', 'emotion.', 'The', 'state', 'of', 'NLP', 'means', 'they', 'still', 'don’t', '‘get’', 'what', 'we’re', 'asking', 'them,', 'never', 'mind', 'how', 'we', 'feel.']), (0.017424956948296124, ['Chatbots', 'were', 'The', 'Next', 'Big', 'Thing.']), (0.01738792704447834, ['According', 'to', 'recent', 'reports,', '70%', 'of', 'the', '100,000+', 'bots', 'on', 'Facebook', 'Messenger', 'are', 'failing', 'to', 'fulfil', 'simple', 'user', 'requests.', 'This', 'is', 'partly', 'a', 'result', 'of', 'developers', 'failing', 'to', 'narrow', 'their', 'bot', 'down', 'to', 'one', 'strong', 'area', 'of', 'focus.']), (0.017197944901772743, ['For', 'plenty', 'of', 'companies,', 'bots', 'just', 'aren’t', 'the', 'right', 'solution.', 'The', 'past', 'two', 'years', 'are', 'littered', 'with', 'cases', 'of', 'bots', 'being', 'blindly', 'applied', 'to', 'problems', 'where', 'they', 'aren’t', 'needed.']), (0.016834534317807685, ['By', 'pitting', 'two', 'such', 'disparate', 'concepts', 'against', 'one', 'another', '(instead', 'of', 'seeing', 'them', 'as', 'separate', 'entities', 'designed', 'to', 'serve', 'different', 'purposes)', 'we', 'discouraged', 'bot', 'development.']), (0.01667202668937311, ['Once', 'upon', 'a', 'time,', 'the', 'only', 'way', 'to', 'interact', 'with', 'computers', 'was', 'by', 'typing', 'arcane', 'commands', 'to', 'the', 'terminal.', 'Visual', 'interfaces', 'using', 'windows,', 'icons', 'or', 'a', 'mouse', 'were', 'a', 'revolution', 'in', 'how', 'we', 'manipulate', 'information']), (0.01598569470041649, ['The', 'next', 'wave', 'will', 'be', 'multimodal', 'apps,', 'where', 'you', 'can', 'say', 'what', 'you', 'want', '(like', 'with', 'Siri)', 'and', 'get', 'back', 'information', 'as', 'a', 'map,', 'text,', 'or', 'even', 'a', 'spoken', 'response.']), (0.01575031115113536, ['Conversational', 'UIs', 'are', 'meant', 'to', 'replicate', 'the', 'way', 'humans', 'prefer', 'to', 'communicate,', 'but', 'they', 'end', 'up', 'requiring', 'extra', 'cognitive', 'effort.', 'Essentially,', 'we’re', 'swapping', 'something', 'simple', 'for', 'a', 'more-complex', 'alternative.']), (0.015729612801704203, ['That’s', 'why', 'it’s', 'still', 'impossible', 'to', 'imagine', 'effective', 'customer', 'support,', 'sales', 'or', 'marketing', 'without', 'the', 'essential', 'human', 'touch:', 'empathy', 'and', 'emotional', 'intelligence.']), (0.015704839890852793, ['People', 'are', 'yelling', 'at', 'Google', 'Home', 'to', 'play', 'their', 'favorite', 'song,', 'ordering', 'pizza', 'from', 'the', 'Domino’s', 'bot', 'and', 'getting', 'makeup', 'tips', 'from', 'Sephora.', 'But', 'in', 'terms', 'of', 'consumer', 'response', 'and', 'developer', 'involvement,', 'chatbots', 'haven’t', 'lived', 'up', 'to', 'the', 'hype', 'generated', 'circa', '2015/16.']), (0.015695219002982458, ['Messaging', 'will', 'continue', 'to', 'gain', 'traction.', 'Chatbots', 'aren’t', 'going', 'away.', 'NLP', 'and', 'AI', 'are', 'becoming', 'more', 'sophisticated', 'every', 'day.']), (0.015595344540985968, ['At', 'the', 'end', 'of', 'the', 'day,', 'no', 'amount', 'of', 'witty', 'quips', 'or', 'human-like', 'mannerisms', 'will', 'save', 'a', 'bot', 'from', 'conversational', 'failure.']), (0.015530345865072766, ['Humans', 'can', 'read', 'between', 'the', 'lines,', 'leverage', 'contextual', 'information', 'and', 'understand', 'double', 'layers', 'like', 'sarcasm.', 'Bots', 'quickly', 'forget', 'what', 'they’re', 'talking', 'about,', 'meaning', 'it’s', 'a', 'bit', 'like', 'conversing', 'with', 'someone', 'who', 'has', 'little', 'or', 'no', 'short-term', 'memory.']), (0.015339429532089699, ['According', 'to', 'Dave', 'Feldman,', 'Vice', 'President', 'of', 'Product', 'Design', 'at', 'Heap,', 'chatbots', 'didn’t', 'just', 'take', 'on', 'one', 'difficult', 'problem', 'and', 'fail:', 'they', 'took', 'on', 'several', 'and', 'failed', 'all', 'of', 'them.']), (0.01526340815420277, ['Today’s', 'most', 'successful', 'bot', 'experiences', 'take', 'a', 'hybrid', 'approach,', 'incorporating', 'chat', 'into', 'a', 'broader', 'strategy', 'that', 'encompasses', 'more', 'traditional', 'elements.']), (0.015202910265584006, ['But', 'that’s', 'not', 'the', 'whole', 'story.']), (0.015006477454649026, ['It’s', 'said', 'that', 'a', 'new', 'product', 'or', 'service', 'needs', 'to', 'be', 'two', 'of', 'the', 'following:', 'better,', 'cheaper,', 'or', 'faster.', 'Are', 'chatbots', 'cheaper', 'or', 'faster', 'than', 'apps?', 'No', '—', 'not', 'yet,', 'at', 'least.']), (0.014971397745075776, ['Building', 'a', 'bot', 'for', 'the', 'sake', 'of', 'it,', 'letting', 'it', 'loose', 'and', 'hoping', 'for', 'the', 'best', 'will', 'never', 'end', 'well:']), (0.01450103434837242, ['If', 'there’s', 'one', 'thing', 'that’s', 'broken', 'about', 'sales', 'and', 'marketing,', 'it’s', 'the', 'lack', 'of', 'humanity:', 'brands', 'hide', 'behind', 'ticket', 'numbers,', 'feedback', 'forms,', 'do-not-reply-emails,', 'automated', 'responses', 'and', 'gated', '‘contact', 'us’', 'forms.']), (0.014263879131088248, ['That’s', 'because', 'these', 'bots', 'are', 'purely', 'a', 'reflection', 'of', 'the', 'capability,', 'fastidiousness', 'and', 'patience', 'of', 'the', 'person', 'who', 'created', 'them;', 'and', 'how', 'many', 'user', 'needs', 'and', 'inputs', 'they', 'were', 'able', 'to', 'anticipate.']), (0.013916391670294785, ['There’s', 'a', 'reasons', 'computing', 'moved', 'from', 'text-based', 'to', 'graphical', 'user', 'interfaces', '(GUIs).', 'On', 'the', 'input', 'side,', 'it’s', 'easier', 'and', 'faster', 'to', 'click', 'than', 'it', 'is', 'to', 'type.']), (0.01362038000534908, ['The', 'vast', 'majority', 'of', 'bots', 'are', 'built', 'using', 'decision-tree', 'logic,', 'where', 'the', 'bot’s', 'canned', 'response', 'relies', 'on', 'spotting', 'specific', 'keywords', 'in', 'the', 'user', 'input.']), (0.013361046488644938, ['People', 'aren’t', 'easily', 'fooled,', 'and', 'pretending', 'a', 'bot', 'is', 'a', 'human', 'is', 'guaranteed', 'to', 'diminish', 'returns', '(not', 'to', 'mention', 'the', 'fact', 'that', 'you’re', 'lying', 'to', 'your', 'users).']), (0.013305260636271856, ['The', 'hype', 'is', 'over.', 'And', 'that’s', 'a', 'good', 'thing.', 'Now,', 'we', 'can', 'start', 'examining', 'the', 'middle-grounded', 'grey', 'area,', 'instead', 'of', 'the', 'hyper-inflated,', 'frantic', 'black', 'and', 'white', 'zone.']), (0.012620796177296497, ['And', 'even', 'those', 'rare', 'bots', 'that', 'are', 'powered', 'by', 'state-of-the-art', 'NLP,', 'and', 'excel', 'at', 'processing', 'and', 'producing', 'content,', 'will', 'fall', 'short', 'in', 'comparison.']), (0.012509545123454311, ['Today’s', 'rule-based', 'dialogue', 'systems', 'are', 'too', 'brittle', 'to', 'deal', 'with', 'this', 'kind', 'of', 'unpredictability,', 'and', 'statistical', 'approaches', 'using', 'machine', 'learning', 'are', 'just', 'as', 'limited.', 'The', 'level', 'of', 'AI', 'required', 'for', 'human-like', 'conversation', 'just', 'isn’t', 'available', 'yet.']), (0.012468325178717035, ['A', 'great', 'bot', 'can', 'be', 'about', 'as', 'useful', 'as', 'an', 'average', 'app.', 'When', 'it', 'comes', 'to', 'rich,', 'sophisticated,', 'multi-layered', 'apps,', 'there’s', 'no', 'competition.']), (0.012382822523638102, ['Yes,', 'our', 'industry', 'massively', 'overestimated', 'the', 'initial', 'impact', 'chatbots', 'would', 'have.', 'Emphasis', 'on', 'initial.']), (0.012246895615545816, ['The', 'advantage', 'of', 'this', 'approach', 'is', 'that', 'it’s', 'pretty', 'easy', 'to', 'list', 'all', 'the', 'cases', 'that', 'they', 'are', 'designed', 'to', 'cover.', 'And', 'that’s', 'precisely', 'their', 'disadvantage,', 'too.']), (0.01208489511476452, ['Chatbots', 'weren’t', 'the', 'first', 'technological', 'development', 'to', 'be', 'talked', 'up', 'in', 'grandiose', 'terms', 'and', 'then', 'slump', 'spectacularly.']), (0.011864127171978356, ['Here’s', 'where', 'I', 'think', 'we’re', 'going', 'wrong:']), (0.011725342837105947, ['That’s', 'because', 'machines', 'let', 'us', 'access', 'vast', 'and', 'complex', 'information', 'systems,', 'and', 'the', 'early', 'graphical', 'information', 'systems', 'were', 'a', 'revolutionary', 'leap', 'forward', 'in', 'helping', 'us', 'locate', 'those', 'systems.']), (0.011400050525570912, ['Developers,', 'apps', 'and', 'platforms', 'will', 'continue', 'to', 'experiment', 'with,', 'and', 'heavily', 'invest', 'in,', 'conversational', 'marketing.']), (0.011339909482660372, ['Some', 'platforms', 'provide', 'a', 'bit', 'of', 'NLP,', 'but', 'even', 'the', 'best', 'is', 'at', 'toddler-level', 'capacity', '(for', 'example,', 'think', 'about', 'Siri', 'understanding', 'your', 'words,', 'but', 'not', 'their', 'meaning.)']), (0.011170587990075474, ['Modern-day', 'apps', 'benefit', 'from', 'decades', 'of', 'research', 'and', 'experimentation.', 'Why', 'would', 'we', 'throw', 'this', 'away?']), (0.011119015534157138, ['An', 'oversized', 'assumption', 'has', 'been', 'that', 'apps', 'are', '‘over’,', 'and', 'would', 'be', 'replaced', 'by', 'bots.']), (0.011106513826894145, ['Because', 'there', 'isn’t', 'even', 'an', 'ecosystem', 'for', 'a', 'platform', 'to', 'dominate.']), (0.01088804107707591, ['Not', 'even', 'close.']), (0.010788257203093695, ['Basically,', 'we’re', 'still', 'trying', 'to', 'achieve', 'the', 'same', 'innovations', 'we', 'were', '30', 'years', 'ago.']), (0.010582327217429802, ['Sure,', 'there', 'are', 'some', 'concepts', 'that', 'we', 'can', 'only', 'express', 'using', 'language', '(“show', 'me', 'all', 'the', 'ways', 'of', 'getting', 'to', 'a', 'museum', 'that', 'give', 'me', '2000', 'steps', 'but', 'don’t', 'take', 'longer', 'than', '35', 'minutes”),', 'but', 'most', 'tasks', 'can', 'be', 'carried', 'out', 'more', 'efficiently', 'and', 'intuitively', 'with', 'GUIs', 'than', 'with', 'a', 'conversational', 'UI.']), (0.01057678974946533, ['You', 'might', 'remember', 'a', 'similar', 'war', 'cry', 'when', 'apps', 'first', 'came', 'onto', 'the', 'scene', 'ten', 'years', 'ago:', 'but', 'do', 'you', 'remember', 'when', 'apps', 'replaced', 'the', 'internet?']), (0.010520521078461053, ['And', 'apps', 'are,', 'tellingly,', 'still', 'alive', 'and', 'well.']), (0.010459738678129002, ['Bots', 'can', 'interface', 'with', 'users', 'in', 'different', 'ways.', 'The', 'big', 'divide', 'is', 'text', 'vs.', 'speech.', 'In', 'the', 'beginning', '(of', 'computer', 'interfaces)', 'was', 'the', '(written)', 'word.']), (0.010327492436904201, ['For', 'now,', 'bots', 'can', 'continue', 'to', 'help', 'us', 'with', 'automated,', 'repetitive,', 'low-level', 'tasks', 'and', 'queries;', 'as', 'cogs', 'in', 'a', 'larger,', 'more', 'complex', 'system.', 'And', 'we', 'did', 'them,', 'and', 'ourselves,', 'a', 'disservice', 'by', 'expecting', 'so', 'much,', 'so', 'soon.']), (0.010261421072789703, ['In', 'an', 'ideal', 'world,', 'the', 'technology', 'known', 'as', 'NLP', '(natural', 'language', 'processing)', 'should', 'allow', 'a', 'chatbot', 'to', 'understand', 'the', 'messages', 'it', 'receives.', 'But', 'NLP', 'is', 'only', 'just', 'emerging', 'from', 'research', 'labs', 'and', 'is', 'very', 'much', 'in', 'its', 'infancy.']), (0.009663308675798425, ['Another', 'bunch', 'of', 'scientists', 'were', 'developing', 'speech-processing', 'software', 'so', 'that', 'you', 'could', 'just', 'speak', 'to', 'your', 'computer,', 'rather', 'than', 'having', 'to', 'type.', 'This', 'turned', 'out', 'to', 'be', 'a', 'whole', 'lot', 'more', 'difficult', 'than', 'anyone', 'originally', 'realised:']), (0.009539949926816414, ['A', 'conversation', 'encompasses', 'so', 'much', 'more', 'than', 'just', 'text.']), (0.009002964604208229, ['We', 'love', 'optical', 'displays', 'of', 'information', 'because', 'we', 'are', 'highly', 'visual', 'creatures.', 'It’s', 'no', 'accident', 'that', 'kids', 'love', 'touch', 'screens.', 'The', 'pioneers', 'who', 'dreamt', 'up', 'graphical', 'interface', 'were', 'inspired', 'by', 'cognitive', 'psychology,', 'the', 'study', 'of', 'how', 'the', 'brain', 'deals', 'with', 'communication.']), (0.008964524043218, ['Another', 'problematic', 'aspect', 'of', 'the', 'sweeping', 'nature', 'of', 'hype', 'is', 'that', 'it', 'tends', 'to', 'bypass', 'essential', 'questions', 'like', 'these.']), (0.008944522945224112, ['Then,', 'graphical', 'user', 'interfaces', '(GUIs)', 'came', 'along', 'and', 'saved', 'the', 'day.', 'We', 'became', 'entranced', 'by', 'windows,', 'mouse', 'clicks,', 'icons.', 'And', 'hey,', 'we', 'eventually', 'got', 'color,', 'too!']), (0.00892855401427772, ['But,', 'if', 'we', 'swap', 'the', 'word', '‘replace’', 'with', '‘extend’,', 'things', 'get', 'much', 'more', 'interesting.']), (0.008875755115994125, ['Tapping', 'or', 'selecting', 'is', 'obviously', 'preferable', 'to', 'typing', 'out', 'a', 'whole', 'sentence,', 'even', 'with', 'predictive', '(often', 'error-prone', ')', 'text.', 'On', 'the', 'output', 'side,', 'the', 'old', 'adage', 'that', 'a', 'picture', 'is', 'worth', 'a', 'thousand', 'words', 'is', 'usually', 'true.']), (0.008611179971757166, ['At', 'the', 'Mobile', 'World', 'Congress', '2017,', 'chatbots', 'were', 'the', 'main', 'headliners.', 'The', 'conference', 'organizers', 'cited', 'an', '‘overwhelming', 'acceptance', 'at', 'the', 'event', 'of', 'the', 'inevitable', 'shift', 'of', 'focus', 'for', 'brands', 'and', 'corporates', 'to', 'chatbots’.']), (0.008555616297202714, ['And', 'in', 'the', 'meantime,', 'there', 'are', 'few', 'high-quality', 'examples', 'of', 'trailblazing', 'bots', 'to', 'lead', 'the', 'way.', 'As', 'Dave', 'Feldman', 'remarked:']), (0.008433711070810592, ['Meanwhile,', 'a', 'bunch', 'of', 'research', 'scientists', 'were', 'busily', 'developing', 'natural', 'language', '(NL)', 'interfaces', 'to', 'databases,', 'instead', 'of', 'having', 'to', 'learn', 'an', 'arcane', 'database', 'query', 'language.']), (0.008286142278658343, ['And', 'conversations', 'are', 'complex.', 'They’re', 'not', 'linear.', 'Topics', 'spin', 'around', 'each', 'other,', 'take', 'random', 'turns,', 'restart', 'or', 'abruptly', 'finish.']), (0.008111949170649075, ['Head', 'of', 'Growth', 'for', 'GrowthBot,', 'Messaging', '&', 'Conversational', 'Strategy', '@HubSpot']), (0.007803962770025296, ['But', 'is', 'that', 'how', 'humans', 'prefer', 'to', 'interact', 'with', 'machines?']), (0.007720009643292527, ['When', 'we', 'were', 'building', 'GrowthBot,', 'we', 'decided', 'to', 'make', 'it', 'specific', 'to', 'sales', 'and', 'marketers:', 'not', 'an', '‘all-rounder’,', 'despite', 'the', 'temptation', 'to', 'get', 'overexcited', 'about', 'potential', 'capabilties.']), (0.007586633271213697, ['Users', 'had', 'to', 'type', 'commands', 'manually', 'into', 'a', 'machine', 'to', 'get', 'anything', 'done.']), (0.007528846432153004, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.006943687607496696, ['Pretty', 'cool,', 'right?', 'The', 'system', 'takes', 'turns', 'in', 'collaborative', 'way,', 'and', 'does', 'a', 'smart', 'job', 'of', 'figuring', 'out', 'what', 'the', 'user', 'wants.']), (0.006575345578345078, ['One', 'year', 'on,', 'we', 'have', 'an', 'answer', 'to', 'that', 'question.']), (0.006464644036754686, ['The', 'next', 'item', 'on', 'the', 'agenda', 'was', 'holding', 'a', 'two-way', 'dialog', 'with', 'a', 'machine.', 'Here’s', 'an', 'example', 'dialog', '(dating', 'back', 'to', 'the', '1990s)', 'with', 'VCR', 'setup', 'system:']), (0.006106864741833724, ['I', 'believe', 'we’re', 'at', 'the', 'very', 'beginning', 'of', 'explosive', 'growth.', 'This', 'sense', 'of', 'anti-climax', 'is', 'completely', 'normal', 'for', 'transformational', 'technology.']), (0.005491545528726648, ['Modern', 'day', 'bots,', 'whether', 'they', 'use', 'typed', 'or', 'spoken', 'input,', 'have', 'to', 'face', 'all', 'these', 'challenges,', 'but', 'also', 'work', 'in', 'an', 'efficient', 'and', 'scalable', 'way', 'on', 'a', 'variety', 'of', 'platforms.']), (0.005078272292097376, ['The', 'age-old', 'hype', 'cycle', 'unfolded', 'in', 'familiar', 'fashion...']), (0.004783960015611919, ['Computers', 'are', 'good', 'at', 'being', 'computers.', 'Searching', 'for', 'data,', 'crunching', 'numbers,', 'analyzing', 'opinions', 'and', 'condensing', 'that', 'information.']), (0.0044782987706082945, ['We', 'look', 'back', 'at', 'our', 'breathless', 'optimism', 'and', 'turn', 'to', 'each', 'other,', 'slightly', 'baffled:']), (0.004415713230697847, ['In', 'a', 'way,', 'those', 'early-adopters', 'weren’t', 'entirely', 'wrong.']), (0.00439169248347647, ['As', 'Matt', 'Asay', 'outlines,', 'this', 'results', 'in', 'another', 'issue:', 'failure', 'to', 'capture', 'the', 'attention', 'and', 'creativity', 'of', 'developers.']), (0.004030735895153497, ['Aiming', 'for', 'a', 'human', 'dimension', 'in', 'business', 'interactions', 'makes', 'sense.']), (0.00401181662352732, ['It', 'was', 'carefully', 'crafted', 'to', 'deal', 'with', 'conversations', 'involving', 'VCRs,', 'and', 'could', 'only', 'operate', 'within', 'strict', 'limitations.']), (0.0037107717281077107, ['“is', 'that', 'it?', 'THAT', 'was', 'the', 'chatbot', 'revolution', 'we', 'were', 'promised?”']), (0.0036527758239345773, ['The', 'predicted', 'paradim', 'shift', 'didn’t', 'materialize.']), (0.0035647360039128113, ['Our', 'hopes', 'were', 'sky', 'high.', 'Bright-eyed', 'and', 'bushy-tailed,', 'the', 'industry', 'was', 'ripe', 'for', 'a', 'new', 'era', 'of', 'innovation:', 'it', 'was', 'time', 'to', 'start', 'socializing', 'with', 'machines.']), (0.0025105311060616077, ['Problems', 'arise', 'when', 'life', 'refuses', 'to', 'fit', 'into', 'those', 'boxes.']), (0.0024658743377005514, ['Expectations', 'built,', 'built,', 'and', 'then.....', 'It', 'all', 'kind', 'of', 'fizzled', 'out.']), (0.0024620042524601655, ['And', 'I', 'can’t', 'wait', 'to', 'see', 'what', 'happens', 'next.']), (0.0017201834862387154, ['Oh,', 'how', 'the', 'headlines', 'blared:']), (0.0017201834862387154, ['Not', 'necessarily.']), (0.0017201834862387154, ['No.']), (0.0017201834862387154, [\"Medium's\", 'largest', 'publication', 'for', 'makers.', 'Subscribe', 'to', 'receive', 'our', 'top', 'stories', 'here', '→', 'https://goo.gl/zHcLJi']), (0.0017201834862387154, ['Digit’s', 'Ethan', 'Bloch', 'sums', 'up', 'the', 'general', 'consensus:']), (0.0017201834862387154, ['As', 'HubSpot', 'team', 'pinpointed:']), (0.0017201834862387154, ['As', 'Bill', 'Gates', 'once', 'said:']), (0.0017201834862387154, ['And', 'why', 'wouldn’t', 'they', 'be?', 'All', 'the', 'road', 'signs', 'pointed', 'towards', 'insane', 'success.'])]\n",
      "Summarize Text: \n",
      " A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.. Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.. Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.. Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.. In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:\n",
      "None\n",
      "Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as random search, aim to simplify the model selection and tuning parts of machine learning by finding the best model for a dataset with little to no manual intervention. However, feature engineering, an arguably more valuable aspect of the machine learning pipeline, remains almost entirely a human labor.\n",
      "Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. This step can be more important than the actual model used because a machine learning algorithm only learns from the data we give it, and creating features that are relevant to a task is absolutely crucial (see the excellent paper “A Few Useful Things to Know about Machine Learning”).\n",
      "Typically, feature engineering is a drawn-out manual process, relying on domain knowledge, intuition, and data manipulation. This process can be extremely tedious and the final features will be limited both by human subjectivity and time. Automated feature engineering aims to help the data scientist by automatically creating many candidate features out of a dataset from which the best can be selected and used for training.\n",
      "In this article, we will walk through an example of using automated feature engineering with the featuretools Python library. We will use an example dataset to show the basics (stay tuned for future posts using real-world data). The complete code for this article is available on GitHub.\n",
      "Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model.\n",
      "The process of constructing features is very time-consuming because each new feature usually requires several steps to build, especially when using information from more than one table. We can group the operations of feature creation into two categories: transformations and aggregations. Let’s look at a few examples to see these concepts in action.\n",
      "A transformation acts on a single table (thinking in terms of Python, a table is just a Pandas DataFrame ) by creating new features out of one or more of the existing columns. As an example, if we have the table of clients below\n",
      "we can create features by finding the month of the joined column or taking the natural log of the income column. These are both transformations because they use information from only one table.\n",
      "On the other hand, aggregations are performed across tables, and use a one-to-many relationship to group observations and then calculate statistics. For example, if we have another table with information on the loans of clients, where each client may have multiple loans, we can calculate statistics such as the average, maximum, and minimum of loans for each client.\n",
      "This process involves grouping the loans table by the client, calculating the aggregations, and then merging the resulting data into the client data. Here’s how we would do that in Python using the language of Pandas.\n",
      "These operations are not difficult by themselves, but if we have hundreds of variables spread across dozens of tables, this process is not feasible to do by hand. Ideally, we want a solution that can automatically perform transformations and aggregations across multiple tables and combine the resulting data into a single table. Although Pandas is a great resource, there’s only so much data manipulation we want to do by hand! (For more on manual feature engineering check out the excellent Python Data Science Handbook).\n",
      "Fortunately, featuretools is exactly the solution we are looking for. This open-source Python library will automatically create many features from a set of related tables. Featuretools is based on a method known as “Deep Feature Synthesis”, which sounds a lot more imposing than it actually is (the name comes from stacking multiple features not because it uses deep learning!).\n",
      "Deep feature synthesis stacks multiple transformation and aggregation operations (which are called feature primitives in the vocab of featuretools) to create features from data spread across many tables. Like most ideas in machine learning, it’s a complex method built on a foundation of simple concepts. By learning one building block at a time, we can form a good understanding of this powerful method.\n",
      "First, let’s take a look at our example data. We already saw some of the dataset above, and the complete collection of tables is as follows:\n",
      "If we have a machine learning task, such as predicting whether a client will repay a future loan, we will want to combine all the information about clients into a single table. The tables are related (through the client_id and the loan_id variables) and we could use a series of transformations and aggregations to do this process by hand. However, we will shortly see that we can instead use featuretools to automate the process.\n",
      "The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes.\n",
      "We can create an empty entityset in featuretools using the following:\n",
      "Now we have to add entities. Each entity must have an index, which is a column with all unique elements. That is, each value in the index must appear in the table only once. The index in the clients dataframe is the client_idbecause each client has only one row in this dataframe. We add an entity with an existing index to an entityset using the following syntax:\n",
      "The loans dataframe also has a unique index, loan_id and the syntax to add this to the entityset is the same as for clients. However, for the payments dataframe, there is no unique index. When we add this entity to the entityset, we need to pass in the parameter make_index = True and specify the name of the index. Also, although featuretools will automatically infer the data type of each column in an entity, we can override this by passing in a dictionary of column types to the parameter variable_types .\n",
      "For this dataframe, even though missed is an integer, this is not a numeric variable since it can only take on 2 discrete values, so we tell featuretools to treat is as a categorical variable. After adding the dataframes to the entityset, we inspect any of them:\n",
      "The column types have been correctly inferred with the modification we specified. Next, we need to specify how the tables in the entityset are related.\n",
      "The best way to think of a relationship between two tables is the analogy of parent to child. This is a one-to-many relationship: each parent can have multiple children. In the realm of tables, a parent table has one row for every parent, but the child table may have multiple rows corresponding to multiple children of the same parent.\n",
      "For example, in our dataset, the clients dataframe is a parent of the loans dataframe. Each client has only one row in clients but may have multiple rows in loans. Likewise, loans is the parent of payments because each loan will have multiple payments. The parents are linked to their children by a shared variable. When we perform aggregations, we group the child table by the parent variable and calculate statistics across the children of each parent.\n",
      "To formalize a relationship in featuretools, we only need to specify the variable that links two tables together. The clients and the loans table are linked via the client_id variable and loans and payments are linked with the loan_id. The syntax for creating a relationship and adding it to the entityset are shown below:\n",
      "The entityset now contains the three entities (tables) and the relationships that link these entities together. After adding entities and formalizing relationships, our entityset is complete and we are ready to make features.\n",
      "Before we can quite get to deep feature synthesis, we need to understand feature primitives. We already know what these are, but we have just been calling them by different names! These are simply the basic operations that we use to form new features:\n",
      "New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives):\n",
      "These primitives can be used by themselves or combined to create features. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). We pass in the entityset, the target_entity , which is the table where we want to add the features, the selected trans_primitives (transformations), and agg_primitives (aggregations):\n",
      "The result is a dataframe of new features for each client (because we made clients the target_entity). For example, we have the month each client joined which is a transformation feature primitive:\n",
      "We also have a number of aggregation primitives such as the average payment amounts for each client:\n",
      "Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives.\n",
      "The complete dataframe has 793 columns of new features!\n",
      "We now have all the pieces in place to understand deep feature synthesis (dfs). In fact, we already performed dfs in the previous function call! A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature.\n",
      "For example, the MEAN(payments.payment_amount) column is a deep feature with a depth of 1 because it was created using a single aggregation. A feature with a depth of two is LAST(loans(MEAN(payments.payment_amount)) This is made by stacking two aggregations: LAST (most recent) on top of MEAN. This represents the average payment size of the most recent loan for each client.\n",
      "We can stack features to any depth we want, but in practice, I have never gone beyond a depth of 2. After this point, the features are difficult to interpret, but I encourage anyone interested to try “going deeper”.\n",
      "We do not have to manually specify the feature primitives, but instead can let featuretools automatically choose features for us. To do this, we use the same ft.dfs function call but do not pass in any feature primitives:\n",
      "Featuretools has built many new features for us to use. While this process does automatically create new features, it will not replace the data scientist because we still have to figure out what to do with all these features. For example, if our goal is to predict whether or not a client will repay a loan, we could look for the features most correlated with a specified outcome. Moreover, if we have domain knowledge, we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features.\n",
      "Automated feature engineering has solved one problem, but created another: too many features. Although it’s difficult to say before fitting a model which of these features will be important, it’s likely not all of them will be relevant to a task we want to train our model on. Moreover, having too many features can lead to poor model performance because the less useful features drown out those that are more important.\n",
      "The problem of too many features is known as the curse of dimensionality. As the number of features increases (the dimension of the data grows) it becomes more and more difficult for a model to learn the mapping between features and targets. In fact, the amount of data needed for the model to perform well scales exponentially with the number of features.\n",
      "The curse of dimensionality is combated with feature reduction (also known as feature selection): the process of removing irrelevant features. This can take on many forms: Principal Component Analysis (PCA), SelectKBest, using feature importances from a model, or auto-encoding using deep neural networks. However, feature reduction is a different topic for another article. For now, we know that we can use featuretools to create numerous features from many tables with minimal effort!\n",
      "Like many topics in machine learning, automated feature engineering with featuretools is a complicated concept built on simple ideas. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables.\n",
      "In future articles, I’ll show how to use this technique on a real world problem, the Home Credit Default Risk competition currently being hosted on Kaggle. Stay tuned for that post, and in the meantime, read this introduction to get started in the competition! I hope that you can now use automated feature engineering as an aid in a data science pipeline. Our models are only as good as the data we give them, and automated feature engineering can help to make the feature creation process more efficient.\n",
      "For more information on featuretools, including advanced usage, check out the online documentation. To see how featuretools is used in practice, read about the work of Feature Labs, the company behind the open-source library.\n",
      "As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Data Scientist and Master Student, Data Science Communicator and Advocate\n",
      "Sharing concepts, ideas, and codes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.04144996240242197, ['Like', 'many', 'topics', 'in', 'machine', 'learning,', 'automated', 'feature', 'engineering', 'with', 'featuretools', 'is', 'a', 'complicated', 'concept', 'built', 'on', 'simple', 'ideas.', 'Using', 'concepts', 'of', 'entitysets,', 'entities,', 'and', 'relationships,', 'featuretools', 'can', 'perform', 'deep', 'feature', 'synthesis', 'to', 'create', 'new', 'features.', 'Deep', 'feature', 'synthesis', 'in', 'turn', 'stacks', 'feature', 'primitives', '—', 'aggregations,', 'which', 'act', 'across', 'a', 'one-to-many', 'relationship', 'between', 'tables,', 'and', 'transformations,', 'functions', 'applied', 'to', 'one', 'or', 'more', 'columns', 'in', 'a', 'single', 'table', '—', 'to', 'build', 'new', 'features', 'from', 'multiple', 'tables.']), (0.03790921873080406, ['Featuretools', 'has', 'built', 'many', 'new', 'features', 'for', 'us', 'to', 'use.', 'While', 'this', 'process', 'does', 'automatically', 'create', 'new', 'features,', 'it', 'will', 'not', 'replace', 'the', 'data', 'scientist', 'because', 'we', 'still', 'have', 'to', 'figure', 'out', 'what', 'to', 'do', 'with', 'all', 'these', 'features.', 'For', 'example,', 'if', 'our', 'goal', 'is', 'to', 'predict', 'whether', 'or', 'not', 'a', 'client', 'will', 'repay', 'a', 'loan,', 'we', 'could', 'look', 'for', 'the', 'features', 'most', 'correlated', 'with', 'a', 'specified', 'outcome.', 'Moreover,', 'if', 'we', 'have', 'domain', 'knowledge,', 'we', 'can', 'use', 'that', 'to', 'choose', 'specific', 'feature', 'primitives', 'or', 'seed', 'deep', 'feature', 'synthesis', 'with', 'candidate', 'features.']), (0.03481946884093158, ['Feature', 'engineering', 'means', 'building', 'additional', 'features', 'out', 'of', 'existing', 'data', 'which', 'is', 'often', 'spread', 'across', 'multiple', 'related', 'tables.', 'Feature', 'engineering', 'requires', 'extracting', 'the', 'relevant', 'information', 'from', 'the', 'data', 'and', 'getting', 'it', 'into', 'a', 'single', 'table', 'which', 'can', 'then', 'be', 'used', 'to', 'train', 'a', 'machine', 'learning', 'model.']), (0.0341683568090424, ['The', 'curse', 'of', 'dimensionality', 'is', 'combated', 'with', 'feature', 'reduction', '(also', 'known', 'as', 'feature', 'selection):', 'the', 'process', 'of', 'removing', 'irrelevant', 'features.', 'This', 'can', 'take', 'on', 'many', 'forms:', 'Principal', 'Component', 'Analysis', '(PCA),', 'SelectKBest,', 'using', 'feature', 'importances', 'from', 'a', 'model,', 'or', 'auto-encoding', 'using', 'deep', 'neural', 'networks.', 'However,', 'feature', 'reduction', 'is', 'a', 'different', 'topic', 'for', 'another', 'article.', 'For', 'now,', 'we', 'know', 'that', 'we', 'can', 'use', 'featuretools', 'to', 'create', 'numerous', 'features', 'from', 'many', 'tables', 'with', 'minimal', 'effort!']), (0.03244816866249538, ['New', 'features', 'are', 'created', 'in', 'featuretools', 'using', 'these', 'primitives', 'either', 'by', 'themselves', 'or', 'stacking', 'multiple', 'primitives.', 'Below', 'is', 'a', 'list', 'of', 'some', 'of', 'the', 'feature', 'primitives', 'in', 'featuretools', '(we', 'can', 'also', 'define', 'custom', 'primitives):']), (0.03107988356795362, ['Typically,', 'feature', 'engineering', 'is', 'a', 'drawn-out', 'manual', 'process,', 'relying', 'on', 'domain', 'knowledge,', 'intuition,', 'and', 'data', 'manipulation.', 'This', 'process', 'can', 'be', 'extremely', 'tedious', 'and', 'the', 'final', 'features', 'will', 'be', 'limited', 'both', 'by', 'human', 'subjectivity', 'and', 'time.', 'Automated', 'feature', 'engineering', 'aims', 'to', 'help', 'the', 'data', 'scientist', 'by', 'automatically', 'creating', 'many', 'candidate', 'features', 'out', 'of', 'a', 'dataset', 'from', 'which', 'the', 'best', 'can', 'be', 'selected', 'and', 'used', 'for', 'training.']), (0.03086446721142959, ['Deep', 'feature', 'synthesis', 'stacks', 'multiple', 'transformation', 'and', 'aggregation', 'operations', '(which', 'are', 'called', 'feature', 'primitives', 'in', 'the', 'vocab', 'of', 'featuretools)', 'to', 'create', 'features', 'from', 'data', 'spread', 'across', 'many', 'tables.', 'Like', 'most', 'ideas', 'in', 'machine', 'learning,', 'it’s', 'a', 'complex', 'method', 'built', 'on', 'a', 'foundation', 'of', 'simple', 'concepts.', 'By', 'learning', 'one', 'building', 'block', 'at', 'a', 'time,', 'we', 'can', 'form', 'a', 'good', 'understanding', 'of', 'this', 'powerful', 'method.']), (0.030851472506223696, ['Feature', 'engineering,', 'also', 'known', 'as', 'feature', 'creation,', 'is', 'the', 'process', 'of', 'constructing', 'new', 'features', 'from', 'existing', 'data', 'to', 'train', 'a', 'machine', 'learning', 'model.', 'This', 'step', 'can', 'be', 'more', 'important', 'than', 'the', 'actual', 'model', 'used', 'because', 'a', 'machine', 'learning', 'algorithm', 'only', 'learns', 'from', 'the', 'data', 'we', 'give', 'it,', 'and', 'creating', 'features', 'that', 'are', 'relevant', 'to', 'a', 'task', 'is', 'absolutely', 'crucial', '(see', 'the', 'excellent', 'paper', '“A', 'Few', 'Useful', 'Things', 'to', 'Know', 'about', 'Machine', 'Learning”).']), (0.03067146599130048, ['We', 'now', 'have', 'all', 'the', 'pieces', 'in', 'place', 'to', 'understand', 'deep', 'feature', 'synthesis', '(dfs).', 'In', 'fact,', 'we', 'already', 'performed', 'dfs', 'in', 'the', 'previous', 'function', 'call!', 'A', 'deep', 'feature', 'is', 'simply', 'a', 'feature', 'made', 'of', 'stacking', 'multiple', 'primitives', 'and', 'dfs', 'is', 'the', 'name', 'of', 'process', 'that', 'makes', 'these', 'features.', 'The', 'depth', 'of', 'a', 'deep', 'feature', 'is', 'the', 'number', 'of', 'primitives', 'required', 'to', 'make', 'the', 'feature.']), (0.030520514197718427, ['Even', 'though', 'we', 'specified', 'only', 'a', 'few', 'feature', 'primitives,', 'featuretools', 'created', 'many', 'new', 'features', 'by', 'combining', 'and', 'stacking', 'these', 'primitives.']), (0.02991935339543243, ['We', 'do', 'not', 'have', 'to', 'manually', 'specify', 'the', 'feature', 'primitives,', 'but', 'instead', 'can', 'let', 'featuretools', 'automatically', 'choose', 'features', 'for', 'us.', 'To', 'do', 'this,', 'we', 'use', 'the', 'same', 'ft.dfs', 'function', 'call', 'but', 'do', 'not', 'pass', 'in', 'any', 'feature', 'primitives:']), (0.029359903421984874, ['The', 'process', 'of', 'constructing', 'features', 'is', 'very', 'time-consuming', 'because', 'each', 'new', 'feature', 'usually', 'requires', 'several', 'steps', 'to', 'build,', 'especially', 'when', 'using', 'information', 'from', 'more', 'than', 'one', 'table.', 'We', 'can', 'group', 'the', 'operations', 'of', 'feature', 'creation', 'into', 'two', 'categories:', 'transformations', 'and', 'aggregations.', 'Let’s', 'look', 'at', 'a', 'few', 'examples', 'to', 'see', 'these', 'concepts', 'in', 'action.']), (0.028278814522911987, ['Fortunately,', 'featuretools', 'is', 'exactly', 'the', 'solution', 'we', 'are', 'looking', 'for.', 'This', 'open-source', 'Python', 'library', 'will', 'automatically', 'create', 'many', 'features', 'from', 'a', 'set', 'of', 'related', 'tables.', 'Featuretools', 'is', 'based', 'on', 'a', 'method', 'known', 'as', '“Deep', 'Feature', 'Synthesis”,', 'which', 'sounds', 'a', 'lot', 'more', 'imposing', 'than', 'it', 'actually', 'is', '(the', 'name', 'comes', 'from', 'stacking', 'multiple', 'features', 'not', 'because', 'it', 'uses', 'deep', 'learning!).']), (0.027008665262274377, ['In', 'future', 'articles,', 'I’ll', 'show', 'how', 'to', 'use', 'this', 'technique', 'on', 'a', 'real', 'world', 'problem,', 'the', 'Home', 'Credit', 'Default', 'Risk', 'competition', 'currently', 'being', 'hosted', 'on', 'Kaggle.', 'Stay', 'tuned', 'for', 'that', 'post,', 'and', 'in', 'the', 'meantime,', 'read', 'this', 'introduction', 'to', 'get', 'started', 'in', 'the', 'competition!', 'I', 'hope', 'that', 'you', 'can', 'now', 'use', 'automated', 'feature', 'engineering', 'as', 'an', 'aid', 'in', 'a', 'data', 'science', 'pipeline.', 'Our', 'models', 'are', 'only', 'as', 'good', 'as', 'the', 'data', 'we', 'give', 'them,', 'and', 'automated', 'feature', 'engineering', 'can', 'help', 'to', 'make', 'the', 'feature', 'creation', 'process', 'more', 'efficient.']), (0.0270076278206123, ['These', 'primitives', 'can', 'be', 'used', 'by', 'themselves', 'or', 'combined', 'to', 'create', 'features.', 'To', 'make', 'features', 'with', 'specified', 'primitives', 'we', 'use', 'the', 'ft.dfs', 'function', '(standing', 'for', 'deep', 'feature', 'synthesis).', 'We', 'pass', 'in', 'the', 'entityset,', 'the', 'target_entity', ',', 'which', 'is', 'the', 'table', 'where', 'we', 'want', 'to', 'add', 'the', 'features,', 'the', 'selected', 'trans_primitives', '(transformations),', 'and', 'agg_primitives', '(aggregations):']), (0.02608622005711437, ['The', 'result', 'is', 'a', 'dataframe', 'of', 'new', 'features', 'for', 'each', 'client', '(because', 'we', 'made', 'clients', 'the', 'target_entity).', 'For', 'example,', 'we', 'have', 'the', 'month', 'each', 'client', 'joined', 'which', 'is', 'a', 'transformation', 'feature', 'primitive:']), (0.025876696253012216, ['These', 'operations', 'are', 'not', 'difficult', 'by', 'themselves,', 'but', 'if', 'we', 'have', 'hundreds', 'of', 'variables', 'spread', 'across', 'dozens', 'of', 'tables,', 'this', 'process', 'is', 'not', 'feasible', 'to', 'do', 'by', 'hand.', 'Ideally,', 'we', 'want', 'a', 'solution', 'that', 'can', 'automatically', 'perform', 'transformations', 'and', 'aggregations', 'across', 'multiple', 'tables', 'and', 'combine', 'the', 'resulting', 'data', 'into', 'a', 'single', 'table.', 'Although', 'Pandas', 'is', 'a', 'great', 'resource,', 'there’s', 'only', 'so', 'much', 'data', 'manipulation', 'we', 'want', 'to', 'do', 'by', 'hand!', '(For', 'more', 'on', 'manual', 'feature', 'engineering', 'check', 'out', 'the', 'excellent', 'Python', 'Data', 'Science', 'Handbook).']), (0.02479786350484961, ['Automated', 'feature', 'engineering', 'has', 'solved', 'one', 'problem,', 'but', 'created', 'another:', 'too', 'many', 'features.', 'Although', 'it’s', 'difficult', 'to', 'say', 'before', 'fitting', 'a', 'model', 'which', 'of', 'these', 'features', 'will', 'be', 'important,', 'it’s', 'likely', 'not', 'all', 'of', 'them', 'will', 'be', 'relevant', 'to', 'a', 'task', 'we', 'want', 'to', 'train', 'our', 'model', 'on.', 'Moreover,', 'having', 'too', 'many', 'features', 'can', 'lead', 'to', 'poor', 'model', 'performance', 'because', 'the', 'less', 'useful', 'features', 'drown', 'out', 'those', 'that', 'are', 'more', 'important.']), (0.02457254691568002, ['Before', 'we', 'can', 'quite', 'get', 'to', 'deep', 'feature', 'synthesis,', 'we', 'need', 'to', 'understand', 'feature', 'primitives.', 'We', 'already', 'know', 'what', 'these', 'are,', 'but', 'we', 'have', 'just', 'been', 'calling', 'them', 'by', 'different', 'names!', 'These', 'are', 'simply', 'the', 'basic', 'operations', 'that', 'we', 'use', 'to', 'form', 'new', 'features:']), (0.02353329499769147, ['In', 'this', 'article,', 'we', 'will', 'walk', 'through', 'an', 'example', 'of', 'using', 'automated', 'feature', 'engineering', 'with', 'the', 'featuretools', 'Python', 'library.', 'We', 'will', 'use', 'an', 'example', 'dataset', 'to', 'show', 'the', 'basics', '(stay', 'tuned', 'for', 'future', 'posts', 'using', 'real-world', 'data).', 'The', 'complete', 'code', 'for', 'this', 'article', 'is', 'available', 'on', 'GitHub.']), (0.023278801928338946, ['For', 'example,', 'the', 'MEAN(payments.payment_amount)', 'column', 'is', 'a', 'deep', 'feature', 'with', 'a', 'depth', 'of', '1', 'because', 'it', 'was', 'created', 'using', 'a', 'single', 'aggregation.', 'A', 'feature', 'with', 'a', 'depth', 'of', 'two', 'is', 'LAST(loans(MEAN(payments.payment_amount))', 'This', 'is', 'made', 'by', 'stacking', 'two', 'aggregations:', 'LAST', '(most', 'recent)', 'on', 'top', 'of', 'MEAN.', 'This', 'represents', 'the', 'average', 'payment', 'size', 'of', 'the', 'most', 'recent', 'loan', 'for', 'each', 'client.']), (0.022903988084503946, ['The', 'problem', 'of', 'too', 'many', 'features', 'is', 'known', 'as', 'the', 'curse', 'of', 'dimensionality.', 'As', 'the', 'number', 'of', 'features', 'increases', '(the', 'dimension', 'of', 'the', 'data', 'grows)', 'it', 'becomes', 'more', 'and', 'more', 'difficult', 'for', 'a', 'model', 'to', 'learn', 'the', 'mapping', 'between', 'features', 'and', 'targets.', 'In', 'fact,', 'the', 'amount', 'of', 'data', 'needed', 'for', 'the', 'model', 'to', 'perform', 'well', 'scales', 'exponentially', 'with', 'the', 'number', 'of', 'features.']), (0.022224154652865386, ['A', 'transformation', 'acts', 'on', 'a', 'single', 'table', '(thinking', 'in', 'terms', 'of', 'Python,', 'a', 'table', 'is', 'just', 'a', 'Pandas', 'DataFrame', ')', 'by', 'creating', 'new', 'features', 'out', 'of', 'one', 'or', 'more', 'of', 'the', 'existing', 'columns.', 'As', 'an', 'example,', 'if', 'we', 'have', 'the', 'table', 'of', 'clients', 'below']), (0.022008001426150954, ['The', 'first', 'two', 'concepts', 'of', 'featuretools', 'are', 'entities', 'and', 'entitysets.', 'An', 'entity', 'is', 'simply', 'a', 'table', '(or', 'a', 'DataFrame', 'if', 'you', 'think', 'in', 'Pandas).', 'An', 'EntitySet', 'is', 'a', 'collection', 'of', 'tables', 'and', 'the', 'relationships', 'between', 'them.', 'Think', 'of', 'an', 'entityset', 'as', 'just', 'another', 'Python', 'data', 'structure,', 'with', 'its', 'own', 'methods', 'and', 'attributes.']), (0.02010854734469443, ['If', 'we', 'have', 'a', 'machine', 'learning', 'task,', 'such', 'as', 'predicting', 'whether', 'a', 'client', 'will', 'repay', 'a', 'future', 'loan,', 'we', 'will', 'want', 'to', 'combine', 'all', 'the', 'information', 'about', 'clients', 'into', 'a', 'single', 'table.', 'The', 'tables', 'are', 'related', '(through', 'the', 'client_id', 'and', 'the', 'loan_id', 'variables)', 'and', 'we', 'could', 'use', 'a', 'series', 'of', 'transformations', 'and', 'aggregations', 'to', 'do', 'this', 'process', 'by', 'hand.', 'However,', 'we', 'will', 'shortly', 'see', 'that', 'we', 'can', 'instead', 'use', 'featuretools', 'to', 'automate', 'the', 'process.']), (0.01914552341198782, ['For', 'example,', 'in', 'our', 'dataset,', 'the', 'clients', 'dataframe', 'is', 'a', 'parent', 'of', 'the', 'loans', 'dataframe.', 'Each', 'client', 'has', 'only', 'one', 'row', 'in', 'clients', 'but', 'may', 'have', 'multiple', 'rows', 'in', 'loans.', 'Likewise,', 'loans', 'is', 'the', 'parent', 'of', 'payments', 'because', 'each', 'loan', 'will', 'have', 'multiple', 'payments.', 'The', 'parents', 'are', 'linked', 'to', 'their', 'children', 'by', 'a', 'shared', 'variable.', 'When', 'we', 'perform', 'aggregations,', 'we', 'group', 'the', 'child', 'table', 'by', 'the', 'parent', 'variable', 'and', 'calculate', 'statistics', 'across', 'the', 'children', 'of', 'each', 'parent.']), (0.018923464284962395, ['We', 'can', 'create', 'an', 'empty', 'entityset', 'in', 'featuretools', 'using', 'the', 'following:']), (0.01851188128666248, ['we', 'can', 'create', 'features', 'by', 'finding', 'the', 'month', 'of', 'the', 'joined', 'column', 'or', 'taking', 'the', 'natural', 'log', 'of', 'the', 'income', 'column.', 'These', 'are', 'both', 'transformations', 'because', 'they', 'use', 'information', 'from', 'only', 'one', 'table.']), (0.01835863767562323, ['The', 'loans', 'dataframe', 'also', 'has', 'a', 'unique', 'index,', 'loan_id', 'and', 'the', 'syntax', 'to', 'add', 'this', 'to', 'the', 'entityset', 'is', 'the', 'same', 'as', 'for', 'clients.', 'However,', 'for', 'the', 'payments', 'dataframe,', 'there', 'is', 'no', 'unique', 'index.', 'When', 'we', 'add', 'this', 'entity', 'to', 'the', 'entityset,', 'we', 'need', 'to', 'pass', 'in', 'the', 'parameter', 'make_index', '=', 'True', 'and', 'specify', 'the', 'name', 'of', 'the', 'index.', 'Also,', 'although', 'featuretools', 'will', 'automatically', 'infer', 'the', 'data', 'type', 'of', 'each', 'column', 'in', 'an', 'entity,', 'we', 'can', 'override', 'this', 'by', 'passing', 'in', 'a', 'dictionary', 'of', 'column', 'types', 'to', 'the', 'parameter', 'variable_types', '.']), (0.017875909420080223, ['This', 'process', 'involves', 'grouping', 'the', 'loans', 'table', 'by', 'the', 'client,', 'calculating', 'the', 'aggregations,', 'and', 'then', 'merging', 'the', 'resulting', 'data', 'into', 'the', 'client', 'data.', 'Here’s', 'how', 'we', 'would', 'do', 'that', 'in', 'Python', 'using', 'the', 'language', 'of', 'Pandas.']), (0.016969007353685388, ['On', 'the', 'other', 'hand,', 'aggregations', 'are', 'performed', 'across', 'tables,', 'and', 'use', 'a', 'one-to-many', 'relationship', 'to', 'group', 'observations', 'and', 'then', 'calculate', 'statistics.', 'For', 'example,', 'if', 'we', 'have', 'another', 'table', 'with', 'information', 'on', 'the', 'loans', 'of', 'clients,', 'where', 'each', 'client', 'may', 'have', 'multiple', 'loans,', 'we', 'can', 'calculate', 'statistics', 'such', 'as', 'the', 'average,', 'maximum,', 'and', 'minimum', 'of', 'loans', 'for', 'each', 'client.']), (0.01696380181464493, ['For', 'more', 'information', 'on', 'featuretools,', 'including', 'advanced', 'usage,', 'check', 'out', 'the', 'online', 'documentation.', 'To', 'see', 'how', 'featuretools', 'is', 'used', 'in', 'practice,', 'read', 'about', 'the', 'work', 'of', 'Feature', 'Labs,', 'the', 'company', 'behind', 'the', 'open-source', 'library.']), (0.01667242883594439, ['To', 'formalize', 'a', 'relationship', 'in', 'featuretools,', 'we', 'only', 'need', 'to', 'specify', 'the', 'variable', 'that', 'links', 'two', 'tables', 'together.', 'The', 'clients', 'and', 'the', 'loans', 'table', 'are', 'linked', 'via', 'the', 'client_id', 'variable', 'and', 'loans', 'and', 'payments', 'are', 'linked', 'with', 'the', 'loan_id.', 'The', 'syntax', 'for', 'creating', 'a', 'relationship', 'and', 'adding', 'it', 'to', 'the', 'entityset', 'are', 'shown', 'below:']), (0.016550950153796044, ['The', 'best', 'way', 'to', 'think', 'of', 'a', 'relationship', 'between', 'two', 'tables', 'is', 'the', 'analogy', 'of', 'parent', 'to', 'child.', 'This', 'is', 'a', 'one-to-many', 'relationship:', 'each', 'parent', 'can', 'have', 'multiple', 'children.', 'In', 'the', 'realm', 'of', 'tables,', 'a', 'parent', 'table', 'has', 'one', 'row', 'for', 'every', 'parent,', 'but', 'the', 'child', 'table', 'may', 'have', 'multiple', 'rows', 'corresponding', 'to', 'multiple', 'children', 'of', 'the', 'same', 'parent.']), (0.015610371257092626, ['Now', 'we', 'have', 'to', 'add', 'entities.', 'Each', 'entity', 'must', 'have', 'an', 'index,', 'which', 'is', 'a', 'column', 'with', 'all', 'unique', 'elements.', 'That', 'is,', 'each', 'value', 'in', 'the', 'index', 'must', 'appear', 'in', 'the', 'table', 'only', 'once.', 'The', 'index', 'in', 'the', 'clients', 'dataframe', 'is', 'the', 'client_idbecause', 'each', 'client', 'has', 'only', 'one', 'row', 'in', 'this', 'dataframe.', 'We', 'add', 'an', 'entity', 'with', 'an', 'existing', 'index', 'to', 'an', 'entityset', 'using', 'the', 'following', 'syntax:']), (0.015111968052744045, ['Machine', 'learning', 'is', 'increasingly', 'moving', 'from', 'hand-designed', 'models', 'to', 'automatically', 'optimized', 'pipelines', 'using', 'tools', 'such', 'as', 'H20,', 'TPOT,', 'and', 'auto-sklearn.', 'These', 'libraries,', 'along', 'with', 'methods', 'such', 'as', 'random', 'search,', 'aim', 'to', 'simplify', 'the', 'model', 'selection', 'and', 'tuning', 'parts', 'of', 'machine', 'learning', 'by', 'finding', 'the', 'best', 'model', 'for', 'a', 'dataset', 'with', 'little', 'to', 'no', 'manual', 'intervention.', 'However,', 'feature', 'engineering,', 'an', 'arguably', 'more', 'valuable', 'aspect', 'of', 'the', 'machine', 'learning', 'pipeline,', 'remains', 'almost', 'entirely', 'a', 'human', 'labor.']), (0.01337286529835379, ['We', 'can', 'stack', 'features', 'to', 'any', 'depth', 'we', 'want,', 'but', 'in', 'practice,', 'I', 'have', 'never', 'gone', 'beyond', 'a', 'depth', 'of', '2.', 'After', 'this', 'point,', 'the', 'features', 'are', 'difficult', 'to', 'interpret,', 'but', 'I', 'encourage', 'anyone', 'interested', 'to', 'try', '“going', 'deeper”.']), (0.01261824544874555, ['Data', 'Scientist', 'and', 'Master', 'Student,', 'Data', 'Science', 'Communicator', 'and', 'Advocate']), (0.011331052828359944, ['The', 'complete', 'dataframe', 'has', '793', 'columns', 'of', 'new', 'features!']), (0.011240943396544224, ['The', 'column', 'types', 'have', 'been', 'correctly', 'inferred', 'with', 'the', 'modification', 'we', 'specified.', 'Next,', 'we', 'need', 'to', 'specify', 'how', 'the', 'tables', 'in', 'the', 'entityset', 'are', 'related.']), (0.010920084743146595, ['The', 'entityset', 'now', 'contains', 'the', 'three', 'entities', '(tables)', 'and', 'the', 'relationships', 'that', 'link', 'these', 'entities', 'together.', 'After', 'adding', 'entities', 'and', 'formalizing', 'relationships,', 'our', 'entityset', 'is', 'complete', 'and', 'we', 'are', 'ready', 'to', 'make', 'features.']), (0.009826513401733548, ['First,', 'let’s', 'take', 'a', 'look', 'at', 'our', 'example', 'data.', 'We', 'already', 'saw', 'some', 'of', 'the', 'dataset', 'above,', 'and', 'the', 'complete', 'collection', 'of', 'tables', 'is', 'as', 'follows:']), (0.009394999468074702, ['For', 'this', 'dataframe,', 'even', 'though', 'missed', 'is', 'an', 'integer,', 'this', 'is', 'not', 'a', 'numeric', 'variable', 'since', 'it', 'can', 'only', 'take', 'on', '2', 'discrete', 'values,', 'so', 'we', 'tell', 'featuretools', 'to', 'treat', 'is', 'as', 'a', 'categorical', 'variable.', 'After', 'adding', 'the', 'dataframes', 'to', 'the', 'entityset,', 'we', 'inspect', 'any', 'of', 'them:']), (0.008165827000951595, ['We', 'also', 'have', 'a', 'number', 'of', 'aggregation', 'primitives', 'such', 'as', 'the', 'average', 'payment', 'amounts', 'for', 'each', 'client:']), (0.004065549802136271, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.0033112582781457643, ['Sharing', 'concepts,', 'ideas,', 'and', 'codes.']), (0.0033112582781457643, ['As', 'always,', 'I', 'welcome', 'feedback', 'and', 'constructive', 'criticism', 'and', 'can', 'be', 'reached', 'on', 'Twitter', '@koehrsen_will.'])]\n",
      "Summarize Text: \n",
      " Like many topics in machine learning, automated feature engineering with featuretools is a complicated concept built on simple ideas. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables.. Featuretools has built many new features for us to use. While this process does automatically create new features, it will not replace the data scientist because we still have to figure out what to do with all these features. For example, if our goal is to predict whether or not a client will repay a loan, we could look for the features most correlated with a specified outcome. Moreover, if we have domain knowledge, we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features.. Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model.. The curse of dimensionality is combated with feature reduction (also known as feature selection): the process of removing irrelevant features. This can take on many forms: Principal Component Analysis (PCA), SelectKBest, using feature importances from a model, or auto-encoding using deep neural networks. However, feature reduction is a different topic for another article. For now, we know that we can use featuretools to create numerous features from many tables with minimal effort!. New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives):\n",
      "None\n",
      "If your understanding of A.I. and Machine Learning is a big question mark, then this is the blog post for you. Here, I gradually increase your AwesomenessicityTM by gluing inspirational videos together with friendly text.\n",
      "Sit down and relax. These videos take time, and if they don’t inspire you to continue to the next section, fair enough.\n",
      "However, if you find yourself at the bottom of this article, you’ve earned your well-rounded knowledge and passion for this new world. Where you go from there is up to you.\n",
      "A.I. was always cool, from moving a paddle in Pong to lighting you up with combos in Street Fighter.\n",
      "A.I. has always revolved around a programmer’s functional guess at how something should behave. Fun, but programmers aren’t always gifted in programming A.I. as we often see. Just Google “epic game fails” to see glitches in A.I., physics, and sometimes even experienced human players.\n",
      "Regardless, A.I. has a new talent. You can teach a computer to play video games, understand language, and even how to identify people or things. This tip-of-the-iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory.\n",
      "I’m talking about Machine Learning.\n",
      "You don’t need to come up with advanced algorithms anymore. You just have to teach a computer to come up with its own advanced algorithm.\n",
      "So how does something like that even work? An algorithm isn’t really written as much as it is sort of... bred. I’m not using breeding as an analogy. Watch this short video, which gives excellent commentary and animations to the high-level concept of creating the A.I.\n",
      "Wow! Right? That’s a crazy process!\n",
      "Now how is it that we can’t even understand the algorithm when it’s done? One great visual was when the A.I. was written to beat Mario games. As a human, we all understand how to play a side-scroller, but identifying the predictive strategy of the resulting A.I. is insane.\n",
      "Impressed? There’s something amazing about this idea, right? The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games.\n",
      "Fortunately for you, Elon Musk already provided a non-profit company to do the latter. Yes, in a dozen lines of code you can hook up any A.I. you want to countless games/tasks!\n",
      "I have two good answers on why you should care. Firstly, Machine Learning (ML) is making computers do things that we’ve never made computers do before. If you want to do something new, not just new to you, but to the world, you can do it with ML.\n",
      "Secondly, if you don’t influence the world, the world will influence you.\n",
      "Right now significant companies are investing in ML, and we’re already seeing it change the world. Thought-leaders are warning that we can’t let this new age of algorithms exist outside of the public eye. Imagine if a few corporate monoliths controlled the Internet. If we don’t take up arms, the science won’t be ours. I think Christian Heilmann said it best in his talk on ML.\n",
      "The concept is useful and cool. We understand it at a high level, but what the heck is actually happening? How does this work?\n",
      "If you want to jump straight in, I suggest you skip this section and move on to the next “How Do I Get Started” section. If you’re motivated to be a DOer in ML, you won’t need these videos.\n",
      "If you’re still trying to grasp how this could even be a thing, the following video is perfect for walking you through the logic, using the classic ML problem of handwriting.\n",
      "Pretty cool huh? That video shows that each layer gets simpler rather than more complicated. Like the function is chewing data into smaller pieces that end in an abstract concept. You can get your hands dirty in interacting with this process on this site (by Adam Harley).\n",
      "It’s cool watching data go through a trained model, but you can even watch your neural network get trained.\n",
      "One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. In a presentation I attended by JavaFXpert’s overview on Machine Learning, I learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network. You get to watch it train the neural model!\n",
      "Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above.\n",
      "These concepts are exciting! Are you ready to be the Einstein of this new era? Breakthroughs are happening every day, so get started now.\n",
      "There are tons of resources available. I’ll be recommending two approaches.\n",
      "In this approach, you’ll understand Machine Learning down to the algorithms and the math. I know this way sounds tough, but how cool would it be to really get into the details and code this stuff from scratch!\n",
      "If you want to be a force in ML, and hold your own in deep conversations, then this is the route for you.\n",
      "I recommend that you try out Brilliant.org’s app (always great for any science lover) and take the Artificial Neural Network course. This course has no time limits and helps you learn ML while killing time in line on your phone.\n",
      "This one costs money after Level 1.\n",
      "Combine the above with simultaneous enrollment in Andrew Ng’s Stanford course on “Machine Learning in 11 weeks”. This is the course that Jim Weaver recommended in his video above. I’ve also had this course independently suggested to me by Jen Looper.\n",
      "Everyone provides a caveat that this course is tough. For some of you that’s a show stopper, but for others, that’s why you’re going to put yourself through it and collect a certificate saying you did.\n",
      "This course is 100% free. You only have to pay for a certificate if you want one.\n",
      "With those two courses, you’ll have a LOT of work to do. Everyone should be impressed if you make it through because that’s not simple.\n",
      "But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways.\n",
      "If you’re not interested in writing the algorithms, but you want to use them to create the next breathtaking website/app, you should jump into TensorFlow and the crash course.\n",
      "TensorFlow is the de facto open-source software library for machine learning. It can be used in countless ways and even with JavaScript. Here’s a crash course.\n",
      "Plenty more information on available courses and rankings can be found here.\n",
      "If taking a course is not your style, you’re still in luck. You don’t have to learn the nitty-gritty of ML in order to use it today. You can efficiently utilize ML as a service in many ways with tech giants who have trained models ready.\n",
      "I would still caution you that there’s no guarantee that your data is safe or even yours, but the offerings of services for ML are quite attractive!\n",
      "Using an ML service might be the best solution for you if you’re excited and able to upload your data to Amazon/Microsoft/Google. I like to think of these services as a gateway drug to advanced ML. Either way, it’s good to get started now.\n",
      "I have to say thank you to all the aforementioned people and videos. They were my inspiration to get started, and though I’m still a newb in the ML world, I’m happy to light the path for others as we embrace this awe-inspiring age we find ourselves in.\n",
      "It’s imperative to reach out and connect with people if you take up learning this craft. Without friendly faces, answers, and sounding boards, anything can be hard. Just being able to ask and get a response is a game changer. Add me, and add the people mentioned above. Friendly people with friendly advice helps!\n",
      "See?\n",
      "I hope this article has inspired you and those around you to learn ML!\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Software Consultant, Adjunct Professor, Published Author, Award Winning Speaker, Mentor, Organizer and Immature Nerd :D — Lately full of React Native Tech\n",
      "Our community publishes stories worth reading on development, design, and data science.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.03818299506427291, ['Even', 'if', 'you’re', 'not', 'a', 'Java', 'buff,', 'the', 'presentation', 'Jim', 'gives', 'on', 'all', 'things', 'Machine', 'Learning', 'is', 'a', 'pretty', 'cool', '1.5+', 'hour', 'introduction', 'into', 'ML', 'concepts,', 'which', 'includes', 'more', 'info', 'on', 'many', 'of', 'the', 'examples', 'above.']), (0.03568102287237253, ['In', 'this', 'approach,', 'you’ll', 'understand', 'Machine', 'Learning', 'down', 'to', 'the', 'algorithms', 'and', 'the', 'math.', 'I', 'know', 'this', 'way', 'sounds', 'tough,', 'but', 'how', 'cool', 'would', 'it', 'be', 'to', 'really', 'get', 'into', 'the', 'details', 'and', 'code', 'this', 'stuff', 'from', 'scratch!']), (0.035405828537177164, ['If', 'taking', 'a', 'course', 'is', 'not', 'your', 'style,', 'you’re', 'still', 'in', 'luck.', 'You', 'don’t', 'have', 'to', 'learn', 'the', 'nitty-gritty', 'of', 'ML', 'in', 'order', 'to', 'use', 'it', 'today.', 'You', 'can', 'efficiently', 'utilize', 'ML', 'as', 'a', 'service', 'in', 'many', 'ways', 'with', 'tech', 'giants', 'who', 'have', 'trained', 'models', 'ready.']), (0.03506560889886748, ['One', 'of', 'the', 'classic', 'real-world', 'examples', 'of', 'Machine', 'Learning', 'in', 'action', 'is', 'the', 'iris', 'data', 'set', 'from', '1936.', 'In', 'a', 'presentation', 'I', 'attended', 'by', 'JavaFXpert’s', 'overview', 'on', 'Machine', 'Learning,', 'I', 'learned', 'how', 'you', 'can', 'use', 'his', 'tool', 'to', 'visualize', 'the', 'adjustment', 'and', 'back', 'propagation', 'of', 'weights', 'to', 'neurons', 'on', 'a', 'neural', 'network.', 'You', 'get', 'to', 'watch', 'it', 'train', 'the', 'neural', 'model!']), (0.03484160220014717, ['It’s', 'cool', 'watching', 'data', 'go', 'through', 'a', 'trained', 'model,', 'but', 'you', 'can', 'even', 'watch', 'your', 'neural', 'network', 'get', 'trained.']), (0.033802150155645694, ['Using', 'an', 'ML', 'service', 'might', 'be', 'the', 'best', 'solution', 'for', 'you', 'if', 'you’re', 'excited', 'and', 'able', 'to', 'upload', 'your', 'data', 'to', 'Amazon/Microsoft/Google.', 'I', 'like', 'to', 'think', 'of', 'these', 'services', 'as', 'a', 'gateway', 'drug', 'to', 'advanced', 'ML.', 'Either', 'way,', 'it’s', 'good', 'to', 'get', 'started', 'now.']), (0.033669539983235866, ['I', 'have', 'two', 'good', 'answers', 'on', 'why', 'you', 'should', 'care.', 'Firstly,', 'Machine', 'Learning', '(ML)', 'is', 'making', 'computers', 'do', 'things', 'that', 'we’ve', 'never', 'made', 'computers', 'do', 'before.', 'If', 'you', 'want', 'to', 'do', 'something', 'new,', 'not', 'just', 'new', 'to', 'you,', 'but', 'to', 'the', 'world,', 'you', 'can', 'do', 'it', 'with', 'ML.']), (0.03244114197420074, ['Impressed?', 'There’s', 'something', 'amazing', 'about', 'this', 'idea,', 'right?', 'The', 'only', 'problem', 'is', 'we', 'don’t', 'know', 'Machine', 'Learning,', 'and', 'we', 'don’t', 'know', 'how', 'to', 'hook', 'it', 'up', 'to', 'video', 'games.']), (0.03232601236623868, ['Regardless,', 'A.I.', 'has', 'a', 'new', 'talent.', 'You', 'can', 'teach', 'a', 'computer', 'to', 'play', 'video', 'games,', 'understand', 'language,', 'and', 'even', 'how', 'to', 'identify', 'people', 'or', 'things.', 'This', 'tip-of-the-iceberg', 'new', 'skill', 'comes', 'from', 'an', 'old', 'concept', 'that', 'only', 'recently', 'got', 'the', 'processing', 'power', 'to', 'exist', 'outside', 'of', 'theory.']), (0.030920652090610352, ['Now', 'how', 'is', 'it', 'that', 'we', 'can’t', 'even', 'understand', 'the', 'algorithm', 'when', 'it’s', 'done?', 'One', 'great', 'visual', 'was', 'when', 'the', 'A.I.', 'was', 'written', 'to', 'beat', 'Mario', 'games.', 'As', 'a', 'human,', 'we', 'all', 'understand', 'how', 'to', 'play', 'a', 'side-scroller,', 'but', 'identifying', 'the', 'predictive', 'strategy', 'of', 'the', 'resulting', 'A.I.', 'is', 'insane.']), (0.02964789499477432, ['So', 'how', 'does', 'something', 'like', 'that', 'even', 'work?', 'An', 'algorithm', 'isn’t', 'really', 'written', 'as', 'much', 'as', 'it', 'is', 'sort', 'of...', 'bred.', 'I’m', 'not', 'using', 'breeding', 'as', 'an', 'analogy.', 'Watch', 'this', 'short', 'video,', 'which', 'gives', 'excellent', 'commentary', 'and', 'animations', 'to', 'the', 'high-level', 'concept', 'of', 'creating', 'the', 'A.I.']), (0.0289299135476004, ['But', 'more', 'so,', 'if', 'you', 'do', 'make', 'it', 'through,', 'you’ll', 'have', 'a', 'deep', 'understanding', 'of', 'the', 'implementation', 'of', 'Machine', 'Learning', 'that', 'will', 'catapult', 'you', 'into', 'successfully', 'applying', 'it', 'in', 'new', 'and', 'world-changing', 'ways.']), (0.028866668664386846, ['If', 'your', 'understanding', 'of', 'A.I.', 'and', 'Machine', 'Learning', 'is', 'a', 'big', 'question', 'mark,', 'then', 'this', 'is', 'the', 'blog', 'post', 'for', 'you.', 'Here,', 'I', 'gradually', 'increase', 'your', 'AwesomenessicityTM', 'by', 'gluing', 'inspirational', 'videos', 'together', 'with', 'friendly', 'text.']), (0.028739422715390244, ['If', 'you’re', 'still', 'trying', 'to', 'grasp', 'how', 'this', 'could', 'even', 'be', 'a', 'thing,', 'the', 'following', 'video', 'is', 'perfect', 'for', 'walking', 'you', 'through', 'the', 'logic,', 'using', 'the', 'classic', 'ML', 'problem', 'of', 'handwriting.']), (0.028135877356636037, ['TensorFlow', 'is', 'the', 'de', 'facto', 'open-source', 'software', 'library', 'for', 'machine', 'learning.', 'It', 'can', 'be', 'used', 'in', 'countless', 'ways', 'and', 'even', 'with', 'JavaScript.', 'Here’s', 'a', 'crash', 'course.']), (0.026697878186773247, ['I', 'would', 'still', 'caution', 'you', 'that', 'there’s', 'no', 'guarantee', 'that', 'your', 'data', 'is', 'safe', 'or', 'even', 'yours,', 'but', 'the', 'offerings', 'of', 'services', 'for', 'ML', 'are', 'quite', 'attractive!']), (0.026213061016153248, ['If', 'you', 'want', 'to', 'jump', 'straight', 'in,', 'I', 'suggest', 'you', 'skip', 'this', 'section', 'and', 'move', 'on', 'to', 'the', 'next', '“How', 'Do', 'I', 'Get', 'Started”', 'section.', 'If', 'you’re', 'motivated', 'to', 'be', 'a', 'DOer', 'in', 'ML,', 'you', 'won’t', 'need', 'these', 'videos.']), (0.02590216048218244, ['Everyone', 'provides', 'a', 'caveat', 'that', 'this', 'course', 'is', 'tough.', 'For', 'some', 'of', 'you', 'that’s', 'a', 'show', 'stopper,', 'but', 'for', 'others,', 'that’s', 'why', 'you’re', 'going', 'to', 'put', 'yourself', 'through', 'it', 'and', 'collect', 'a', 'certificate', 'saying', 'you', 'did.']), (0.02501672755639224, ['I', 'have', 'to', 'say', 'thank', 'you', 'to', 'all', 'the', 'aforementioned', 'people', 'and', 'videos.', 'They', 'were', 'my', 'inspiration', 'to', 'get', 'started,', 'and', 'though', 'I’m', 'still', 'a', 'newb', 'in', 'the', 'ML', 'world,', 'I’m', 'happy', 'to', 'light', 'the', 'path', 'for', 'others', 'as', 'we', 'embrace', 'this', 'awe-inspiring', 'age', 'we', 'find', 'ourselves', 'in.']), (0.024434180862027748, ['I’m', 'talking', 'about', 'Machine', 'Learning.']), (0.023579052881112447, ['If', 'you’re', 'not', 'interested', 'in', 'writing', 'the', 'algorithms,', 'but', 'you', 'want', 'to', 'use', 'them', 'to', 'create', 'the', 'next', 'breathtaking', 'website/app,', 'you', 'should', 'jump', 'into', 'TensorFlow', 'and', 'the', 'crash', 'course.']), (0.02253254043854411, ['I', 'recommend', 'that', 'you', 'try', 'out', 'Brilliant.org’s', 'app', '(always', 'great', 'for', 'any', 'science', 'lover)', 'and', 'take', 'the', 'Artificial', 'Neural', 'Network', 'course.', 'This', 'course', 'has', 'no', 'time', 'limits', 'and', 'helps', 'you', 'learn', 'ML', 'while', 'killing', 'time', 'in', 'line', 'on', 'your', 'phone.']), (0.022312707108302174, ['A.I.', 'has', 'always', 'revolved', 'around', 'a', 'programmer’s', 'functional', 'guess', 'at', 'how', 'something', 'should', 'behave.', 'Fun,', 'but', 'programmers', 'aren’t', 'always', 'gifted', 'in', 'programming', 'A.I.', 'as', 'we', 'often', 'see.', 'Just', 'Google', '“epic', 'game', 'fails”', 'to', 'see', 'glitches', 'in', 'A.I.,', 'physics,', 'and', 'sometimes', 'even', 'experienced', 'human', 'players.']), (0.021543360366732732, ['Right', 'now', 'significant', 'companies', 'are', 'investing', 'in', 'ML,', 'and', 'we’re', 'already', 'seeing', 'it', 'change', 'the', 'world.', 'Thought-leaders', 'are', 'warning', 'that', 'we', 'can’t', 'let', 'this', 'new', 'age', 'of', 'algorithms', 'exist', 'outside', 'of', 'the', 'public', 'eye.', 'Imagine', 'if', 'a', 'few', 'corporate', 'monoliths', 'controlled', 'the', 'Internet.', 'If', 'we', 'don’t', 'take', 'up', 'arms,', 'the', 'science', 'won’t', 'be', 'ours.', 'I', 'think', 'Christian', 'Heilmann', 'said', 'it', 'best', 'in', 'his', 'talk', 'on', 'ML.']), (0.020497280712045075, ['Combine', 'the', 'above', 'with', 'simultaneous', 'enrollment', 'in', 'Andrew', 'Ng’s', 'Stanford', 'course', 'on', '“Machine', 'Learning', 'in', '11', 'weeks”.', 'This', 'is', 'the', 'course', 'that', 'Jim', 'Weaver', 'recommended', 'in', 'his', 'video', 'above.', 'I’ve', 'also', 'had', 'this', 'course', 'independently', 'suggested', 'to', 'me', 'by', 'Jen', 'Looper.']), (0.02010118523087008, ['This', 'course', 'is', '100%', 'free.', 'You', 'only', 'have', 'to', 'pay', 'for', 'a', 'certificate', 'if', 'you', 'want', 'one.']), (0.0193227701460445, ['It’s', 'imperative', 'to', 'reach', 'out', 'and', 'connect', 'with', 'people', 'if', 'you', 'take', 'up', 'learning', 'this', 'craft.', 'Without', 'friendly', 'faces,', 'answers,', 'and', 'sounding', 'boards,', 'anything', 'can', 'be', 'hard.', 'Just', 'being', 'able', 'to', 'ask', 'and', 'get', 'a', 'response', 'is', 'a', 'game', 'changer.', 'Add', 'me,', 'and', 'add', 'the', 'people', 'mentioned', 'above.', 'Friendly', 'people', 'with', 'friendly', 'advice', 'helps!']), (0.019198601074682606, ['If', 'you', 'want', 'to', 'be', 'a', 'force', 'in', 'ML,', 'and', 'hold', 'your', 'own', 'in', 'deep', 'conversations,', 'then', 'this', 'is', 'the', 'route', 'for', 'you.']), (0.018974460907569907, ['Fortunately', 'for', 'you,', 'Elon', 'Musk', 'already', 'provided', 'a', 'non-profit', 'company', 'to', 'do', 'the', 'latter.', 'Yes,', 'in', 'a', 'dozen', 'lines', 'of', 'code', 'you', 'can', 'hook', 'up', 'any', 'A.I.', 'you', 'want', 'to', 'countless', 'games/tasks!']), (0.018267030306453134, ['Pretty', 'cool', 'huh?', 'That', 'video', 'shows', 'that', 'each', 'layer', 'gets', 'simpler', 'rather', 'than', 'more', 'complicated.', 'Like', 'the', 'function', 'is', 'chewing', 'data', 'into', 'smaller', 'pieces', 'that', 'end', 'in', 'an', 'abstract', 'concept.', 'You', 'can', 'get', 'your', 'hands', 'dirty', 'in', 'interacting', 'with', 'this', 'process', 'on', 'this', 'site', '(by', 'Adam', 'Harley).']), (0.01797620142183263, ['With', 'those', 'two', 'courses,', 'you’ll', 'have', 'a', 'LOT', 'of', 'work', 'to', 'do.', 'Everyone', 'should', 'be', 'impressed', 'if', 'you', 'make', 'it', 'through', 'because', 'that’s', 'not', 'simple.']), (0.017353885161671, ['Secondly,', 'if', 'you', 'don’t', 'influence', 'the', 'world,', 'the', 'world', 'will', 'influence', 'you.']), (0.017344283413277484, ['These', 'concepts', 'are', 'exciting!', 'Are', 'you', 'ready', 'to', 'be', 'the', 'Einstein', 'of', 'this', 'new', 'era?', 'Breakthroughs', 'are', 'happening', 'every', 'day,', 'so', 'get', 'started', 'now.']), (0.01562960229209005, ['However,', 'if', 'you', 'find', 'yourself', 'at', 'the', 'bottom', 'of', 'this', 'article,', 'you’ve', 'earned', 'your', 'well-rounded', 'knowledge', 'and', 'passion', 'for', 'this', 'new', 'world.', 'Where', 'you', 'go', 'from', 'there', 'is', 'up', 'to', 'you.']), (0.01543515436135704, ['Sit', 'down', 'and', 'relax.', 'These', 'videos', 'take', 'time,', 'and', 'if', 'they', 'don’t', 'inspire', 'you', 'to', 'continue', 'to', 'the', 'next', 'section,', 'fair', 'enough.']), (0.014716576171525539, ['You', 'don’t', 'need', 'to', 'come', 'up', 'with', 'advanced', 'algorithms', 'anymore.', 'You', 'just', 'have', 'to', 'teach', 'a', 'computer', 'to', 'come', 'up', 'with', 'its', 'own', 'advanced', 'algorithm.']), (0.013018371580967861, ['A.I.', 'was', 'always', 'cool,', 'from', 'moving', 'a', 'paddle', 'in', 'Pong', 'to', 'lighting', 'you', 'up', 'with', 'combos', 'in', 'Street', 'Fighter.']), (0.011433244675981495, ['Wow!', 'Right?', 'That’s', 'a', 'crazy', 'process!']), (0.010043271420142337, ['The', 'concept', 'is', 'useful', 'and', 'cool.', 'We', 'understand', 'it', 'at', 'a', 'high', 'level,', 'but', 'what', 'the', 'heck', 'is', 'actually', 'happening?', 'How', 'does', 'this', 'work?']), (0.008847552781326009, ['Our', 'community', 'publishes', 'stories', 'worth', 'reading', 'on', 'development,', 'design,', 'and', 'data', 'science.']), (0.007077661361549723, ['I', 'hope', 'this', 'article', 'has', 'inspired', 'you', 'and', 'those', 'around', 'you', 'to', 'learn', 'ML!']), (0.006831287524166432, ['There', 'are', 'tons', 'of', 'resources', 'available.', 'I’ll', 'be', 'recommending', 'two', 'approaches.']), (0.005781244369967941, ['This', 'one', 'costs', 'money', 'after', 'Level', '1.']), (0.005757091136271214, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.0048827270741395225, ['Software', 'Consultant,', 'Adjunct', 'Professor,', 'Published', 'Author,', 'Award', 'Winning', 'Speaker,', 'Mentor,', 'Organizer', 'and', 'Immature', 'Nerd', ':D', '—', 'Lately', 'full', 'of', 'React', 'Native', 'Tech']), (0.003311258278145696, ['See?']), (0.003311258278145696, ['Plenty', 'more', 'information', 'on', 'available', 'courses', 'and', 'rankings', 'can', 'be', 'found', 'here.'])]\n",
      "Summarize Text: \n",
      " Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above.. In this approach, you’ll understand Machine Learning down to the algorithms and the math. I know this way sounds tough, but how cool would it be to really get into the details and code this stuff from scratch!. If taking a course is not your style, you’re still in luck. You don’t have to learn the nitty-gritty of ML in order to use it today. You can efficiently utilize ML as a service in many ways with tech giants who have trained models ready.. One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. In a presentation I attended by JavaFXpert’s overview on Machine Learning, I learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network. You get to watch it train the neural model!. It’s cool watching data go through a trained model, but you can even watch your neural network get trained.\n",
      "None\n",
      "Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.\n",
      "Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.\n",
      "Recently, I gave a talk at the O’Reilly AI conference in Beijing about some of the interesting lessons we’ve learned in the world of NLP. While there, I was lucky enough to attend a tutorial on Deep Reinforcement Learning (Deep RL) from scratch by Unity Technologies. I thought that the session, led by Arthur Juliani, was extremely informative and wanted to share some big takeaways below.\n",
      "In our conversations with companies, we’ve seen a rise of interesting Deep RL applications, tools and results. In parallel, the inner workings and applications of Deep RL, such as AlphaGo pictured above, can often seem esoteric and hard to understand. In this post, I will give an overview of core aspects of the field that can be understood by anyone.\n",
      "Many of the visuals are from the slides of the talk, and some are new. The explanations and opinions are mine. If anything is unclear, reach out to me here!\n",
      "Deep RL is a field that has seen vast amounts of research interest, including learning to play Atari games, beating pro players at Dota 2, and defeating Go champions. Contrary to many classical Deep Learning problems that often focus on perception (does this image contain a stop sign?), Deep RL adds the dimension of actions that influence the environment (what is the goal, and how do I get there?). In dialog systems for example, classical Deep Learning aims to learn the right response for a given query. On the other hand, Deep Reinforcement Learning focuses on the right sequences of sentences that will lead to a positive outcome, for example a happy customer.\n",
      "This makes Deep RL particularly attractive for tasks that require planning and adaptation, such as manufacturing or self-driving. However, industry applications have trailed behind the rapidly advancing results coming out of the research community. A major reason is that Deep RL often requires an agent to experiment millions of times before learning anything useful. The best way to do this rapidly is by using a simulation environment. This tutorial will be using Unity to create environments to train agents in.\n",
      "For this workshop led by Arthur Juliani and Leon Chen, their goal was to get every participants to successfully train multiple Deep RL algorithms in 4 hours. A tall order! Below, is a comprehensive overview of many of the main algorithms that power Deep RL today. For a more complete set of tutorials, Arthur Juliani wrote an 8-part series starting here.\n",
      "Deep RL can be used to best the top human players at Go, but to understand how that’s done, you first need to understand a few simple concepts, starting with much easier problems.\n",
      "1/It all starts with slot machines\n",
      "Let’s imagine you are faced with 4 chests that you can pick from at each turn. Each of them have a different average payout, and your goal is to maximize the total payout you receive after a fixed number of turns. This is a classic problem called Multi-armed bandits and is where we will start. The crux of the problem is to balance exploration, which helps us learn about which states are good, and exploitation, where we now use what we know to pick the best slot machine.\n",
      "Here, we will utilize a value function that maps our actions to an estimated reward, called the Q function. First, we’ll initialize all Q values at equal values. Then, we’ll update the Q value of each action (picking each chest) based on how good the payout was after choosing this action. This allows us to learn a good value function. We will approximate our Q function using a neural network (starting with a very shallow one) that learns a probability distribution (by using a softmax) over the 4 potential chests.\n",
      "While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. Intuitively, we might want to use a policy that picks the action with the highest Q value. This performs poorly in practice, as our Q estimates will be very wrong at the start before we gather enough experience through trial and error. This is why we need to add a mechanism to our policy to encourage exploration. One way to do that is to use epsilon greedy, which consists of taking a random action with probability epsilon. We start with epsilon being close to 1, always choosing random actions, and lower epsilon as we go along and learn more about which chests are good. Eventually, we learn which chests are best.\n",
      "In practice, we might want to take a more subtle approach than either taking the action we think is the best, or a random action. A popular method is Boltzmann Exploration, which adjust probabilities based on our current estimate of how good each chest is, adding in a randomness factor.\n",
      "2/Adding different states\n",
      "The previous example was a world in which we were always in the same state, waiting to pick from the same 4 chests in front of us. Most real-word problems consist of many different states. That is what we will add to our environment next. Now, the background behind chests alternates between 3 colors at each turn, changing the average values of the chests. This means we need to learn a Q function that depends not only on the action (the chest we pick), but the state (what the color of the background is). This version of the problem is called Contextual Multi-armed Bandits.\n",
      "Surprisingly, we can use the same approach as before. The only thing we need to add is an extra dense layer to our neural network, that will take in as input a vector representing the current state of the world.\n",
      "3/Learning about the consequences of our actions\n",
      "There is another key factor that makes our current problem simpler than mosts. In most environments, such as in the maze depicted above, the actions that we take have an impact on the state of the world. If we move up on this grid, we might receive a reward or we might receive nothing, but the next turn we will be in a different state. This is where we finally introduce a need for planning.\n",
      "First, we will define our Q function as the immediate reward in our current state, plus the discounted reward we are expecting by taking all of our future actions. This solution works if our Q estimate of states is accurate, so how can we learn a good estimate?\n",
      "We will use a method called Temporal Difference (TD) learning to learn a good Q function. The idea is to only look at a limited number of steps in the future. TD(1) for example, only uses the next 2 states to evaluate the reward.\n",
      "Surprisingly, we can use TD(0), which looks at the current state, and our estimate of the reward the next turn, and get great results. The structure of the network is the same, but we need to go through one forward step before receiving the error. We then use this error to back propagate gradients, like in traditional Deep Learning, and update our value estimates.\n",
      "3+/Introducing Monte Carlo\n",
      "Another method to estimate the eventual success of our actions is Monte Carlo Estimates. This consists of playing out the entire episode with our current policy until we reach an end (success by reaching a green block or failure by reaching a red block in the image above) and use that result to update our value estimates for each traversed state. This allows us to propagate values efficiently in one batch at the end of an episode, instead of every time we make a move. The cost is that we are introducing noise to our estimates, since we attribute very distant rewards to them.\n",
      "4/The world is rarely discrete\n",
      "The previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value. In the maze for example, there were 49 states (squares) and 4 actions (move in each adjacent direction). In this environment, we are trying to learn how to balance a ball on a 2 dimensional paddle, by deciding at each time step whether we want to tilt the paddle left or right. Here, the state space becomes continuous (the angle of the paddle, and the position of the ball). The good news is, we can still use Neural Networks to approximate this function!\n",
      "A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). This constrains our learning process, as we have to have an exploration strategy that is built in to the policy itself, but allows us to tie results directly to our reasoning, and enables us to learn more efficiently.\n",
      "The approach we will use here is called Policy Gradients, and is an on-policy method. Previously, we were first learning a value function Q for each action in each state and then building a policy on top. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. Since we are learning on policy, we cannot use methods such as epsilon greedy (which includes random choices), to get our agent to explore the environment. The way that we encourage exploration is by using a method called entropy regularization, which pushes our probability estimates to be wider, and thus will encourage us to make riskier choices to explore the space.\n",
      "4+/Leveraging deep learning for representations\n",
      "In practice, many state of the art RL methods require learning both a policy and value estimates. The way we do this with deep learning is by having both be two separate outputs of the same backbone neural network, which will make it easier for our neural network to learn good representations.\n",
      "One method to do this is Advantage Actor Critic (A2C). We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. Instead of updating our value function based on rewards, we update it based on our advantage, which measures how much better or worse an action was than our previous value function estimated it to be. This helps make learning more stable compared to simple Q Learning and Vanilla Policy Gradients.\n",
      "5/Learning directly from the screen\n",
      "There is an additional advantage to using Deep Learning for these methods, which is that Deep Neural Networks excel at perceptive tasks. When a human plays a game, the information received is not a list of states, but an image (usually of a screen, or a board, or the surrounding environment).\n",
      "Image-based Learning combines a Convolutional Neural Network (CNN) with RL. In this environment, we pass in a raw image instead of features, and add a 2 layer CNN to our architecture without changing anything else! We can even inspect activations to see what the network picks up on to determine value, and policy. In the example below, we can see that the network uses the current score and distant obstacles to estimate the value of the current state, while focusing on nearby obstacles for determining actions. Neat!\n",
      "As a side note, while toying around with the provided implementation, I’ve found that visual learning is very sensitive to hyperparameters. Changing the discount rate slightly for example, completely prevented the neural network from learning even on a toy application. This is a widely known problem, but it is interesting to see it first hand.\n",
      "6/Nuanced actions\n",
      "So far, we’ve played with environments with continuous and discrete state spaces. However, every environment we studied had a discrete action space: we could move in one of four directions, or tilt the paddle to the left or right. Ideally, for applications such as self-driving cars, we would like to learn continuous actions, such as turning the steering wheel between 0 and 360 degrees. In this environment called 3D ball world, we can choose to tilt the paddle to any value on each of its axes. This gives us more control as to how we perform actions, but makes the action space much larger.\n",
      "We can approach this by approximating our potential choices with Gaussian distributions. We learn a probability distribution over potential actions by learning the mean and variance of a Gaussian distribution, and our policy we sample from that distribution. Simple, in theory :).\n",
      "7/Next steps for the brave\n",
      "There are a few concepts that separate the algorithms described above from state of the art approaches. It’s interesting to see that conceptually, the best robotics and game-playing algorithms are not that far away from the ones we just explored:\n",
      "That’s it for this overview, I hope this has been informative and fun! If you are looking to dive deeper into the theory of RL, give Arthur’s posts a read, or diving deeper by following David Silver’s UCL course. If you are looking to learn more about the projects we do at Insight, or how we work with companies, please check us out below, or reach out to me here.\n",
      "Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.\n",
      "Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "AI Lead at Insight AI @EmmanuelAmeisen\n",
      "Insight Fellows Program - Your bridge to a career in data\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.044717626929950786, ['In', 'practice,', 'many', 'state', 'of', 'the', 'art', 'RL', 'methods', 'require', 'learning', 'both', 'a', 'policy', 'and', 'value', 'estimates.', 'The', 'way', 'we', 'do', 'this', 'with', 'deep', 'learning', 'is', 'by', 'having', 'both', 'be', 'two', 'separate', 'outputs', 'of', 'the', 'same', 'backbone', 'neural', 'network,', 'which', 'will', 'make', 'it', 'easier', 'for', 'our', 'neural', 'network', 'to', 'learn', 'good', 'representations.']), (0.043191571319535334, ['The', 'approach', 'we', 'will', 'use', 'here', 'is', 'called', 'Policy', 'Gradients,', 'and', 'is', 'an', 'on-policy', 'method.', 'Previously,', 'we', 'were', 'first', 'learning', 'a', 'value', 'function', 'Q', 'for', 'each', 'action', 'in', 'each', 'state', 'and', 'then', 'building', 'a', 'policy', 'on', 'top.', 'In', 'Vanilla', 'Policy', 'Gradient,', 'we', 'still', 'use', 'Monte', 'Carlo', 'Estimates,', 'but', 'we', 'learn', 'our', 'policy', 'directly', 'through', 'a', 'loss', 'function', 'that', 'increases', 'the', 'probability', 'of', 'choosing', 'rewarding', 'actions.', 'Since', 'we', 'are', 'learning', 'on', 'policy,', 'we', 'cannot', 'use', 'methods', 'such', 'as', 'epsilon', 'greedy', '(which', 'includes', 'random', 'choices),', 'to', 'get', 'our', 'agent', 'to', 'explore', 'the', 'environment.', 'The', 'way', 'that', 'we', 'encourage', 'exploration', 'is', 'by', 'using', 'a', 'method', 'called', 'entropy', 'regularization,', 'which', 'pushes', 'our', 'probability', 'estimates', 'to', 'be', 'wider,', 'and', 'thus', 'will', 'encourage', 'us', 'to', 'make', 'riskier', 'choices', 'to', 'explore', 'the', 'space.']), (0.04043451049857401, ['One', 'method', 'to', 'do', 'this', 'is', 'Advantage', 'Actor', 'Critic', '(A2C).', 'We', 'learn', 'our', 'policy', 'directly', 'with', 'policy', 'gradients', '(defined', 'above),', 'and', 'learn', 'a', 'value', 'function', 'using', 'something', 'called', 'Advantage.', 'Instead', 'of', 'updating', 'our', 'value', 'function', 'based', 'on', 'rewards,', 'we', 'update', 'it', 'based', 'on', 'our', 'advantage,', 'which', 'measures', 'how', 'much', 'better', 'or', 'worse', 'an', 'action', 'was', 'than', 'our', 'previous', 'value', 'function', 'estimated', 'it', 'to', 'be.', 'This', 'helps', 'make', 'learning', 'more', 'stable', 'compared', 'to', 'simple', 'Q', 'Learning', 'and', 'Vanilla', 'Policy', 'Gradients.']), (0.038038469063490954, ['While', 'the', 'value', 'function', 'tells', 'us', 'how', 'good', 'we', 'estimate', 'each', 'action', 'to', 'be,', 'the', 'policy', 'is', 'the', 'function', 'that', 'determines', 'which', 'actions', 'we', 'end', 'up', 'taking.', 'Intuitively,', 'we', 'might', 'want', 'to', 'use', 'a', 'policy', 'that', 'picks', 'the', 'action', 'with', 'the', 'highest', 'Q', 'value.', 'This', 'performs', 'poorly', 'in', 'practice,', 'as', 'our', 'Q', 'estimates', 'will', 'be', 'very', 'wrong', 'at', 'the', 'start', 'before', 'we', 'gather', 'enough', 'experience', 'through', 'trial', 'and', 'error.', 'This', 'is', 'why', 'we', 'need', 'to', 'add', 'a', 'mechanism', 'to', 'our', 'policy', 'to', 'encourage', 'exploration.', 'One', 'way', 'to', 'do', 'that', 'is', 'to', 'use', 'epsilon', 'greedy,', 'which', 'consists', 'of', 'taking', 'a', 'random', 'action', 'with', 'probability', 'epsilon.', 'We', 'start', 'with', 'epsilon', 'being', 'close', 'to', '1,', 'always', 'choosing', 'random', 'actions,', 'and', 'lower', 'epsilon', 'as', 'we', 'go', 'along', 'and', 'learn', 'more', 'about', 'which', 'chests', 'are', 'good.', 'Eventually,', 'we', 'learn', 'which', 'chests', 'are', 'best.']), (0.037042273187238524, ['Deep', 'RL', 'is', 'a', 'field', 'that', 'has', 'seen', 'vast', 'amounts', 'of', 'research', 'interest,', 'including', 'learning', 'to', 'play', 'Atari', 'games,', 'beating', 'pro', 'players', 'at', 'Dota', '2,', 'and', 'defeating', 'Go', 'champions.', 'Contrary', 'to', 'many', 'classical', 'Deep', 'Learning', 'problems', 'that', 'often', 'focus', 'on', 'perception', '(does', 'this', 'image', 'contain', 'a', 'stop', 'sign?),', 'Deep', 'RL', 'adds', 'the', 'dimension', 'of', 'actions', 'that', 'influence', 'the', 'environment', '(what', 'is', 'the', 'goal,', 'and', 'how', 'do', 'I', 'get', 'there?).', 'In', 'dialog', 'systems', 'for', 'example,', 'classical', 'Deep', 'Learning', 'aims', 'to', 'learn', 'the', 'right', 'response', 'for', 'a', 'given', 'query.', 'On', 'the', 'other', 'hand,', 'Deep', 'Reinforcement', 'Learning', 'focuses', 'on', 'the', 'right', 'sequences', 'of', 'sentences', 'that', 'will', 'lead', 'to', 'a', 'positive', 'outcome,', 'for', 'example', 'a', 'happy', 'customer.']), (0.036384440975107915, ['A', 'note', 'about', 'off-policy', 'vs', 'on-policy', 'learning:', 'The', 'methods', 'we', 'used', 'previously,', 'are', 'off-policy', 'methods,', 'meaning', 'we', 'can', 'generate', 'data', 'with', 'any', 'strategy(using', 'epsilon', 'greedy', 'for', 'example)', 'and', 'learn', 'from', 'it.', 'On-policy', 'methods', 'can', 'only', 'learn', 'from', 'actions', 'that', 'were', 'taken', 'following', 'our', 'policy', '(remember,', 'a', 'policy', 'is', 'the', 'method', 'we', 'use', 'to', 'determine', 'which', 'actions', 'to', 'take).', 'This', 'constrains', 'our', 'learning', 'process,', 'as', 'we', 'have', 'to', 'have', 'an', 'exploration', 'strategy', 'that', 'is', 'built', 'in', 'to', 'the', 'policy', 'itself,', 'but', 'allows', 'us', 'to', 'tie', 'results', 'directly', 'to', 'our', 'reasoning,', 'and', 'enables', 'us', 'to', 'learn', 'more', 'efficiently.']), (0.033788046231147575, ['Here,', 'we', 'will', 'utilize', 'a', 'value', 'function', 'that', 'maps', 'our', 'actions', 'to', 'an', 'estimated', 'reward,', 'called', 'the', 'Q', 'function.', 'First,', 'we’ll', 'initialize', 'all', 'Q', 'values', 'at', 'equal', 'values.', 'Then,', 'we’ll', 'update', 'the', 'Q', 'value', 'of', 'each', 'action', '(picking', 'each', 'chest)', 'based', 'on', 'how', 'good', 'the', 'payout', 'was', 'after', 'choosing', 'this', 'action.', 'This', 'allows', 'us', 'to', 'learn', 'a', 'good', 'value', 'function.', 'We', 'will', 'approximate', 'our', 'Q', 'function', 'using', 'a', 'neural', 'network', '(starting', 'with', 'a', 'very', 'shallow', 'one)', 'that', 'learns', 'a', 'probability', 'distribution', '(by', 'using', 'a', 'softmax)', 'over', 'the', '4', 'potential', 'chests.']), (0.033641701567404036, ['We', 'will', 'use', 'a', 'method', 'called', 'Temporal', 'Difference', '(TD)', 'learning', 'to', 'learn', 'a', 'good', 'Q', 'function.', 'The', 'idea', 'is', 'to', 'only', 'look', 'at', 'a', 'limited', 'number', 'of', 'steps', 'in', 'the', 'future.', 'TD(1)', 'for', 'example,', 'only', 'uses', 'the', 'next', '2', 'states', 'to', 'evaluate', 'the', 'reward.']), (0.03338958575897854, ['The', 'previous', 'methods', 'were', 'using', 'neural', 'networks', 'to', 'approximate', 'our', 'value', 'estimates', 'by', 'mapping', 'from', 'a', 'discrete', 'number', 'of', 'states', 'and', 'actions', 'to', 'a', 'value.', 'In', 'the', 'maze', 'for', 'example,', 'there', 'were', '49', 'states', '(squares)', 'and', '4', 'actions', '(move', 'in', 'each', 'adjacent', 'direction).', 'In', 'this', 'environment,', 'we', 'are', 'trying', 'to', 'learn', 'how', 'to', 'balance', 'a', 'ball', 'on', 'a', '2', 'dimensional', 'paddle,', 'by', 'deciding', 'at', 'each', 'time', 'step', 'whether', 'we', 'want', 'to', 'tilt', 'the', 'paddle', 'left', 'or', 'right.', 'Here,', 'the', 'state', 'space', 'becomes', 'continuous', '(the', 'angle', 'of', 'the', 'paddle,', 'and', 'the', 'position', 'of', 'the', 'ball).', 'The', 'good', 'news', 'is,', 'we', 'can', 'still', 'use', 'Neural', 'Networks', 'to', 'approximate', 'this', 'function!']), (0.028491359502821185, ['Surprisingly,', 'we', 'can', 'use', 'TD(0),', 'which', 'looks', 'at', 'the', 'current', 'state,', 'and', 'our', 'estimate', 'of', 'the', 'reward', 'the', 'next', 'turn,', 'and', 'get', 'great', 'results.', 'The', 'structure', 'of', 'the', 'network', 'is', 'the', 'same,', 'but', 'we', 'need', 'to', 'go', 'through', 'one', 'forward', 'step', 'before', 'receiving', 'the', 'error.', 'We', 'then', 'use', 'this', 'error', 'to', 'back', 'propagate', 'gradients,', 'like', 'in', 'traditional', 'Deep', 'Learning,', 'and', 'update', 'our', 'value', 'estimates.']), (0.027197725468148765, ['Want', 'to', 'learn', 'about', 'applied', 'Artificial', 'Intelligence', 'from', 'leading', 'practitioners', 'in', 'Silicon', 'Valley,', 'New', 'York,', 'or', 'Toronto?', 'Learn', 'more', 'about', 'the', 'Insight', 'Artificial', 'Intelligence', 'Fellows', 'Program.']), (0.027197725468148765, ['Want', 'to', 'learn', 'about', 'applied', 'Artificial', 'Intelligence', 'from', 'leading', 'practitioners', 'in', 'Silicon', 'Valley,', 'New', 'York,', 'or', 'Toronto?', 'Learn', 'more', 'about', 'the', 'Insight', 'Artificial', 'Intelligence', 'Fellows', 'Program.']), (0.02718359425963634, ['4+/Leveraging', 'deep', 'learning', 'for', 'representations']), (0.026256418825724097, ['The', 'previous', 'example', 'was', 'a', 'world', 'in', 'which', 'we', 'were', 'always', 'in', 'the', 'same', 'state,', 'waiting', 'to', 'pick', 'from', 'the', 'same', '4', 'chests', 'in', 'front', 'of', 'us.', 'Most', 'real-word', 'problems', 'consist', 'of', 'many', 'different', 'states.', 'That', 'is', 'what', 'we', 'will', 'add', 'to', 'our', 'environment', 'next.', 'Now,', 'the', 'background', 'behind', 'chests', 'alternates', 'between', '3', 'colors', 'at', 'each', 'turn,', 'changing', 'the', 'average', 'values', 'of', 'the', 'chests.', 'This', 'means', 'we', 'need', 'to', 'learn', 'a', 'Q', 'function', 'that', 'depends', 'not', 'only', 'on', 'the', 'action', '(the', 'chest', 'we', 'pick),', 'but', 'the', 'state', '(what', 'the', 'color', 'of', 'the', 'background', 'is).', 'This', 'version', 'of', 'the', 'problem', 'is', 'called', 'Contextual', 'Multi-armed', 'Bandits.']), (0.025378559890759628, ['Another', 'method', 'to', 'estimate', 'the', 'eventual', 'success', 'of', 'our', 'actions', 'is', 'Monte', 'Carlo', 'Estimates.', 'This', 'consists', 'of', 'playing', 'out', 'the', 'entire', 'episode', 'with', 'our', 'current', 'policy', 'until', 'we', 'reach', 'an', 'end', '(success', 'by', 'reaching', 'a', 'green', 'block', 'or', 'failure', 'by', 'reaching', 'a', 'red', 'block', 'in', 'the', 'image', 'above)', 'and', 'use', 'that', 'result', 'to', 'update', 'our', 'value', 'estimates', 'for', 'each', 'traversed', 'state.', 'This', 'allows', 'us', 'to', 'propagate', 'values', 'efficiently', 'in', 'one', 'batch', 'at', 'the', 'end', 'of', 'an', 'episode,', 'instead', 'of', 'every', 'time', 'we', 'make', 'a', 'move.', 'The', 'cost', 'is', 'that', 'we', 'are', 'introducing', 'noise', 'to', 'our', 'estimates,', 'since', 'we', 'attribute', 'very', 'distant', 'rewards', 'to', 'them.']), (0.025320123782103524, ['Let’s', 'imagine', 'you', 'are', 'faced', 'with', '4', 'chests', 'that', 'you', 'can', 'pick', 'from', 'at', 'each', 'turn.', 'Each', 'of', 'them', 'have', 'a', 'different', 'average', 'payout,', 'and', 'your', 'goal', 'is', 'to', 'maximize', 'the', 'total', 'payout', 'you', 'receive', 'after', 'a', 'fixed', 'number', 'of', 'turns.', 'This', 'is', 'a', 'classic', 'problem', 'called', 'Multi-armed', 'bandits', 'and', 'is', 'where', 'we', 'will', 'start.', 'The', 'crux', 'of', 'the', 'problem', 'is', 'to', 'balance', 'exploration,', 'which', 'helps', 'us', 'learn', 'about', 'which', 'states', 'are', 'good,', 'and', 'exploitation,', 'where', 'we', 'now', 'use', 'what', 'we', 'know', 'to', 'pick', 'the', 'best', 'slot', 'machine.']), (0.025183062993020897, ['First,', 'we', 'will', 'define', 'our', 'Q', 'function', 'as', 'the', 'immediate', 'reward', 'in', 'our', 'current', 'state,', 'plus', 'the', 'discounted', 'reward', 'we', 'are', 'expecting', 'by', 'taking', 'all', 'of', 'our', 'future', 'actions.', 'This', 'solution', 'works', 'if', 'our', 'Q', 'estimate', 'of', 'states', 'is', 'accurate,', 'so', 'how', 'can', 'we', 'learn', 'a', 'good', 'estimate?']), (0.02492935295735067, ['Are', 'you', 'a', 'company', 'working', 'in', 'AI', 'and', 'would', 'like', 'to', 'get', 'involved', 'in', 'the', 'Insight', 'AI', 'Fellows', 'Program?', 'Feel', 'free', 'to', 'get', 'in', 'touch.']), (0.02492935295735067, ['Are', 'you', 'a', 'company', 'working', 'in', 'AI', 'and', 'would', 'like', 'to', 'get', 'involved', 'in', 'the', 'Insight', 'AI', 'Fellows', 'Program?', 'Feel', 'free', 'to', 'get', 'in', 'touch.']), (0.024634264127112154, ['We', 'can', 'approach', 'this', 'by', 'approximating', 'our', 'potential', 'choices', 'with', 'Gaussian', 'distributions.', 'We', 'learn', 'a', 'probability', 'distribution', 'over', 'potential', 'actions', 'by', 'learning', 'the', 'mean', 'and', 'variance', 'of', 'a', 'Gaussian', 'distribution,', 'and', 'our', 'policy', 'we', 'sample', 'from', 'that', 'distribution.', 'Simple,', 'in', 'theory', ':).']), (0.02340183964588297, ['There', 'is', 'an', 'additional', 'advantage', 'to', 'using', 'Deep', 'Learning', 'for', 'these', 'methods,', 'which', 'is', 'that', 'Deep', 'Neural', 'Networks', 'excel', 'at', 'perceptive', 'tasks.', 'When', 'a', 'human', 'plays', 'a', 'game,', 'the', 'information', 'received', 'is', 'not', 'a', 'list', 'of', 'states,', 'but', 'an', 'image', '(usually', 'of', 'a', 'screen,', 'or', 'a', 'board,', 'or', 'the', 'surrounding', 'environment).']), (0.023171139615126967, ['So', 'far,', 'we’ve', 'played', 'with', 'environments', 'with', 'continuous', 'and', 'discrete', 'state', 'spaces.', 'However,', 'every', 'environment', 'we', 'studied', 'had', 'a', 'discrete', 'action', 'space:', 'we', 'could', 'move', 'in', 'one', 'of', 'four', 'directions,', 'or', 'tilt', 'the', 'paddle', 'to', 'the', 'left', 'or', 'right.', 'Ideally,', 'for', 'applications', 'such', 'as', 'self-driving', 'cars,', 'we', 'would', 'like', 'to', 'learn', 'continuous', 'actions,', 'such', 'as', 'turning', 'the', 'steering', 'wheel', 'between', '0', 'and', '360', 'degrees.', 'In', 'this', 'environment', 'called', '3D', 'ball', 'world,', 'we', 'can', 'choose', 'to', 'tilt', 'the', 'paddle', 'to', 'any', 'value', 'on', 'each', 'of', 'its', 'axes.', 'This', 'gives', 'us', 'more', 'control', 'as', 'to', 'how', 'we', 'perform', 'actions,', 'but', 'makes', 'the', 'action', 'space', 'much', 'larger.']), (0.023122915038275873, ['This', 'makes', 'Deep', 'RL', 'particularly', 'attractive', 'for', 'tasks', 'that', 'require', 'planning', 'and', 'adaptation,', 'such', 'as', 'manufacturing', 'or', 'self-driving.', 'However,', 'industry', 'applications', 'have', 'trailed', 'behind', 'the', 'rapidly', 'advancing', 'results', 'coming', 'out', 'of', 'the', 'research', 'community.', 'A', 'major', 'reason', 'is', 'that', 'Deep', 'RL', 'often', 'requires', 'an', 'agent', 'to', 'experiment', 'millions', 'of', 'times', 'before', 'learning', 'anything', 'useful.', 'The', 'best', 'way', 'to', 'do', 'this', 'rapidly', 'is', 'by', 'using', 'a', 'simulation', 'environment.', 'This', 'tutorial', 'will', 'be', 'using', 'Unity', 'to', 'create', 'environments', 'to', 'train', 'agents', 'in.']), (0.022413750707133445, ['Image-based', 'Learning', 'combines', 'a', 'Convolutional', 'Neural', 'Network', '(CNN)', 'with', 'RL.', 'In', 'this', 'environment,', 'we', 'pass', 'in', 'a', 'raw', 'image', 'instead', 'of', 'features,', 'and', 'add', 'a', '2', 'layer', 'CNN', 'to', 'our', 'architecture', 'without', 'changing', 'anything', 'else!', 'We', 'can', 'even', 'inspect', 'activations', 'to', 'see', 'what', 'the', 'network', 'picks', 'up', 'on', 'to', 'determine', 'value,', 'and', 'policy.', 'In', 'the', 'example', 'below,', 'we', 'can', 'see', 'that', 'the', 'network', 'uses', 'the', 'current', 'score', 'and', 'distant', 'obstacles', 'to', 'estimate', 'the', 'value', 'of', 'the', 'current', 'state,', 'while', 'focusing', 'on', 'nearby', 'obstacles', 'for', 'determining', 'actions.', 'Neat!']), (0.020959185111820988, ['For', 'this', 'workshop', 'led', 'by', 'Arthur', 'Juliani', 'and', 'Leon', 'Chen,', 'their', 'goal', 'was', 'to', 'get', 'every', 'participants', 'to', 'successfully', 'train', 'multiple', 'Deep', 'RL', 'algorithms', 'in', '4', 'hours.', 'A', 'tall', 'order!', 'Below,', 'is', 'a', 'comprehensive', 'overview', 'of', 'many', 'of', 'the', 'main', 'algorithms', 'that', 'power', 'Deep', 'RL', 'today.', 'For', 'a', 'more', 'complete', 'set', 'of', 'tutorials,', 'Arthur', 'Juliani', 'wrote', 'an', '8-part', 'series', 'starting', 'here.']), (0.019710005978966855, ['There', 'is', 'another', 'key', 'factor', 'that', 'makes', 'our', 'current', 'problem', 'simpler', 'than', 'mosts.', 'In', 'most', 'environments,', 'such', 'as', 'in', 'the', 'maze', 'depicted', 'above,', 'the', 'actions', 'that', 'we', 'take', 'have', 'an', 'impact', 'on', 'the', 'state', 'of', 'the', 'world.', 'If', 'we', 'move', 'up', 'on', 'this', 'grid,', 'we', 'might', 'receive', 'a', 'reward', 'or', 'we', 'might', 'receive', 'nothing,', 'but', 'the', 'next', 'turn', 'we', 'will', 'be', 'in', 'a', 'different', 'state.', 'This', 'is', 'where', 'we', 'finally', 'introduce', 'a', 'need', 'for', 'planning.']), (0.019453324949716137, ['Surprisingly,', 'we', 'can', 'use', 'the', 'same', 'approach', 'as', 'before.', 'The', 'only', 'thing', 'we', 'need', 'to', 'add', 'is', 'an', 'extra', 'dense', 'layer', 'to', 'our', 'neural', 'network,', 'that', 'will', 'take', 'in', 'as', 'input', 'a', 'vector', 'representing', 'the', 'current', 'state', 'of', 'the', 'world.']), (0.019360671756680346, ['Recently,', 'I', 'gave', 'a', 'talk', 'at', 'the', 'O’Reilly', 'AI', 'conference', 'in', 'Beijing', 'about', 'some', 'of', 'the', 'interesting', 'lessons', 'we’ve', 'learned', 'in', 'the', 'world', 'of', 'NLP.', 'While', 'there,', 'I', 'was', 'lucky', 'enough', 'to', 'attend', 'a', 'tutorial', 'on', 'Deep', 'Reinforcement', 'Learning', '(Deep', 'RL)', 'from', 'scratch', 'by', 'Unity', 'Technologies.', 'I', 'thought', 'that', 'the', 'session,', 'led', 'by', 'Arthur', 'Juliani,', 'was', 'extremely', 'informative', 'and', 'wanted', 'to', 'share', 'some', 'big', 'takeaways', 'below.']), (0.019008405660989602, ['As', 'a', 'side', 'note,', 'while', 'toying', 'around', 'with', 'the', 'provided', 'implementation,', 'I’ve', 'found', 'that', 'visual', 'learning', 'is', 'very', 'sensitive', 'to', 'hyperparameters.', 'Changing', 'the', 'discount', 'rate', 'slightly', 'for', 'example,', 'completely', 'prevented', 'the', 'neural', 'network', 'from', 'learning', 'even', 'on', 'a', 'toy', 'application.', 'This', 'is', 'a', 'widely', 'known', 'problem,', 'but', 'it', 'is', 'interesting', 'to', 'see', 'it', 'first', 'hand.']), (0.018697978848749183, ['Deep', 'RL', 'can', 'be', 'used', 'to', 'best', 'the', 'top', 'human', 'players', 'at', 'Go,', 'but', 'to', 'understand', 'how', 'that’s', 'done,', 'you', 'first', 'need', 'to', 'understand', 'a', 'few', 'simple', 'concepts,', 'starting', 'with', 'much', 'easier', 'problems.']), (0.01766518332661626, ['In', 'our', 'conversations', 'with', 'companies,', 'we’ve', 'seen', 'a', 'rise', 'of', 'interesting', 'Deep', 'RL', 'applications,', 'tools', 'and', 'results.', 'In', 'parallel,', 'the', 'inner', 'workings', 'and', 'applications', 'of', 'Deep', 'RL,', 'such', 'as', 'AlphaGo', 'pictured', 'above,', 'can', 'often', 'seem', 'esoteric', 'and', 'hard', 'to', 'understand.', 'In', 'this', 'post,', 'I', 'will', 'give', 'an', 'overview', 'of', 'core', 'aspects', 'of', 'the', 'field', 'that', 'can', 'be', 'understood', 'by', 'anyone.']), (0.017583705294148946, ['In', 'practice,', 'we', 'might', 'want', 'to', 'take', 'a', 'more', 'subtle', 'approach', 'than', 'either', 'taking', 'the', 'action', 'we', 'think', 'is', 'the', 'best,', 'or', 'a', 'random', 'action.', 'A', 'popular', 'method', 'is', 'Boltzmann', 'Exploration,', 'which', 'adjust', 'probabilities', 'based', 'on', 'our', 'current', 'estimate', 'of', 'how', 'good', 'each', 'chest', 'is,', 'adding', 'in', 'a', 'randomness', 'factor.']), (0.01626087340284764, ['AI', 'Lead', 'at', 'Insight', 'AI', '@EmmanuelAmeisen']), (0.015841793630508208, ['6/Nuanced', 'actions']), (0.014217670011137697, ['3/Learning', 'about', 'the', 'consequences', 'of', 'our', 'actions']), (0.013155639452599132, ['That’s', 'it', 'for', 'this', 'overview,', 'I', 'hope', 'this', 'has', 'been', 'informative', 'and', 'fun!', 'If', 'you', 'are', 'looking', 'to', 'dive', 'deeper', 'into', 'the', 'theory', 'of', 'RL,', 'give', 'Arthur’s', 'posts', 'a', 'read,', 'or', 'diving', 'deeper', 'by', 'following', 'David', 'Silver’s', 'UCL', 'course.', 'If', 'you', 'are', 'looking', 'to', 'learn', 'more', 'about', 'the', 'projects', 'we', 'do', 'at', 'Insight,', 'or', 'how', 'we', 'work', 'with', 'companies,', 'please', 'check', 'us', 'out', 'below,', 'or', 'reach', 'out', 'to', 'me', 'here.']), (0.011324921188021482, ['Insight', 'Fellows', 'Program', '-', 'Your', 'bridge', 'to', 'a', 'career', 'in', 'data']), (0.010429435238551638, ['There', 'are', 'a', 'few', 'concepts', 'that', 'separate', 'the', 'algorithms', 'described', 'above', 'from', 'state', 'of', 'the', 'art', 'approaches.', 'It’s', 'interesting', 'to', 'see', 'that', 'conceptually,', 'the', 'best', 'robotics', 'and', 'game-playing', 'algorithms', 'are', 'not', 'that', 'far', 'away', 'from', 'the', 'ones', 'we', 'just', 'explored:']), (0.01021567666254521, ['2/Adding', 'different', 'states']), (0.006446732926685452, ['4/The', 'world', 'is', 'rarely', 'discrete']), (0.006325131610757893, ['Many', 'of', 'the', 'visuals', 'are', 'from', 'the', 'slides', 'of', 'the', 'talk,', 'and', 'some', 'are', 'new.', 'The', 'explanations', 'and', 'opinions', 'are', 'mine.', 'If', 'anything', 'is', 'unclear,', 'reach', 'out', 'to', 'me', 'here!']), (0.00571167701965515, ['3+/Introducing', 'Monte', 'Carlo']), (0.004978043595164348, ['5/Learning', 'directly', 'from', 'the', 'screen']), (0.0047590444213148776, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.004378029741765858, ['7/Next', 'steps', 'for', 'the', 'brave']), (0.00407745940073336, ['1/It', 'all', 'starts', 'with', 'slot', 'machines'])]\n",
      "Summarize Text: \n",
      " In practice, many state of the art RL methods require learning both a policy and value estimates. The way we do this with deep learning is by having both be two separate outputs of the same backbone neural network, which will make it easier for our neural network to learn good representations.. The approach we will use here is called Policy Gradients, and is an on-policy method. Previously, we were first learning a value function Q for each action in each state and then building a policy on top. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. Since we are learning on policy, we cannot use methods such as epsilon greedy (which includes random choices), to get our agent to explore the environment. The way that we encourage exploration is by using a method called entropy regularization, which pushes our probability estimates to be wider, and thus will encourage us to make riskier choices to explore the space.. One method to do this is Advantage Actor Critic (A2C). We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. Instead of updating our value function based on rewards, we update it based on our advantage, which measures how much better or worse an action was than our previous value function estimated it to be. This helps make learning more stable compared to simple Q Learning and Vanilla Policy Gradients.. While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. Intuitively, we might want to use a policy that picks the action with the highest Q value. This performs poorly in practice, as our Q estimates will be very wrong at the start before we gather enough experience through trial and error. This is why we need to add a mechanism to our policy to encourage exploration. One way to do that is to use epsilon greedy, which consists of taking a random action with probability epsilon. We start with epsilon being close to 1, always choosing random actions, and lower epsilon as we go along and learn more about which chests are good. Eventually, we learn which chests are best.. Deep RL is a field that has seen vast amounts of research interest, including learning to play Atari games, beating pro players at Dota 2, and defeating Go champions. Contrary to many classical Deep Learning problems that often focus on perception (does this image contain a stop sign?), Deep RL adds the dimension of actions that influence the environment (what is the goal, and how do I get there?). In dialog systems for example, classical Deep Learning aims to learn the right response for a given query. On the other hand, Deep Reinforcement Learning focuses on the right sequences of sentences that will lead to a positive outcome, for example a happy customer.\n",
      "None\n",
      "The advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task, often achievable in a single line of code.\n",
      "However, understanding convolutions, especially for the first time can often feel a bit unnerving, with terms like kernels, filters, channels and so on all stacked onto each other. Yet, convolutions as a concept are fascinatingly powerful and highly extensible, and in this post, we’ll break down the mechanics of the convolution operation, step-by-step, relate it to the standard fully connected network, and explore just how they build up a strong visual hierarchy, making them powerful feature extractors for images.\n",
      "The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n",
      "The kernel repeats this process for every location it slides over, converting a 2D matrix of features into yet another 2D matrix of features. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer.\n",
      "Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.\n",
      "This is all in pretty stark contrast to a fully connected layer. In the above example, we have 5×5=25 input features, and 3×3=9 output features. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. Do take note of this, as it’ll be critical to our later discussion.\n",
      "Before we move on, it’s definitely worth looking into two techniques that are commonplace in convolution layers: Padding and Strides.\n",
      "Padding does something pretty clever to solve this: pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”). This way, the kernel when sliding can allow the original edge pixels to be at its center, while extending into the fake pixels beyond the edge, producing an output the same size as the input.\n",
      "The idea of the stride is to skip some of the slide locations of the kernel. A stride of 1 means to pick slides a pixel apart, so basically every single slide, acting as a standard convolution. A stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2, a stride of 3 means skipping every 2 slides, downsizing roughly by factor 3, and so on.\n",
      "More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, in favor of strided convolutions when needing to reduce their output sizes.\n",
      "Of course, the diagrams above only deals with the case where the image has a single input channel. In practicality, most input images have 3 channels, and that number only increases the deeper you go into a network. It’s pretty easy to think of channels, in general, as being a “view” of the image as a whole, emphasising some aspects, de-emphasising others.\n",
      "So this is where a key distinction between terms comes in handy: whereas in the 1 channel case, where the term filter and kernel are interchangeable, in the general case, they’re actually pretty different. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique.\n",
      "Each filter in a convolution layer produces one and only one output channel, and they do it like so:\n",
      "Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).\n",
      "Each of the per-channel processed versions are then summed together to form one channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.\n",
      "Finally, then there’s the bias term. The way the bias term works here is that each output filter has one bias term. The bias gets added to the output channel so far to produce the final output channel.\n",
      "And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. They are then concatenated together to produce the overall output, with the number of output channels being the number of filters. A nonlinearity is then usually applied before passing this as input to another convolution layer, which then repeats this process.\n",
      "Even with the mechanics of the convolution layer down, it can still be hard to relate it back to a standard feed-forward network, and it still doesn’t explain why convolutions scale to, and work so much better for image data.\n",
      "Suppose we have a 4×4 input, and we want to transform it into a 2×2 grid. If we were using a feedforward network, we’d reshape the 4×4 input into a vector of length 16, and pass it through a densely connected layer with 16 inputs and 4 outputs. One could visualize the weight matrix W for a layer:\n",
      "And although the convolution kernel operation may seem a bit strange at first, it is still a linear transformation with an equivalent transformation matrix. If we were to use a kernel K of size 3 on the reshaped 4×4 input to get a 2×2 output, the equivalent transformation matrix would be:\n",
      "(Note: while the above matrix is an equivalent transformation matrix, the actual operation is usually implemented as a very different matrix multiplication[2])\n",
      "The convolution then, as a whole, is still a linear transformation, but at the same time it’s also a dramatically different kind of transformation. For a matrix with 64 elements, there’s just 9 parameters which themselves are reused several times. Each output node only gets to see a select number of inputs (the ones inside the kernel). There is no interaction with any of the other inputs, as the weights to them are set to 0.\n",
      "It’s useful to see the convolution operation as a hard prior on the weight matrix. In this context, by prior, I mean predefined network parameters. For example, when you use a pretrained model for image classification, you use the pretrained network parameters as your prior, as a feature extractor to your final densely connected layer.\n",
      "In that sense, there’s a direct intuition between why both are so efficient (compared to their alternatives). Transfer learning is efficient by orders of magnitude compared to random initialization, because you only really need to optimize the parameters of the final fully connected layer, which means you can have fantastic performance with only a few dozen images per class.\n",
      "Here, you don’t need to optimize all 64 parameters, because we set most of them to zero (and they’ll stay that way), and the rest we convert to shared parameters, resulting in only 9 actual parameters to optimize. This efficiency matters, because when you move from the 784 inputs of MNIST to real world 224×224×3 images, thats over 150,000 inputs. A dense layer attempting to halve the input to 75,000 inputs would still require over 10 billion parameters. For comparison, the entirety of ResNet-50 has some 25 million parameters.\n",
      "So fixing some parameters to 0, and tying parameters increases efficiency, but unlike the transfer learning case, where we know the prior is good because it works on a large general set of images, how do we know this is any good?\n",
      "The answer lies in the feature combinations the prior leads the parameters to learn.\n",
      "Early on in this article, we discussed that:\n",
      "So with backpropagation coming in all the way from the classification nodes of the network, the kernels have the interesting task of learning weights to produce features only from a set of local inputs. Additionally, because the kernel itself is applied across the entire image, the features the kernel learns must be general enough to come from any part of the image.\n",
      "If this were any other kind of data, eg. categorical data of app installs, this would’ve been a disaster, for just because your number of app installs and app type columns are next to each other doesn’t mean they have any “local, shared features” common with app install dates and time used. Sure, the four may have an underlying higher level feature (eg. which apps people want most) that can be found, but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two. The four could’ve been in any (consistent) order and still be valid!\n",
      "Pixels however, always appear in a consistent order, and nearby pixels influence a pixel e.g. if all nearby pixels are red, it’s pretty likely the pixel is also red. If there are deviations, that’s an interesting anomaly that could be converted into a feature, and all this can be detected from comparing a pixel with its neighbors, with other pixels in its locality.\n",
      "And this idea is really what a lot of earlier computer vision feature extraction methods were based around. For instance, for edge detection, one can use a Sobel edge detection filter, a kernel with fixed parameters, operating just like the standard one-channel convolution:\n",
      "For a non-edge containing grid (eg. the background sky), most of the pixels are the same value, so the overall output of the kernel at that point is 0. For a grid with an vertical edge, there is a difference between the pixels to the left and right of the edge, and the kernel computes that difference to be non-zero, activating and revealing the edges. The kernel only works only a 3×3 grids at a time, detecting anomalies on a local scale, yet when applied across the entire image, is enough to detect a certain feature on a global scale, anywhere in the image!\n",
      "So the key difference we make with deep learning is ask this question: Can useful kernels be learnt? For early layers operating on raw pixels, we could reasonably expect feature detectors of fairly low level features, like edges, lines, etc.\n",
      "There’s an entire branch of deep learning research focused on making neural network models interpretable. One of the most powerful tools to come out of that is Feature Visualization using optimization[3]. The idea at core is simple: optimize a image (usually initialized with random noise) to activate a filter as strongly as possible. This does make intuitive sense: if the optimized image is completely filled with edges, that’s strong evidence that’s what the filter itself is looking for and is activated by. Using this, we can peek into the learnt filters, and the results are stunning:\n",
      "One important thing to notice here is that convolved images are still images. The output of a small grid of pixels from the top left of an image will still be on the top left. So you can run another convolution layer on top of another (such as the two on the left) to extract deeper features, which we visualize.\n",
      "Yet, however deep our feature detectors get, without any further changes they’ll still be operating on very small patches of the image. No matter how deep your detectors are, you can’t detect faces from a 3×3 grid. And this is where the idea of the receptive field comes in.\n",
      "A essential design choice of any CNN architecture is that the input sizes grow smaller and smaller from the start to the end of the network, while the number of channels grow deeper. This, as mentioned earlier, is often done through strides or pooling layers. Locality determines what inputs from the previous layer the outputs get to see. The receptive field determines what area of the original input to the entire network the output gets to see.\n",
      "The idea of a strided convolution is that we only process slides a fixed distance apart, and skip the ones in the middle. From a different point of view, we only keep outputs a fixed distance apart, and remove the rest[1].\n",
      "We then apply a nonlinearity to the output, and per usual, then stack another new convolution layer on top. And this is where things get interesting. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:\n",
      "This is because the output of the strided layer still does represent the same image. It is not so much cropping as it is resizing, only thing is that each single pixel in the output is a “representative” of a larger area (of whose other pixels were discarded) from the same rough location from the original input. So when the next layer’s kernel operates on the output, it’s operating on pixels collected from a larger area.\n",
      "(Note: if you’re familiar with dilated convolutions, note that the above is not a dilated convolution. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)\n",
      "This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer.\n",
      "Followed by a pooling/strided layer, the network continues to create detectors for even higher level features (parts, patterns), as we see for mixed4a.\n",
      "The repeated reduction in image size across the network results in, by the 5th block on convolutions, input sizes of just 7×7, compared to inputs of 224×224. At this point, each single pixel represents a grid of 32×32 pixels, which is huge.\n",
      "Compared to earlier layers, where an activation meant detecting an edge, here, an activation on the tiny 7×7 grid is one for a very high level feature, such as for birds.\n",
      "The network as a whole progresses from a small number of filters (64 in case of GoogLeNet), detecting low level features, to a very large number of filters(1024 in the final convolution), each looking for an extremely specific high level feature. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image.\n",
      "Compared to what a standard feedforward network would have done, the output here is really nothing short of awe-inspiring. A standard feedforward network would have produced abstract feature vectors, from combinations of every single pixel in the image, requiring intractable amounts of data to train.\n",
      "The CNN, with the priors imposed on it, starts by learning very low level feature detectors, and as across the layers as its receptive field is expanded, learns to combine those low-level features into progressively higher level features; not an abstract combination of every single pixel, but rather, a strong visual hierarchy of concepts.\n",
      "By detecting low level features, and using them to detect higher level features as it progresses up its visual hierarchy, it is eventually able to detect entire visual concepts such as faces, birds, trees, etc, and that’s what makes them such powerful, yet efficient with image data.\n",
      "With the visual hierarchy CNNs build, it is pretty reasonable to assume that their vision systems are similar to humans. And they’re really great with real world images, but they also fail in ways that strongly suggest their vision systems aren’t entirely human-like. The most major problem: Adversarial Examples[4], examples which have been specifically modified to fool the model.\n",
      "Adversarial examples would be a non-issue if the only tampered ones that caused the models to fail were ones that even humans would notice. The problem is, the models are susceptible to attacks by samples which have only been tampered with ever so slightly, and would clearly not fool any human. This opens the door for models to silently fail, which can be pretty dangerous for a wide range of applications from self-driving cars to healthcare.\n",
      "Robustness against adversarial attacks is currently a highly active area of research, the subject of many papers and even competitions, and solutions will certainly improve CNN architectures to become safer and more reliable.\n",
      "CNNs were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services, ranging from face detection in your photo gallery to making better medical diagnoses. They might be the key method in computer vision going forward, or some other new breakthrough might just be around the corner. Regardless, one thing is for sure: they’re nothing short of amazing, at the heart of many present-day innovative applications, and are most certainly worth deeply understanding.\n",
      "Hope you enjoyed this article! If you’d like to stay connected, you’ll find me on Twitter here. If you have a question, comments are welcome! — I find them to be useful to my own learning process as well.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Curious programmer, tinkers around in Python and deep learning.\n",
      "Sharing concepts, ideas, and codes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.03112430579494499, ['This', 'is', 'all', 'in', 'pretty', 'stark', 'contrast', 'to', 'a', 'fully', 'connected', 'layer.', 'In', 'the', 'above', 'example,', 'we', 'have', '5×5=25', 'input', 'features,', 'and', '3×3=9', 'output', 'features.', 'If', 'this', 'were', 'a', 'standard', 'fully', 'connected', 'layer,', 'you’d', 'have', 'a', 'weight', 'matrix', 'of', '25×9', '=', '225', 'parameters,', 'with', 'every', 'output', 'feature', 'being', 'the', 'weighted', 'sum', 'of', 'every', 'single', 'input', 'feature.', 'Convolutions', 'allow', 'us', 'to', 'do', 'this', 'transformation', 'with', 'only', '9', 'parameters,', 'with', 'each', 'output', 'feature,', 'instead', 'of', '“looking', 'at”', 'every', 'input', 'feature,', 'only', 'getting', 'to', '“look”', 'at', 'input', 'features', 'coming', 'from', 'roughly', 'the', 'same', 'location.', 'Do', 'take', 'note', 'of', 'this,', 'as', 'it’ll', 'be', 'critical', 'to', 'our', 'later', 'discussion.']), (0.030536310789739632, ['The', 'kernel', 'repeats', 'this', 'process', 'for', 'every', 'location', 'it', 'slides', 'over,', 'converting', 'a', '2D', 'matrix', 'of', 'features', 'into', 'yet', 'another', '2D', 'matrix', 'of', 'features.', 'The', 'output', 'features', 'are', 'essentially,', 'the', 'weighted', 'sums', '(with', 'the', 'weights', 'being', 'the', 'values', 'of', 'the', 'kernel', 'itself)', 'of', 'the', 'input', 'features', 'located', 'roughly', 'in', 'the', 'same', 'location', 'of', 'the', 'output', 'pixel', 'on', 'the', 'input', 'layer.']), (0.030415088742519344, ['And', 'with', 'the', 'single', 'filter', 'case', 'down,', 'the', 'case', 'for', 'any', 'number', 'of', 'filters', 'is', 'identical:', 'Each', 'filter', 'processes', 'the', 'input', 'with', 'its', 'own,', 'different', 'set', 'of', 'kernels', 'and', 'a', 'scalar', 'bias', 'with', 'the', 'process', 'described', 'above,', 'producing', 'a', 'single', 'output', 'channel.', 'They', 'are', 'then', 'concatenated', 'together', 'to', 'produce', 'the', 'overall', 'output,', 'with', 'the', 'number', 'of', 'output', 'channels', 'being', 'the', 'number', 'of', 'filters.', 'A', 'nonlinearity', 'is', 'then', 'usually', 'applied', 'before', 'passing', 'this', 'as', 'input', 'to', 'another', 'convolution', 'layer,', 'which', 'then', 'repeats', 'this', 'process.']), (0.02863789258598746, ['Whether', 'or', 'not', 'an', 'input', 'feature', 'falls', 'within', 'this', '“roughly', 'same', 'location”,', 'gets', 'determined', 'directly', 'by', 'whether', 'it’s', 'in', 'the', 'area', 'of', 'the', 'kernel', 'that', 'produced', 'the', 'output', 'or', 'not.', 'This', 'means', 'the', 'size', 'of', 'the', 'kernel', 'directly', 'determines', 'how', 'many', '(or', 'few)', 'input', 'features', 'get', 'combined', 'in', 'the', 'production', 'of', 'a', 'new', 'output', 'feature.']), (0.027150595564242316, ['This', 'is', 'because', 'the', 'output', 'of', 'the', 'strided', 'layer', 'still', 'does', 'represent', 'the', 'same', 'image.', 'It', 'is', 'not', 'so', 'much', 'cropping', 'as', 'it', 'is', 'resizing,', 'only', 'thing', 'is', 'that', 'each', 'single', 'pixel', 'in', 'the', 'output', 'is', 'a', '“representative”', 'of', 'a', 'larger', 'area', '(of', 'whose', 'other', 'pixels', 'were', 'discarded)', 'from', 'the', 'same', 'rough', 'location', 'from', 'the', 'original', 'input.', 'So', 'when', 'the', 'next', 'layer’s', 'kernel', 'operates', 'on', 'the', 'output,', 'it’s', 'operating', 'on', 'pixels', 'collected', 'from', 'a', 'larger', 'area.']), (0.026921270894949032, ['Each', 'filter', 'in', 'a', 'convolution', 'layer', 'produces', 'one', 'and', 'only', 'one', 'output', 'channel,', 'and', 'they', 'do', 'it', 'like', 'so:']), (0.02614286021781747, ['The', '2D', 'convolution', 'is', 'a', 'fairly', 'simple', 'operation', 'at', 'heart:', 'you', 'start', 'with', 'a', 'kernel,', 'which', 'is', 'simply', 'a', 'small', 'matrix', 'of', 'weights.', 'This', 'kernel', '“slides”', 'over', 'the', '2D', 'input', 'data,', 'performing', 'an', 'elementwise', 'multiplication', 'with', 'the', 'part', 'of', 'the', 'input', 'it', 'is', 'currently', 'on,', 'and', 'then', 'summing', 'up', 'the', 'results', 'into', 'a', 'single', 'output', 'pixel.']), (0.025068771196100677, ['So', 'this', 'is', 'where', 'a', 'key', 'distinction', 'between', 'terms', 'comes', 'in', 'handy:', 'whereas', 'in', 'the', '1', 'channel', 'case,', 'where', 'the', 'term', 'filter', 'and', 'kernel', 'are', 'interchangeable,', 'in', 'the', 'general', 'case,', 'they’re', 'actually', 'pretty', 'different.', 'Each', 'filter', 'actually', 'happens', 'to', 'be', 'a', 'collection', 'of', 'kernels,', 'with', 'there', 'being', 'one', 'kernel', 'for', 'every', 'single', 'input', 'channel', 'to', 'the', 'layer,', 'and', 'each', 'kernel', 'being', 'unique.']), (0.024542204939936217, ['The', 'network', 'as', 'a', 'whole', 'progresses', 'from', 'a', 'small', 'number', 'of', 'filters', '(64', 'in', 'case', 'of', 'GoogLeNet),', 'detecting', 'low', 'level', 'features,', 'to', 'a', 'very', 'large', 'number', 'of', 'filters(1024', 'in', 'the', 'final', 'convolution),', 'each', 'looking', 'for', 'an', 'extremely', 'specific', 'high', 'level', 'feature.', 'Followed', 'by', 'a', 'final', 'pooling', 'layer,', 'which', 'collapses', 'each', '7×7', 'grid', 'into', 'a', 'single', 'pixel,', 'each', 'channel', 'is', 'a', 'feature', 'detector', 'with', 'a', 'receptive', 'field', 'equivalent', 'to', 'the', 'entire', 'image.']), (0.024419587642362538, ['We', 'then', 'apply', 'a', 'nonlinearity', 'to', 'the', 'output,', 'and', 'per', 'usual,', 'then', 'stack', 'another', 'new', 'convolution', 'layer', 'on', 'top.', 'And', 'this', 'is', 'where', 'things', 'get', 'interesting.', 'Even', 'if', 'were', 'we', 'to', 'apply', 'a', 'kernel', 'of', 'the', 'same', 'size', '(3×3),', 'having', 'the', 'same', 'local', 'area,', 'to', 'the', 'output', 'of', 'the', 'strided', 'convolution,', 'the', 'kernel', 'would', 'have', 'a', 'larger', 'effective', 'receptive', 'field:']), (0.023570508182915412, ['And', 'although', 'the', 'convolution', 'kernel', 'operation', 'may', 'seem', 'a', 'bit', 'strange', 'at', 'first,', 'it', 'is', 'still', 'a', 'linear', 'transformation', 'with', 'an', 'equivalent', 'transformation', 'matrix.', 'If', 'we', 'were', 'to', 'use', 'a', 'kernel', 'K', 'of', 'size', '3', 'on', 'the', 'reshaped', '4×4', 'input', 'to', 'get', 'a', '2×2', 'output,', 'the', 'equivalent', 'transformation', 'matrix', 'would', 'be:']), (0.022646763097413977, ['The', 'CNN,', 'with', 'the', 'priors', 'imposed', 'on', 'it,', 'starts', 'by', 'learning', 'very', 'low', 'level', 'feature', 'detectors,', 'and', 'as', 'across', 'the', 'layers', 'as', 'its', 'receptive', 'field', 'is', 'expanded,', 'learns', 'to', 'combine', 'those', 'low-level', 'features', 'into', 'progressively', 'higher', 'level', 'features;', 'not', 'an', 'abstract', 'combination', 'of', 'every', 'single', 'pixel,', 'but', 'rather,', 'a', 'strong', 'visual', 'hierarchy', 'of', 'concepts.']), (0.02223533242492051, ['So', 'the', 'key', 'difference', 'we', 'make', 'with', 'deep', 'learning', 'is', 'ask', 'this', 'question:', 'Can', 'useful', 'kernels', 'be', 'learnt?', 'For', 'early', 'layers', 'operating', 'on', 'raw', 'pixels,', 'we', 'could', 'reasonably', 'expect', 'feature', 'detectors', 'of', 'fairly', 'low', 'level', 'features,', 'like', 'edges,', 'lines,', 'etc.']), (0.022023773773986077, ['For', 'a', 'non-edge', 'containing', 'grid', '(eg.', 'the', 'background', 'sky),', 'most', 'of', 'the', 'pixels', 'are', 'the', 'same', 'value,', 'so', 'the', 'overall', 'output', 'of', 'the', 'kernel', 'at', 'that', 'point', 'is', '0.', 'For', 'a', 'grid', 'with', 'an', 'vertical', 'edge,', 'there', 'is', 'a', 'difference', 'between', 'the', 'pixels', 'to', 'the', 'left', 'and', 'right', 'of', 'the', 'edge,', 'and', 'the', 'kernel', 'computes', 'that', 'difference', 'to', 'be', 'non-zero,', 'activating', 'and', 'revealing', 'the', 'edges.', 'The', 'kernel', 'only', 'works', 'only', 'a', '3×3', 'grids', 'at', 'a', 'time,', 'detecting', 'anomalies', 'on', 'a', 'local', 'scale,', 'yet', 'when', 'applied', 'across', 'the', 'entire', 'image,', 'is', 'enough', 'to', 'detect', 'a', 'certain', 'feature', 'on', 'a', 'global', 'scale,', 'anywhere', 'in', 'the', 'image!']), (0.02199298504304699, ['The', 'convolution', 'then,', 'as', 'a', 'whole,', 'is', 'still', 'a', 'linear', 'transformation,', 'but', 'at', 'the', 'same', 'time', 'it’s', 'also', 'a', 'dramatically', 'different', 'kind', 'of', 'transformation.', 'For', 'a', 'matrix', 'with', '64', 'elements,', 'there’s', 'just', '9', 'parameters', 'which', 'themselves', 'are', 'reused', 'several', 'times.', 'Each', 'output', 'node', 'only', 'gets', 'to', 'see', 'a', 'select', 'number', 'of', 'inputs', '(the', 'ones', 'inside', 'the', 'kernel).', 'There', 'is', 'no', 'interaction', 'with', 'any', 'of', 'the', 'other', 'inputs,', 'as', 'the', 'weights', 'to', 'them', 'are', 'set', 'to', '0.']), (0.021824691838022894, ['Compared', 'to', 'what', 'a', 'standard', 'feedforward', 'network', 'would', 'have', 'done,', 'the', 'output', 'here', 'is', 'really', 'nothing', 'short', 'of', 'awe-inspiring.', 'A', 'standard', 'feedforward', 'network', 'would', 'have', 'produced', 'abstract', 'feature', 'vectors,', 'from', 'combinations', 'of', 'every', 'single', 'pixel', 'in', 'the', 'image,', 'requiring', 'intractable', 'amounts', 'of', 'data', 'to', 'train.']), (0.021801291770719038, ['One', 'important', 'thing', 'to', 'notice', 'here', 'is', 'that', 'convolved', 'images', 'are', 'still', 'images.', 'The', 'output', 'of', 'a', 'small', 'grid', 'of', 'pixels', 'from', 'the', 'top', 'left', 'of', 'an', 'image', 'will', 'still', 'be', 'on', 'the', 'top', 'left.', 'So', 'you', 'can', 'run', 'another', 'convolution', 'layer', 'on', 'top', 'of', 'another', '(such', 'as', 'the', 'two', 'on', 'the', 'left)', 'to', 'extract', 'deeper', 'features,', 'which', 'we', 'visualize.']), (0.021241411433917567, ['This', 'expansion', 'of', 'the', 'receptive', 'field', 'allows', 'the', 'convolution', 'layers', 'to', 'combine', 'the', 'low', 'level', 'features', '(lines,', 'edges),', 'into', 'higher', 'level', 'features', '(curves,', 'textures),', 'as', 'we', 'see', 'in', 'the', 'mixed3a', 'layer.']), (0.021073103630874127, ['So', 'with', 'backpropagation', 'coming', 'in', 'all', 'the', 'way', 'from', 'the', 'classification', 'nodes', 'of', 'the', 'network,', 'the', 'kernels', 'have', 'the', 'interesting', 'task', 'of', 'learning', 'weights', 'to', 'produce', 'features', 'only', 'from', 'a', 'set', 'of', 'local', 'inputs.', 'Additionally,', 'because', 'the', 'kernel', 'itself', 'is', 'applied', 'across', 'the', 'entire', 'image,', 'the', 'features', 'the', 'kernel', 'learns', 'must', 'be', 'general', 'enough', 'to', 'come', 'from', 'any', 'part', 'of', 'the', 'image.']), (0.020702470713302258, ['A', 'essential', 'design', 'choice', 'of', 'any', 'CNN', 'architecture', 'is', 'that', 'the', 'input', 'sizes', 'grow', 'smaller', 'and', 'smaller', 'from', 'the', 'start', 'to', 'the', 'end', 'of', 'the', 'network,', 'while', 'the', 'number', 'of', 'channels', 'grow', 'deeper.', 'This,', 'as', 'mentioned', 'earlier,', 'is', 'often', 'done', 'through', 'strides', 'or', 'pooling', 'layers.', 'Locality', 'determines', 'what', 'inputs', 'from', 'the', 'previous', 'layer', 'the', 'outputs', 'get', 'to', 'see.', 'The', 'receptive', 'field', 'determines', 'what', 'area', 'of', 'the', 'original', 'input', 'to', 'the', 'entire', 'network', 'the', 'output', 'gets', 'to', 'see.']), (0.020246238160204134, ['There’s', 'an', 'entire', 'branch', 'of', 'deep', 'learning', 'research', 'focused', 'on', 'making', 'neural', 'network', 'models', 'interpretable.', 'One', 'of', 'the', 'most', 'powerful', 'tools', 'to', 'come', 'out', 'of', 'that', 'is', 'Feature', 'Visualization', 'using', 'optimization[3].', 'The', 'idea', 'at', 'core', 'is', 'simple:', 'optimize', 'a', 'image', '(usually', 'initialized', 'with', 'random', 'noise)', 'to', 'activate', 'a', 'filter', 'as', 'strongly', 'as', 'possible.', 'This', 'does', 'make', 'intuitive', 'sense:', 'if', 'the', 'optimized', 'image', 'is', 'completely', 'filled', 'with', 'edges,', 'that’s', 'strong', 'evidence', 'that’s', 'what', 'the', 'filter', 'itself', 'is', 'looking', 'for', 'and', 'is', 'activated', 'by.', 'Using', 'this,', 'we', 'can', 'peek', 'into', 'the', 'learnt', 'filters,', 'and', 'the', 'results', 'are', 'stunning:']), (0.02004575543917941, ['It’s', 'useful', 'to', 'see', 'the', 'convolution', 'operation', 'as', 'a', 'hard', 'prior', 'on', 'the', 'weight', 'matrix.', 'In', 'this', 'context,', 'by', 'prior,', 'I', 'mean', 'predefined', 'network', 'parameters.', 'For', 'example,', 'when', 'you', 'use', 'a', 'pretrained', 'model', 'for', 'image', 'classification,', 'you', 'use', 'the', 'pretrained', 'network', 'parameters', 'as', 'your', 'prior,', 'as', 'a', 'feature', 'extractor', 'to', 'your', 'final', 'densely', 'connected', 'layer.']), (0.0199057518976781, ['Finally,', 'then', 'there’s', 'the', 'bias', 'term.', 'The', 'way', 'the', 'bias', 'term', 'works', 'here', 'is', 'that', 'each', 'output', 'filter', 'has', 'one', 'bias', 'term.', 'The', 'bias', 'gets', 'added', 'to', 'the', 'output', 'channel', 'so', 'far', 'to', 'produce', 'the', 'final', 'output', 'channel.']), (0.019756238924113753, ['Each', 'of', 'the', 'per-channel', 'processed', 'versions', 'are', 'then', 'summed', 'together', 'to', 'form', 'one', 'channel.', 'The', 'kernels', 'of', 'a', 'filter', 'each', 'produce', 'one', 'version', 'of', 'each', 'channel,', 'and', 'the', 'filter', 'as', 'a', 'whole', 'produces', 'one', 'overall', 'output', 'channel.']), (0.01890902243620872, ['And', 'this', 'idea', 'is', 'really', 'what', 'a', 'lot', 'of', 'earlier', 'computer', 'vision', 'feature', 'extraction', 'methods', 'were', 'based', 'around.', 'For', 'instance,', 'for', 'edge', 'detection,', 'one', 'can', 'use', 'a', 'Sobel', 'edge', 'detection', 'filter,', 'a', 'kernel', 'with', 'fixed', 'parameters,', 'operating', 'just', 'like', 'the', 'standard', 'one-channel', 'convolution:']), (0.01883583563336432, ['Each', 'of', 'the', 'kernels', 'of', 'the', 'filter', '“slides”', 'over', 'their', 'respective', 'input', 'channels,', 'producing', 'a', 'processed', 'version', 'of', 'each.', 'Some', 'kernels', 'may', 'have', 'stronger', 'weights', 'than', 'others,', 'to', 'give', 'more', 'emphasis', 'to', 'certain', 'input', 'channels', 'than', 'others', '(eg.', 'a', 'filter', 'may', 'have', 'a', 'red', 'kernel', 'channel', 'with', 'stronger', 'weights', 'than', 'others,', 'and', 'hence,', 'respond', 'more', 'to', 'differences', 'in', 'the', 'red', 'channel', 'features', 'than', 'the', 'others).']), (0.01880111792976439, ['Even', 'with', 'the', 'mechanics', 'of', 'the', 'convolution', 'layer', 'down,', 'it', 'can', 'still', 'be', 'hard', 'to', 'relate', 'it', 'back', 'to', 'a', 'standard', 'feed-forward', 'network,', 'and', 'it', 'still', 'doesn’t', 'explain', 'why', 'convolutions', 'scale', 'to,', 'and', 'work', 'so', 'much', 'better', 'for', 'image', 'data.']), (0.018270544560795493, ['The', 'repeated', 'reduction', 'in', 'image', 'size', 'across', 'the', 'network', 'results', 'in,', 'by', 'the', '5th', 'block', 'on', 'convolutions,', 'input', 'sizes', 'of', 'just', '7×7,', 'compared', 'to', 'inputs', 'of', '224×224.', 'At', 'this', 'point,', 'each', 'single', 'pixel', 'represents', 'a', 'grid', 'of', '32×32', 'pixels,', 'which', 'is', 'huge.']), (0.018257944001141137, ['Of', 'course,', 'the', 'diagrams', 'above', 'only', 'deals', 'with', 'the', 'case', 'where', 'the', 'image', 'has', 'a', 'single', 'input', 'channel.', 'In', 'practicality,', 'most', 'input', 'images', 'have', '3', 'channels,', 'and', 'that', 'number', 'only', 'increases', 'the', 'deeper', 'you', 'go', 'into', 'a', 'network.', 'It’s', 'pretty', 'easy', 'to', 'think', 'of', 'channels,', 'in', 'general,', 'as', 'being', 'a', '“view”', 'of', 'the', 'image', 'as', 'a', 'whole,', 'emphasising', 'some', 'aspects,', 'de-emphasising', 'others.']), (0.017230385359152826, ['By', 'detecting', 'low', 'level', 'features,', 'and', 'using', 'them', 'to', 'detect', 'higher', 'level', 'features', 'as', 'it', 'progresses', 'up', 'its', 'visual', 'hierarchy,', 'it', 'is', 'eventually', 'able', 'to', 'detect', 'entire', 'visual', 'concepts', 'such', 'as', 'faces,', 'birds,', 'trees,', 'etc,', 'and', 'that’s', 'what', 'makes', 'them', 'such', 'powerful,', 'yet', 'efficient', 'with', 'image', 'data.']), (0.0172171438115337, ['The', 'advent', 'of', 'powerful', 'and', 'versatile', 'deep', 'learning', 'frameworks', 'in', 'recent', 'years', 'has', 'made', 'it', 'possible', 'to', 'implement', 'convolution', 'layers', 'into', 'a', 'deep', 'learning', 'model', 'an', 'extremely', 'simple', 'task,', 'often', 'achievable', 'in', 'a', 'single', 'line', 'of', 'code.']), (0.016805591272968868, ['Padding', 'does', 'something', 'pretty', 'clever', 'to', 'solve', 'this:', 'pad', 'the', 'edges', 'with', 'extra,', '“fake”', 'pixels', '(usually', 'of', 'value', '0,', 'hence', 'the', 'oft-used', 'term', '“zero', 'padding”).', 'This', 'way,', 'the', 'kernel', 'when', 'sliding', 'can', 'allow', 'the', 'original', 'edge', 'pixels', 'to', 'be', 'at', 'its', 'center,', 'while', 'extending', 'into', 'the', 'fake', 'pixels', 'beyond', 'the', 'edge,', 'producing', 'an', 'output', 'the', 'same', 'size', 'as', 'the', 'input.']), (0.01660506816357466, ['Followed', 'by', 'a', 'pooling/strided', 'layer,', 'the', 'network', 'continues', 'to', 'create', 'detectors', 'for', 'even', 'higher', 'level', 'features', '(parts,', 'patterns),', 'as', 'we', 'see', 'for', 'mixed4a.']), (0.016302862616151475, ['Yet,', 'however', 'deep', 'our', 'feature', 'detectors', 'get,', 'without', 'any', 'further', 'changes', 'they’ll', 'still', 'be', 'operating', 'on', 'very', 'small', 'patches', 'of', 'the', 'image.', 'No', 'matter', 'how', 'deep', 'your', 'detectors', 'are,', 'you', 'can’t', 'detect', 'faces', 'from', 'a', '3×3', 'grid.', 'And', 'this', 'is', 'where', 'the', 'idea', 'of', 'the', 'receptive', 'field', 'comes', 'in.']), (0.015903489199383485, ['However,', 'understanding', 'convolutions,', 'especially', 'for', 'the', 'first', 'time', 'can', 'often', 'feel', 'a', 'bit', 'unnerving,', 'with', 'terms', 'like', 'kernels,', 'filters,', 'channels', 'and', 'so', 'on', 'all', 'stacked', 'onto', 'each', 'other.', 'Yet,', 'convolutions', 'as', 'a', 'concept', 'are', 'fascinatingly', 'powerful', 'and', 'highly', 'extensible,', 'and', 'in', 'this', 'post,', 'we’ll', 'break', 'down', 'the', 'mechanics', 'of', 'the', 'convolution', 'operation,', 'step-by-step,', 'relate', 'it', 'to', 'the', 'standard', 'fully', 'connected', 'network,', 'and', 'explore', 'just', 'how', 'they', 'build', 'up', 'a', 'strong', 'visual', 'hierarchy,', 'making', 'them', 'powerful', 'feature', 'extractors', 'for', 'images.']), (0.015652617787807462, ['Suppose', 'we', 'have', 'a', '4×4', 'input,', 'and', 'we', 'want', 'to', 'transform', 'it', 'into', 'a', '2×2', 'grid.', 'If', 'we', 'were', 'using', 'a', 'feedforward', 'network,', 'we’d', 'reshape', 'the', '4×4', 'input', 'into', 'a', 'vector', 'of', 'length', '16,', 'and', 'pass', 'it', 'through', 'a', 'densely', 'connected', 'layer', 'with', '16', 'inputs', 'and', '4', 'outputs.', 'One', 'could', 'visualize', 'the', 'weight', 'matrix', 'W', 'for', 'a', 'layer:']), (0.014881097142744536, ['Here,', 'you', 'don’t', 'need', 'to', 'optimize', 'all', '64', 'parameters,', 'because', 'we', 'set', 'most', 'of', 'them', 'to', 'zero', '(and', 'they’ll', 'stay', 'that', 'way),', 'and', 'the', 'rest', 'we', 'convert', 'to', 'shared', 'parameters,', 'resulting', 'in', 'only', '9', 'actual', 'parameters', 'to', 'optimize.', 'This', 'efficiency', 'matters,', 'because', 'when', 'you', 'move', 'from', 'the', '784', 'inputs', 'of', 'MNIST', 'to', 'real', 'world', '224×224×3', 'images,', 'thats', 'over', '150,000', 'inputs.', 'A', 'dense', 'layer', 'attempting', 'to', 'halve', 'the', 'input', 'to', '75,000', 'inputs', 'would', 'still', 'require', 'over', '10', 'billion', 'parameters.', 'For', 'comparison,', 'the', 'entirety', 'of', 'ResNet-50', 'has', 'some', '25', 'million', 'parameters.']), (0.013292828122321281, ['(Note:', 'if', 'you’re', 'familiar', 'with', 'dilated', 'convolutions,', 'note', 'that', 'the', 'above', 'is', 'not', 'a', 'dilated', 'convolution.', 'Both', 'are', 'methods', 'of', 'increasing', 'the', 'receptive', 'field,', 'but', 'dilated', 'convolutions', 'are', 'a', 'single', 'layer,', 'while', 'this', 'takes', 'place', 'on', 'a', 'regular', 'convolution', 'following', 'a', 'strided', 'convolution,', 'with', 'a', 'nonlinearity', 'inbetween)']), (0.013236177611422352, ['Compared', 'to', 'earlier', 'layers,', 'where', 'an', 'activation', 'meant', 'detecting', 'an', 'edge,', 'here,', 'an', 'activation', 'on', 'the', 'tiny', '7×7', 'grid', 'is', 'one', 'for', 'a', 'very', 'high', 'level', 'feature,', 'such', 'as', 'for', 'birds.']), (0.013066335680025841, ['In', 'that', 'sense,', 'there’s', 'a', 'direct', 'intuition', 'between', 'why', 'both', 'are', 'so', 'efficient', '(compared', 'to', 'their', 'alternatives).', 'Transfer', 'learning', 'is', 'efficient', 'by', 'orders', 'of', 'magnitude', 'compared', 'to', 'random', 'initialization,', 'because', 'you', 'only', 'really', 'need', 'to', 'optimize', 'the', 'parameters', 'of', 'the', 'final', 'fully', 'connected', 'layer,', 'which', 'means', 'you', 'can', 'have', 'fantastic', 'performance', 'with', 'only', 'a', 'few', 'dozen', 'images', 'per', 'class.']), (0.012720248266225608, ['If', 'this', 'were', 'any', 'other', 'kind', 'of', 'data,', 'eg.', 'categorical', 'data', 'of', 'app', 'installs,', 'this', 'would’ve', 'been', 'a', 'disaster,', 'for', 'just', 'because', 'your', 'number', 'of', 'app', 'installs', 'and', 'app', 'type', 'columns', 'are', 'next', 'to', 'each', 'other', 'doesn’t', 'mean', 'they', 'have', 'any', '“local,', 'shared', 'features”', 'common', 'with', 'app', 'install', 'dates', 'and', 'time', 'used.', 'Sure,', 'the', 'four', 'may', 'have', 'an', 'underlying', 'higher', 'level', 'feature', '(eg.', 'which', 'apps', 'people', 'want', 'most)', 'that', 'can', 'be', 'found,', 'but', 'that', 'gives', 'us', 'no', 'reason', 'to', 'believe', 'the', 'parameters', 'for', 'the', 'first', 'two', 'are', 'exactly', 'the', 'same', 'as', 'the', 'parameters', 'for', 'the', 'latter', 'two.', 'The', 'four', 'could’ve', 'been', 'in', 'any', '(consistent)', 'order', 'and', 'still', 'be', 'valid!']), (0.012682742751449415, ['More', 'modern', 'networks,', 'such', 'as', 'the', 'ResNet', 'architectures', 'entirely', 'forgo', 'pooling', 'layers', 'in', 'their', 'internal', 'layers,', 'in', 'favor', 'of', 'strided', 'convolutions', 'when', 'needing', 'to', 'reduce', 'their', 'output', 'sizes.']), (0.012265958918325319, ['Pixels', 'however,', 'always', 'appear', 'in', 'a', 'consistent', 'order,', 'and', 'nearby', 'pixels', 'influence', 'a', 'pixel', 'e.g.', 'if', 'all', 'nearby', 'pixels', 'are', 'red,', 'it’s', 'pretty', 'likely', 'the', 'pixel', 'is', 'also', 'red.', 'If', 'there', 'are', 'deviations,', 'that’s', 'an', 'interesting', 'anomaly', 'that', 'could', 'be', 'converted', 'into', 'a', 'feature,', 'and', 'all', 'this', 'can', 'be', 'detected', 'from', 'comparing', 'a', 'pixel', 'with', 'its', 'neighbors,', 'with', 'other', 'pixels', 'in', 'its', 'locality.']), (0.011670375026426347, ['So', 'fixing', 'some', 'parameters', 'to', '0,', 'and', 'tying', 'parameters', 'increases', 'efficiency,', 'but', 'unlike', 'the', 'transfer', 'learning', 'case,', 'where', 'we', 'know', 'the', 'prior', 'is', 'good', 'because', 'it', 'works', 'on', 'a', 'large', 'general', 'set', 'of', 'images,', 'how', 'do', 'we', 'know', 'this', 'is', 'any', 'good?']), (0.011296640361917841, ['The', 'answer', 'lies', 'in', 'the', 'feature', 'combinations', 'the', 'prior', 'leads', 'the', 'parameters', 'to', 'learn.']), (0.01129477283572134, ['The', 'idea', 'of', 'a', 'strided', 'convolution', 'is', 'that', 'we', 'only', 'process', 'slides', 'a', 'fixed', 'distance', 'apart,', 'and', 'skip', 'the', 'ones', 'in', 'the', 'middle.', 'From', 'a', 'different', 'point', 'of', 'view,', 'we', 'only', 'keep', 'outputs', 'a', 'fixed', 'distance', 'apart,', 'and', 'remove', 'the', 'rest[1].']), (0.011113433628732641, ['The', 'idea', 'of', 'the', 'stride', 'is', 'to', 'skip', 'some', 'of', 'the', 'slide', 'locations', 'of', 'the', 'kernel.', 'A', 'stride', 'of', '1', 'means', 'to', 'pick', 'slides', 'a', 'pixel', 'apart,', 'so', 'basically', 'every', 'single', 'slide,', 'acting', 'as', 'a', 'standard', 'convolution.', 'A', 'stride', 'of', '2', 'means', 'picking', 'slides', '2', 'pixels', 'apart,', 'skipping', 'every', 'other', 'slide', 'in', 'the', 'process,', 'downsizing', 'by', 'roughly', 'a', 'factor', 'of', '2,', 'a', 'stride', 'of', '3', 'means', 'skipping', 'every', '2', 'slides,', 'downsizing', 'roughly', 'by', 'factor', '3,', 'and', 'so', 'on.']), (0.010605416375309129, ['CNNs', 'were', 'the', 'models', 'that', 'allowed', 'computer', 'vision', 'to', 'scale', 'from', 'simple', 'applications', 'to', 'powering', 'sophisticated', 'products', 'and', 'services,', 'ranging', 'from', 'face', 'detection', 'in', 'your', 'photo', 'gallery', 'to', 'making', 'better', 'medical', 'diagnoses.', 'They', 'might', 'be', 'the', 'key', 'method', 'in', 'computer', 'vision', 'going', 'forward,', 'or', 'some', 'other', 'new', 'breakthrough', 'might', 'just', 'be', 'around', 'the', 'corner.', 'Regardless,', 'one', 'thing', 'is', 'for', 'sure:', 'they’re', 'nothing', 'short', 'of', 'amazing,', 'at', 'the', 'heart', 'of', 'many', 'present-day', 'innovative', 'applications,', 'and', 'are', 'most', 'certainly', 'worth', 'deeply', 'understanding.']), (0.010597562242294996, ['Before', 'we', 'move', 'on,', 'it’s', 'definitely', 'worth', 'looking', 'into', 'two', 'techniques', 'that', 'are', 'commonplace', 'in', 'convolution', 'layers:', 'Padding', 'and', 'Strides.']), (0.009903808294452678, ['(Note:', 'while', 'the', 'above', 'matrix', 'is', 'an', 'equivalent', 'transformation', 'matrix,', 'the', 'actual', 'operation', 'is', 'usually', 'implemented', 'as', 'a', 'very', 'different', 'matrix', 'multiplication[2])']), (0.008929279049252115, ['Adversarial', 'examples', 'would', 'be', 'a', 'non-issue', 'if', 'the', 'only', 'tampered', 'ones', 'that', 'caused', 'the', 'models', 'to', 'fail', 'were', 'ones', 'that', 'even', 'humans', 'would', 'notice.', 'The', 'problem', 'is,', 'the', 'models', 'are', 'susceptible', 'to', 'attacks', 'by', 'samples', 'which', 'have', 'only', 'been', 'tampered', 'with', 'ever', 'so', 'slightly,', 'and', 'would', 'clearly', 'not', 'fool', 'any', 'human.', 'This', 'opens', 'the', 'door', 'for', 'models', 'to', 'silently', 'fail,', 'which', 'can', 'be', 'pretty', 'dangerous', 'for', 'a', 'wide', 'range', 'of', 'applications', 'from', 'self-driving', 'cars', 'to', 'healthcare.']), (0.008724828992750261, ['With', 'the', 'visual', 'hierarchy', 'CNNs', 'build,', 'it', 'is', 'pretty', 'reasonable', 'to', 'assume', 'that', 'their', 'vision', 'systems', 'are', 'similar', 'to', 'humans.', 'And', 'they’re', 'really', 'great', 'with', 'real', 'world', 'images,', 'but', 'they', 'also', 'fail', 'in', 'ways', 'that', 'strongly', 'suggest', 'their', 'vision', 'systems', 'aren’t', 'entirely', 'human-like.', 'The', 'most', 'major', 'problem:', 'Adversarial', 'Examples[4],', 'examples', 'which', 'have', 'been', 'specifically', 'modified', 'to', 'fool', 'the', 'model.']), (0.00847795196444077, ['Hope', 'you', 'enjoyed', 'this', 'article!', 'If', 'you’d', 'like', 'to', 'stay', 'connected,', 'you’ll', 'find', 'me', 'on', 'Twitter', 'here.', 'If', 'you', 'have', 'a', 'question,', 'comments', 'are', 'welcome!', '—', 'I', 'find', 'them', 'to', 'be', 'useful', 'to', 'my', 'own', 'learning', 'process', 'as', 'well.']), (0.006708163479316099, ['Robustness', 'against', 'adversarial', 'attacks', 'is', 'currently', 'a', 'highly', 'active', 'area', 'of', 'research,', 'the', 'subject', 'of', 'many', 'papers', 'and', 'even', 'competitions,', 'and', 'solutions', 'will', 'certainly', 'improve', 'CNN', 'architectures', 'to', 'become', 'safer', 'and', 'more', 'reliable.']), (0.005735410232602085, ['Curious', 'programmer,', 'tinkers', 'around', 'in', 'Python', 'and', 'deep', 'learning.']), (0.004048937117669944, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.0033065325198483415, ['Early', 'on', 'in', 'this', 'article,', 'we', 'discussed', 'that:']), (0.002624671916010499, ['Sharing', 'concepts,', 'ideas,', 'and', 'codes.'])]\n",
      "Summarize Text: \n",
      " This is all in pretty stark contrast to a fully connected layer. In the above example, we have 5×5=25 input features, and 3×3=9 output features. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. Do take note of this, as it’ll be critical to our later discussion.. The kernel repeats this process for every location it slides over, converting a 2D matrix of features into yet another 2D matrix of features. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer.. And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. They are then concatenated together to produce the overall output, with the number of output channels being the number of filters. A nonlinearity is then usually applied before passing this as input to another convolution layer, which then repeats this process.. Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.. This is because the output of the strided layer still does represent the same image. It is not so much cropping as it is resizing, only thing is that each single pixel in the output is a “representative” of a larger area (of whose other pixels were discarded) from the same rough location from the original input. So when the next layer’s kernel operates on the output, it’s operating on pixels collected from a larger area.\n",
      "None\n",
      "There is an ongoing debate about whether or not designers should write code. Wherever you fall on this issue, most people would agree that designers should know about code. This helps designers understand constraints and empathize with developers. It also allows designers to think outside of the pixel perfect box when problem solving. For the same reasons, designers should know about machine learning.\n",
      "Put simply, machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Even though Arthur Samuel coined the term over fifty years ago, only recently have we seen the most exciting applications of machine learning — digital assistants, autonomous driving, and spam-free email all exist thanks to machine learning.\n",
      "Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Only in the past few years companies like Google, Amazon, and Apple have made some of their powerful machine learning tools available to developers. Now is the best time to learn about machine learning and apply it to the products you are building.\n",
      "Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Designers should be able to talk with software developers about what is possible, how to prepare, and what outcomes to expect. Below are a few example applications that should serve as inspiration for these conversations.\n",
      "Machine learning can help create user-centric products by personalizing experiences to the individuals who use them. This allows us to improve things like recommendations, search results, notifications, and ads.\n",
      "Machine learning is effective at finding abnormal content. Credit card companies use this to detect fraud, email providers use this to detect spam, and social media companies use this to detect things like hate speech.\n",
      "Machine learning has enabled computers to begin to understand the things we say (natural-language processing) and the things we see (computer vision). This allows Siri to understand “Siri, set a reminder...”, Google Photos to create albums of your dog, and Facebook to describe a photo to those visually impaired.\n",
      "Machine learning is also helpful in understanding how users are grouped. This insight can then be used to look at analytics on a group-by-group basis. From here, different features can be evaluated across groups or be rolled out to only a particular group of users.\n",
      "Machine learning allows us to make predictions about how a user might behave next. Knowing this, we can help prepare for a user’s next action. For example, if we can predict what content a user is planning on viewing, we can preload that content so it’s immediately ready when they want it.\n",
      "Depending on the application and what data is available, there are different types of machine learning algorithms to choose from. I’ll briefly cover each of the following.\n",
      "Supervised learning allows us to make predictions using correctly labeled data. Labeled data is a group of examples that has informative tags or outputs. For example, photos with associated hashtags or a house’s features (eq. number of bedrooms, location) and its price.\n",
      "By using supervised learning we can fit a line to the labelled data that either splits the data into categories or represents the trend of the data. Using this line we are able to make predictions on new data. For example, we can look at new photos and predict hashtags or look at a new house’s features and predict its price.\n",
      "If the output we are trying to predict is a list of tags or values we call it classification. If the output we are trying to predict is a number we call it regression.\n",
      "Unsupervised learning is helpful when we have unlabeled data or we are not exactly sure what outputs (like an image’s hashtags or a house’s price) are meaningful. Instead we can identify patterns among unlabeled data. For example, we can identify related items on an e-commerce website or recommend items to someone based on others who made similar purchases.\n",
      "If the pattern is a group we call it a cluster. If the pattern is a rule (e.q. if this, then that) we call it an association.\n",
      "Reinforcement learning doesn’t use an existing data set. Instead we create an agent to collect its own data through trial-and-error in an environment where it is reinforced with a reward. For example, an agent can learn to play Mario by receiving a positive reward for collecting coins and a negative reward for walking into a Goomba.\n",
      "Reinforcement learning is inspired by the way that humans learn and has turned out to be an effective way to teach computers. Specifically, reinforcement has been effective at training computers to play games like Go and Dota.\n",
      "Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. identifying objects in an image with supervised learning requires a labeled data set of images). However, constraints are the fruit of creativity. In some cases, you can set out to collect data that is not already available or consider other approaches.\n",
      "Even though machine learning is a science, it comes with a margin of error. It is important to consider how a user’s experience might be impacted by this margin of error. For example, when an autonomous car fails to recognize its surroundings people can get hurt.\n",
      "Even though machine learning has never been as accessible as it is today, it still requires additional resources (developers and time) to be integrated into a product. This makes it important to think about whether the resulting impact justifies the amount of resources needed to implement.\n",
      "We have barely covered the tip of the iceberg, but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product. If you are interested in learning more about machine learning, here are some helpful resources:\n",
      "Thanks for reading. Chat with me on Twitter @samueldrozdov\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Digital Product Designer samueldrozdov.com\n",
      "Curated stories on user experience, usability, and product design. By @fabriciot and @caioab.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.0742048791311824, ['Over', 'the', 'past', 'decade', 'new', 'algorithms,', 'better', 'hardware,', 'and', 'more', 'data', 'have', 'made', 'machine', 'learning', 'an', 'order', 'of', 'magnitude', 'more', 'effective.', 'Only', 'in', 'the', 'past', 'few', 'years', 'companies', 'like', 'Google,', 'Amazon,', 'and', 'Apple', 'have', 'made', 'some', 'of', 'their', 'powerful', 'machine', 'learning', 'tools', 'available', 'to', 'developers.', 'Now', 'is', 'the', 'best', 'time', 'to', 'learn', 'about', 'machine', 'learning', 'and', 'apply', 'it', 'to', 'the', 'products', 'you', 'are', 'building.']), (0.06925352774547075, ['Put', 'simply,', 'machine', 'learning', 'is', 'a', '“field', 'of', 'study', 'that', 'gives', 'computers', 'the', 'ability', 'to', 'learn', 'without', 'being', 'explicitly', 'programmed”', '(Arthur', 'Samuel,', '1959).', 'Even', 'though', 'Arthur', 'Samuel', 'coined', 'the', 'term', 'over', 'fifty', 'years', 'ago,', 'only', 'recently', 'have', 'we', 'seen', 'the', 'most', 'exciting', 'applications', 'of', 'machine', 'learning', '—', 'digital', 'assistants,', 'autonomous', 'driving,', 'and', 'spam-free', 'email', 'all', 'exist', 'thanks', 'to', 'machine', 'learning.']), (0.06410433681413348, ['We', 'have', 'barely', 'covered', 'the', 'tip', 'of', 'the', 'iceberg,', 'but', 'hopefully', 'at', 'this', 'point', 'you', 'feel', 'more', 'comfortable', 'thinking', 'about', 'how', 'machine', 'learning', 'can', 'be', 'applied', 'to', 'your', 'product.', 'If', 'you', 'are', 'interested', 'in', 'learning', 'more', 'about', 'machine', 'learning,', 'here', 'are', 'some', 'helpful', 'resources:']), (0.06270703421966853, ['Understanding', 'the', 'problem', 'you', 'are', 'trying', 'to', 'solve', 'and', 'the', 'available', 'data', 'will', 'constrain', 'the', 'types', 'of', 'machine', 'learning', 'you', 'can', 'use', '(e.q.', 'identifying', 'objects', 'in', 'an', 'image', 'with', 'supervised', 'learning', 'requires', 'a', 'labeled', 'data', 'set', 'of', 'images).', 'However,', 'constraints', 'are', 'the', 'fruit', 'of', 'creativity.', 'In', 'some', 'cases,', 'you', 'can', 'set', 'out', 'to', 'collect', 'data', 'that', 'is', 'not', 'already', 'available', 'or', 'consider', 'other', 'approaches.']), (0.06043993517227339, ['Since', 'machine', 'learning', 'is', 'now', 'more', 'accessible', 'than', 'ever', 'before,', 'designers', 'today', 'have', 'the', 'opportunity', 'to', 'think', 'about', 'how', 'machine', 'learning', 'can', 'be', 'applied', 'to', 'improve', 'their', 'products.', 'Designers', 'should', 'be', 'able', 'to', 'talk', 'with', 'software', 'developers', 'about', 'what', 'is', 'possible,', 'how', 'to', 'prepare,', 'and', 'what', 'outcomes', 'to', 'expect.', 'Below', 'are', 'a', 'few', 'example', 'applications', 'that', 'should', 'serve', 'as', 'inspiration', 'for', 'these', 'conversations.']), (0.05231784573524628, ['Machine', 'learning', 'allows', 'us', 'to', 'make', 'predictions', 'about', 'how', 'a', 'user', 'might', 'behave', 'next.', 'Knowing', 'this,', 'we', 'can', 'help', 'prepare', 'for', 'a', 'user’s', 'next', 'action.', 'For', 'example,', 'if', 'we', 'can', 'predict', 'what', 'content', 'a', 'user', 'is', 'planning', 'on', 'viewing,', 'we', 'can', 'preload', 'that', 'content', 'so', 'it’s', 'immediately', 'ready', 'when', 'they', 'want', 'it.']), (0.052135341194517876, ['Machine', 'learning', 'can', 'help', 'create', 'user-centric', 'products', 'by', 'personalizing', 'experiences', 'to', 'the', 'individuals', 'who', 'use', 'them.', 'This', 'allows', 'us', 'to', 'improve', 'things', 'like', 'recommendations,', 'search', 'results,', 'notifications,', 'and', 'ads.']), (0.05117400643884318, ['Depending', 'on', 'the', 'application', 'and', 'what', 'data', 'is', 'available,', 'there', 'are', 'different', 'types', 'of', 'machine', 'learning', 'algorithms', 'to', 'choose', 'from.', 'I’ll', 'briefly', 'cover', 'each', 'of', 'the', 'following.']), (0.05002185697890172, ['Supervised', 'learning', 'allows', 'us', 'to', 'make', 'predictions', 'using', 'correctly', 'labeled', 'data.', 'Labeled', 'data', 'is', 'a', 'group', 'of', 'examples', 'that', 'has', 'informative', 'tags', 'or', 'outputs.', 'For', 'example,', 'photos', 'with', 'associated', 'hashtags', 'or', 'a', 'house’s', 'features', '(eq.', 'number', 'of', 'bedrooms,', 'location)', 'and', 'its', 'price.']), (0.04559753271200611, ['By', 'using', 'supervised', 'learning', 'we', 'can', 'fit', 'a', 'line', 'to', 'the', 'labelled', 'data', 'that', 'either', 'splits', 'the', 'data', 'into', 'categories', 'or', 'represents', 'the', 'trend', 'of', 'the', 'data.', 'Using', 'this', 'line', 'we', 'are', 'able', 'to', 'make', 'predictions', 'on', 'new', 'data.', 'For', 'example,', 'we', 'can', 'look', 'at', 'new', 'photos', 'and', 'predict', 'hashtags', 'or', 'look', 'at', 'a', 'new', 'house’s', 'features', 'and', 'predict', 'its', 'price.']), (0.04399673466314301, ['Machine', 'learning', 'is', 'also', 'helpful', 'in', 'understanding', 'how', 'users', 'are', 'grouped.', 'This', 'insight', 'can', 'then', 'be', 'used', 'to', 'look', 'at', 'analytics', 'on', 'a', 'group-by-group', 'basis.', 'From', 'here,', 'different', 'features', 'can', 'be', 'evaluated', 'across', 'groups', 'or', 'be', 'rolled', 'out', 'to', 'only', 'a', 'particular', 'group', 'of', 'users.']), (0.04135602752802124, ['Even', 'though', 'machine', 'learning', 'is', 'a', 'science,', 'it', 'comes', 'with', 'a', 'margin', 'of', 'error.', 'It', 'is', 'important', 'to', 'consider', 'how', 'a', 'user’s', 'experience', 'might', 'be', 'impacted', 'by', 'this', 'margin', 'of', 'error.', 'For', 'example,', 'when', 'an', 'autonomous', 'car', 'fails', 'to', 'recognize', 'its', 'surroundings', 'people', 'can', 'get', 'hurt.']), (0.04067468716616971, ['Machine', 'learning', 'has', 'enabled', 'computers', 'to', 'begin', 'to', 'understand', 'the', 'things', 'we', 'say', '(natural-language', 'processing)', 'and', 'the', 'things', 'we', 'see', '(computer', 'vision).', 'This', 'allows', 'Siri', 'to', 'understand', '“Siri,', 'set', 'a', 'reminder...”,', 'Google', 'Photos', 'to', 'create', 'albums', 'of', 'your', 'dog,', 'and', 'Facebook', 'to', 'describe', 'a', 'photo', 'to', 'those', 'visually', 'impaired.']), (0.03988067872026044, ['Reinforcement', 'learning', 'doesn’t', 'use', 'an', 'existing', 'data', 'set.', 'Instead', 'we', 'create', 'an', 'agent', 'to', 'collect', 'its', 'own', 'data', 'through', 'trial-and-error', 'in', 'an', 'environment', 'where', 'it', 'is', 'reinforced', 'with', 'a', 'reward.', 'For', 'example,', 'an', 'agent', 'can', 'learn', 'to', 'play', 'Mario', 'by', 'receiving', 'a', 'positive', 'reward', 'for', 'collecting', 'coins', 'and', 'a', 'negative', 'reward', 'for', 'walking', 'into', 'a', 'Goomba.']), (0.03926487016543843, ['Even', 'though', 'machine', 'learning', 'has', 'never', 'been', 'as', 'accessible', 'as', 'it', 'is', 'today,', 'it', 'still', 'requires', 'additional', 'resources', '(developers', 'and', 'time)', 'to', 'be', 'integrated', 'into', 'a', 'product.', 'This', 'makes', 'it', 'important', 'to', 'think', 'about', 'whether', 'the', 'resulting', 'impact', 'justifies', 'the', 'amount', 'of', 'resources', 'needed', 'to', 'implement.']), (0.03925844165103789, ['Machine', 'learning', 'is', 'effective', 'at', 'finding', 'abnormal', 'content.', 'Credit', 'card', 'companies', 'use', 'this', 'to', 'detect', 'fraud,', 'email', 'providers', 'use', 'this', 'to', 'detect', 'spam,', 'and', 'social', 'media', 'companies', 'use', 'this', 'to', 'detect', 'things', 'like', 'hate', 'speech.']), (0.03219445465998106, ['Unsupervised', 'learning', 'is', 'helpful', 'when', 'we', 'have', 'unlabeled', 'data', 'or', 'we', 'are', 'not', 'exactly', 'sure', 'what', 'outputs', '(like', 'an', 'image’s', 'hashtags', 'or', 'a', 'house’s', 'price)', 'are', 'meaningful.', 'Instead', 'we', 'can', 'identify', 'patterns', 'among', 'unlabeled', 'data.', 'For', 'example,', 'we', 'can', 'identify', 'related', 'items', 'on', 'an', 'e-commerce', 'website', 'or', 'recommend', 'items', 'to', 'someone', 'based', 'on', 'others', 'who', 'made', 'similar', 'purchases.']), (0.02878848451564825, ['Reinforcement', 'learning', 'is', 'inspired', 'by', 'the', 'way', 'that', 'humans', 'learn', 'and', 'has', 'turned', 'out', 'to', 'be', 'an', 'effective', 'way', 'to', 'teach', 'computers.', 'Specifically,', 'reinforcement', 'has', 'been', 'effective', 'at', 'training', 'computers', 'to', 'play', 'games', 'like', 'Go', 'and', 'Dota.']), (0.02505789791533082, ['There', 'is', 'an', 'ongoing', 'debate', 'about', 'whether', 'or', 'not', 'designers', 'should', 'write', 'code.', 'Wherever', 'you', 'fall', 'on', 'this', 'issue,', 'most', 'people', 'would', 'agree', 'that', 'designers', 'should', 'know', 'about', 'code.', 'This', 'helps', 'designers', 'understand', 'constraints', 'and', 'empathize', 'with', 'developers.', 'It', 'also', 'allows', 'designers', 'to', 'think', 'outside', 'of', 'the', 'pixel', 'perfect', 'box', 'when', 'problem', 'solving.', 'For', 'the', 'same', 'reasons,', 'designers', 'should', 'know', 'about', 'machine', 'learning.']), (0.02075127739121354, ['If', 'the', 'output', 'we', 'are', 'trying', 'to', 'predict', 'is', 'a', 'list', 'of', 'tags', 'or', 'values', 'we', 'call', 'it', 'classification.', 'If', 'the', 'output', 'we', 'are', 'trying', 'to', 'predict', 'is', 'a', 'number', 'we', 'call', 'it', 'regression.']), (0.01895942888885742, ['Curated', 'stories', 'on', 'user', 'experience,', 'usability,', 'and', 'product', 'design.', 'By', '@fabriciot', 'and', '@caioab.']), (0.017233392083732313, ['Digital', 'Product', 'Designer', 'samueldrozdov.com']), (0.0168965604187866, ['If', 'the', 'pattern', 'is', 'a', 'group', 'we', 'call', 'it', 'a', 'cluster.', 'If', 'the', 'pattern', 'is', 'a', 'rule', '(e.q.', 'if', 'this,', 'then', 'that)', 'we', 'call', 'it', 'an', 'association.']), (0.007519587965911811, ['Thanks', 'for', 'reading.', 'Chat', 'with', 'me', 'on', 'Twitter', '@samueldrozdov']), (0.006211180124223604, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.'])]\n",
      "Summarize Text: \n",
      " Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Only in the past few years companies like Google, Amazon, and Apple have made some of their powerful machine learning tools available to developers. Now is the best time to learn about machine learning and apply it to the products you are building.. Put simply, machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Even though Arthur Samuel coined the term over fifty years ago, only recently have we seen the most exciting applications of machine learning — digital assistants, autonomous driving, and spam-free email all exist thanks to machine learning.. We have barely covered the tip of the iceberg, but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product. If you are interested in learning more about machine learning, here are some helpful resources:. Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. identifying objects in an image with supervised learning requires a labeled data set of images). However, constraints are the fruit of creativity. In some cases, you can set out to collect data that is not already available or consider other approaches.. Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Designers should be able to talk with software developers about what is possible, how to prepare, and what outcomes to expect. Below are a few example applications that should serve as inspiration for these conversations.\n",
      "None\n",
      "Data science interviews certainly aren’t easy. I know this first hand. I’ve participated in over 50 individual interviews and phone screens while applying for competitive internships over the last calendar year. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews.\n",
      "Long story short, I’ve decided to sort through all my bookmarks and notes in order to deliver a comprehensive list of data science resources.\n",
      "With this list by your side, you should have more than enough effective tools at your disposal next time you’re prepping for a big interview.\n",
      "It’s worth noting that many of these resources are naturally going to geared towards entry-level and intern data science positions, as that’s where my expertise lies. Keep that in mind and enjoy!\n",
      "Here’s some of the more general resources covering data science as a whole. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. While the ebook itself is a couple bucks out of pocket, the answers themselves are free on Quora. These were some of my favorite full-coverage questions to practice with right before an interview.\n",
      "Even Data Scientists cannot escape the dreaded algorithmic coding interview. In my experience, this isn’t the case 100% of the time, but chances are you’ll be asked to work through something similar to an easy or medium question on LeetCode or HackerRank.\n",
      "As far as language goes, most companies will let you use whatever language you want. Personally, I did almost all of my algorithmic coding in Java even though the positions were targeted at Python and R programmers. If I had to recommend one thing, it’s to break out your wallet and invest in Cracking the Coding Interview. It absolutely lives up to the hype. I plan to continue using it for years to come.\n",
      "Once the interviewer knows that you can think-through problems and code effectively, chances are that you’ll move onto some more data science specific applications. Depending on the interviewer and the position, you will likely be able to choose between Python and R as your tool of choice. Since I’m partial to Python, my resources below will primarily focus on effectively using Pandas and NumPy for data analysis.\n",
      "A data science interview typically isn’t complete without checking your knowledge of SQL. This can be done over the phone or through a live coding question, more likely the latter. I’ve found that the difficulty level of these questions can vary a good bit, ranging from being painfully easy to requiring complex joins and obscure functions.\n",
      "Our good friend, statistics is still crucial for Data Scientists and it’s reflected as such in interviews. I had many interviews begin by seeing if I can explain a common statistics or probability concept in simple and concise terms. As positions get more experienced, I suspect this happens less and less as traditional statistical questions begin to take the more practical form of A/B testing scenarios, covered later in the post.\n",
      "You’ll notice that I’ve compiled a few more resources here than in other sections. This isn’t a mistake. Machine learning is a complex field that is a virtual guarantee in data science interviews today.\n",
      "The way that you’ll be tested on this is no guarantee however. It may come up as a conceptual question regarding cross validation or bias-variance tradeoff, or it may take the form of a take home assignment with a dataset attached. I’ve seen both several times, so you’ve got to be prepared for anything.\n",
      "Specifically, check out the Machine Learning Flashcards below, they’re only a couple bucks and were my by far my favorite way to quiz myself on any conceptual ML stuff.\n",
      "This won’t be covered in every single data science interview, but it’s certainly not uncommon. Most interviews will have atleast one section solely dedicated to product thinking which often lends itself to A/B testing of some sort. Make sure your familiar with the concepts and statistical background necessary in order to be prepared when it comes up. If you have time to spare, I took the free online course by Udacity and overall, I was pretty impressed.\n",
      "Lastly, I wanted to call out all of the posts related to data science jobs and interviewing that I read over and over again to understand, not only how to prepare, but what to expect as well. If you only check out one section here, this is the one to focus on. This is the layer that sits on top of all the technical skills and application. Don’t overlook it.\n",
      "I hope you find these resources useful during your next interview or job search. I know I did, truthfully I’m just glad that I saved these links somewhere. Lastly, this post is part of an ongoing initiative to ‘open-source’ my experience applying and interviewing at data science positions, so if you enjoyed this content then be sure to follow me for more stuff like this.\n",
      "If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!\n",
      "If you enjoyed this post, feel free to hit the clap button and if you’re interested in posts to come, make sure to follow me on Medium at the link below — I’ll be writing and shipping every day this month as part of a 30-Day Challenge.\n",
      "This article was originally published on conordewey.com\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Data Scientist & Writer | www.conordewey.com\n",
      "Sharing concepts, ideas, and codes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.08372708469849836, ['Here’s', 'some', 'of', 'the', 'more', 'general', 'resources', 'covering', 'data', 'science', 'as', 'a', 'whole.', 'Specifically,', 'I', 'highly', 'recommend', 'checking', 'out', 'the', 'first', 'two', 'links', 'regarding', '120', 'Data', 'Science', 'Interview', 'Questions.', 'While', 'the', 'ebook', 'itself', 'is', 'a', 'couple', 'bucks', 'out', 'of', 'pocket,', 'the', 'answers', 'themselves', 'are', 'free', 'on', 'Quora.', 'These', 'were', 'some', 'of', 'my', 'favorite', 'full-coverage', 'questions', 'to', 'practice', 'with', 'right', 'before', 'an', 'interview.']), (0.07828476125982264, ['You’ll', 'notice', 'that', 'I’ve', 'compiled', 'a', 'few', 'more', 'resources', 'here', 'than', 'in', 'other', 'sections.', 'This', 'isn’t', 'a', 'mistake.', 'Machine', 'learning', 'is', 'a', 'complex', 'field', 'that', 'is', 'a', 'virtual', 'guarantee', 'in', 'data', 'science', 'interviews', 'today.']), (0.07667480730702225, ['If', 'you’re', 'interested', 'in', 'receiving', 'my', 'weekly', 'rundown', 'of', 'interesting', 'articles', 'and', 'resources', 'focused', 'on', 'data', 'science,', 'machine', 'learning,', 'and', 'artificial', 'intelligence,', 'then', 'subscribe', 'to', 'Self', 'Driven', 'Data', 'Science', 'using', 'the', 'form', 'below!']), (0.0749293552373581, ['Data', 'science', 'interviews', 'certainly', 'aren’t', 'easy.', 'I', 'know', 'this', 'first', 'hand.', 'I’ve', 'participated', 'in', 'over', '50', 'individual', 'interviews', 'and', 'phone', 'screens', 'while', 'applying', 'for', 'competitive', 'internships', 'over', 'the', 'last', 'calendar', 'year.', 'Through', 'this', 'exciting', 'and', 'somewhat', '(at', 'times,', 'very)', 'painful', 'process,', 'I’ve', 'accumulated', 'a', 'plethora', 'of', 'useful', 'resources', 'that', 'helped', 'me', 'prepare', 'for', 'and', 'eventually', 'pass', 'data', 'science', 'interviews.']), (0.06231760475073376, ['Once', 'the', 'interviewer', 'knows', 'that', 'you', 'can', 'think-through', 'problems', 'and', 'code', 'effectively,', 'chances', 'are', 'that', 'you’ll', 'move', 'onto', 'some', 'more', 'data', 'science', 'specific', 'applications.', 'Depending', 'on', 'the', 'interviewer', 'and', 'the', 'position,', 'you', 'will', 'likely', 'be', 'able', 'to', 'choose', 'between', 'Python', 'and', 'R', 'as', 'your', 'tool', 'of', 'choice.', 'Since', 'I’m', 'partial', 'to', 'Python,', 'my', 'resources', 'below', 'will', 'primarily', 'focus', 'on', 'effectively', 'using', 'Pandas', 'and', 'NumPy', 'for', 'data', 'analysis.']), (0.061098956103240155, ['I', 'hope', 'you', 'find', 'these', 'resources', 'useful', 'during', 'your', 'next', 'interview', 'or', 'job', 'search.', 'I', 'know', 'I', 'did,', 'truthfully', 'I’m', 'just', 'glad', 'that', 'I', 'saved', 'these', 'links', 'somewhere.', 'Lastly,', 'this', 'post', 'is', 'part', 'of', 'an', 'ongoing', 'initiative', 'to', '‘open-source’', 'my', 'experience', 'applying', 'and', 'interviewing', 'at', 'data', 'science', 'positions,', 'so', 'if', 'you', 'enjoyed', 'this', 'content', 'then', 'be', 'sure', 'to', 'follow', 'me', 'for', 'more', 'stuff', 'like', 'this.']), (0.059312063838305694, ['Long', 'story', 'short,', 'I’ve', 'decided', 'to', 'sort', 'through', 'all', 'my', 'bookmarks', 'and', 'notes', 'in', 'order', 'to', 'deliver', 'a', 'comprehensive', 'list', 'of', 'data', 'science', 'resources.']), (0.05571897381878748, ['It’s', 'worth', 'noting', 'that', 'many', 'of', 'these', 'resources', 'are', 'naturally', 'going', 'to', 'geared', 'towards', 'entry-level', 'and', 'intern', 'data', 'science', 'positions,', 'as', 'that’s', 'where', 'my', 'expertise', 'lies.', 'Keep', 'that', 'in', 'mind', 'and', 'enjoy!']), (0.05494486227880721, ['This', 'won’t', 'be', 'covered', 'in', 'every', 'single', 'data', 'science', 'interview,', 'but', 'it’s', 'certainly', 'not', 'uncommon.', 'Most', 'interviews', 'will', 'have', 'atleast', 'one', 'section', 'solely', 'dedicated', 'to', 'product', 'thinking', 'which', 'often', 'lends', 'itself', 'to', 'A/B', 'testing', 'of', 'some', 'sort.', 'Make', 'sure', 'your', 'familiar', 'with', 'the', 'concepts', 'and', 'statistical', 'background', 'necessary', 'in', 'order', 'to', 'be', 'prepared', 'when', 'it', 'comes', 'up.', 'If', 'you', 'have', 'time', 'to', 'spare,', 'I', 'took', 'the', 'free', 'online', 'course', 'by', 'Udacity', 'and', 'overall,', 'I', 'was', 'pretty', 'impressed.']), (0.054936345433424544, ['A', 'data', 'science', 'interview', 'typically', 'isn’t', 'complete', 'without', 'checking', 'your', 'knowledge', 'of', 'SQL.', 'This', 'can', 'be', 'done', 'over', 'the', 'phone', 'or', 'through', 'a', 'live', 'coding', 'question,', 'more', 'likely', 'the', 'latter.', 'I’ve', 'found', 'that', 'the', 'difficulty', 'level', 'of', 'these', 'questions', 'can', 'vary', 'a', 'good', 'bit,', 'ranging', 'from', 'being', 'painfully', 'easy', 'to', 'requiring', 'complex', 'joins', 'and', 'obscure', 'functions.']), (0.0486823359888561, ['Even', 'Data', 'Scientists', 'cannot', 'escape', 'the', 'dreaded', 'algorithmic', 'coding', 'interview.', 'In', 'my', 'experience,', 'this', 'isn’t', 'the', 'case', '100%', 'of', 'the', 'time,', 'but', 'chances', 'are', 'you’ll', 'be', 'asked', 'to', 'work', 'through', 'something', 'similar', 'to', 'an', 'easy', 'or', 'medium', 'question', 'on', 'LeetCode', 'or', 'HackerRank.']), (0.046447597774214096, ['Lastly,', 'I', 'wanted', 'to', 'call', 'out', 'all', 'of', 'the', 'posts', 'related', 'to', 'data', 'science', 'jobs', 'and', 'interviewing', 'that', 'I', 'read', 'over', 'and', 'over', 'again', 'to', 'understand,', 'not', 'only', 'how', 'to', 'prepare,', 'but', 'what', 'to', 'expect', 'as', 'well.', 'If', 'you', 'only', 'check', 'out', 'one', 'section', 'here,', 'this', 'is', 'the', 'one', 'to', 'focus', 'on.', 'This', 'is', 'the', 'layer', 'that', 'sits', 'on', 'top', 'of', 'all', 'the', 'technical', 'skills', 'and', 'application.', 'Don’t', 'overlook', 'it.']), (0.044560062186281384, ['Data', 'Scientist', '&', 'Writer', '|', 'www.conordewey.com']), (0.038654740285165974, ['Our', 'good', 'friend,', 'statistics', 'is', 'still', 'crucial', 'for', 'Data', 'Scientists', 'and', 'it’s', 'reflected', 'as', 'such', 'in', 'interviews.', 'I', 'had', 'many', 'interviews', 'begin', 'by', 'seeing', 'if', 'I', 'can', 'explain', 'a', 'common', 'statistics', 'or', 'probability', 'concept', 'in', 'simple', 'and', 'concise', 'terms.', 'As', 'positions', 'get', 'more', 'experienced,', 'I', 'suspect', 'this', 'happens', 'less', 'and', 'less', 'as', 'traditional', 'statistical', 'questions', 'begin', 'to', 'take', 'the', 'more', 'practical', 'form', 'of', 'A/B', 'testing', 'scenarios,', 'covered', 'later', 'in', 'the', 'post.']), (0.03200151322002511, ['If', 'you', 'enjoyed', 'this', 'post,', 'feel', 'free', 'to', 'hit', 'the', 'clap', 'button', 'and', 'if', 'you’re', 'interested', 'in', 'posts', 'to', 'come,', 'make', 'sure', 'to', 'follow', 'me', 'on', 'Medium', 'at', 'the', 'link', 'below', '—', 'I’ll', 'be', 'writing', 'and', 'shipping', 'every', 'day', 'this', 'month', 'as', 'part', 'of', 'a', '30-Day', 'Challenge.']), (0.027147998549426487, ['As', 'far', 'as', 'language', 'goes,', 'most', 'companies', 'will', 'let', 'you', 'use', 'whatever', 'language', 'you', 'want.', 'Personally,', 'I', 'did', 'almost', 'all', 'of', 'my', 'algorithmic', 'coding', 'in', 'Java', 'even', 'though', 'the', 'positions', 'were', 'targeted', 'at', 'Python', 'and', 'R', 'programmers.', 'If', 'I', 'had', 'to', 'recommend', 'one', 'thing,', 'it’s', 'to', 'break', 'out', 'your', 'wallet', 'and', 'invest', 'in', 'Cracking', 'the', 'Coding', 'Interview.', 'It', 'absolutely', 'lives', 'up', 'to', 'the', 'hype.', 'I', 'plan', 'to', 'continue', 'using', 'it', 'for', 'years', 'to', 'come.']), (0.0264793444462887, ['The', 'way', 'that', 'you’ll', 'be', 'tested', 'on', 'this', 'is', 'no', 'guarantee', 'however.', 'It', 'may', 'come', 'up', 'as', 'a', 'conceptual', 'question', 'regarding', 'cross', 'validation', 'or', 'bias-variance', 'tradeoff,', 'or', 'it', 'may', 'take', 'the', 'form', 'of', 'a', 'take', 'home', 'assignment', 'with', 'a', 'dataset', 'attached.', 'I’ve', 'seen', 'both', 'several', 'times,', 'so', 'you’ve', 'got', 'to', 'be', 'prepared', 'for', 'anything.']), (0.02295099297047354, ['Specifically,', 'check', 'out', 'the', 'Machine', 'Learning', 'Flashcards', 'below,', 'they’re', 'only', 'a', 'couple', 'bucks', 'and', 'were', 'my', 'by', 'far', 'my', 'favorite', 'way', 'to', 'quiz', 'myself', 'on', 'any', 'conceptual', 'ML', 'stuff.']), (0.02149104125252221, ['With', 'this', 'list', 'by', 'your', 'side,', 'you', 'should', 'have', 'more', 'than', 'enough', 'effective', 'tools', 'at', 'your', 'disposal', 'next', 'time', 'you’re', 'prepping', 'for', 'a', 'big', 'interview.']), (0.014861233477015713, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.007389162561865286, ['This', 'article', 'was', 'originally', 'published', 'on', 'conordewey.com']), (0.007389162561865286, ['Sharing', 'concepts,', 'ideas,', 'and', 'codes.'])]\n",
      "Summarize Text: \n",
      " Here’s some of the more general resources covering data science as a whole. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. While the ebook itself is a couple bucks out of pocket, the answers themselves are free on Quora. These were some of my favorite full-coverage questions to practice with right before an interview.. You’ll notice that I’ve compiled a few more resources here than in other sections. This isn’t a mistake. Machine learning is a complex field that is a virtual guarantee in data science interviews today.. If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!. Data science interviews certainly aren’t easy. I know this first hand. I’ve participated in over 50 individual interviews and phone screens while applying for competitive internships over the last calendar year. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews.. Once the interviewer knows that you can think-through problems and code effectively, chances are that you’ll move onto some more data science specific applications. Depending on the interviewer and the position, you will likely be able to choose between Python and R as your tool of choice. Since I’m partial to Python, my resources below will primarily focus on effectively using Pandas and NumPy for data analysis.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "generate_summary(new_df['text'][40],5)\n",
    "#generate_summary((new_df['text'][i],10) for i in recommend(\"python_for_data_science:_8_concepts_you_may_have_forgotten\n",
    "#\"))\n",
    "#generate_summary(recommend(\"python_for_data_science:_8_concepts_you_may_have_forgotten\"))\n",
    "for i in (recommend(\"python_for_data_science:_8_concepts_you_may_have_forgotten\")) :\n",
    "    print(generate_summary(new_df.iloc[i[0]].text,5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c29a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
