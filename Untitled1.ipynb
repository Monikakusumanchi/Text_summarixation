{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90659d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff17a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44653e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.700297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.482855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reading_time\n",
       "count    337.000000\n",
       "mean       9.700297\n",
       "std        5.482855\n",
       "min        2.000000\n",
       "25%        6.000000\n",
       "50%        8.000000\n",
       "75%       13.000000\n",
       "max       31.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44adfd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7d4aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots were the next big thing: what happened? – The Startup – Medium'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d975a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'claps', 'reading_time', 'link', 'title', 'text'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1facac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author          0\n",
       "claps           0\n",
       "reading_time    0\n",
       "link            0\n",
       "title           0\n",
       "text            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76108b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['title'] = df['title'].apply(lambda x: ''.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbc8a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots were the next big thing: what happened? – The Startup – Medium'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62761b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda x:[i.replace(\" \",\"_\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e491cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].apply(lambda x:[i.replace(\" \",\"_\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0553b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df['title'] + df['author'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ce2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df.tags.apply(lambda x:''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d0f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df.title.apply(lambda x:''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5dac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots_were_the_next_big_thing:_what_happened?_–_The_Startup_–_MediumJustin_Lee'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "    v = ' '.join([ps.stem(i) for i in text.split()])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f44645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbots_were_the_next_big_thing:_what_happened?_–_The_Startup_–_MediumJustin_Lee'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['title'] = df['title'].apply(lambda x: ','.join(map(str, x)))\n",
    "df.iloc[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae0330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-77b86c153cd3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags'] = new_df.tags.apply(lambda x:x.lower())\n",
      "<ipython-input-18-77b86c153cd3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['title'] = new_df.title.apply(lambda x:x.lower())\n"
     ]
    }
   ],
   "source": [
    "new_df = df[['author','title','tags','text','link']]\n",
    "new_df['tags'] = new_df.tags.apply(lambda x:x.lower())\n",
    "new_df['title'] = new_df.title.apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad860a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['title'][0]\n",
    "#new_df['title'] = np.asarray(new_df['title'])\n",
    "#new_df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ff7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      chatbots_were_the_next_big_thing:_what_happene...\n",
       "1      python_for_data_science:_8_concepts_you_may_ha...\n",
       "2      automated_feature_engineering_in_python_–_towa...\n",
       "3      machine_learning:_how_to_go_from_zero_to_hero_...\n",
       "4      reinforcement_learning_from_scratch_–_insight_...\n",
       "                             ...                        \n",
       "332    you_can_build_a_neural_network_in_javascript_e...\n",
       "333    artificial_intelligence,_ai_in_2018_and_beyond...\n",
       "334    spiking_neural_networks,_the_next_generation_o...\n",
       "335    surprise!_neurons_are_now_more_complex_than_we...\n",
       "336    “wth_does_a_neural_network_even_learn??”_—_a_n...\n",
       "Name: tags, Length: 337, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502997e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(max_features = 5000, stop_words = 'english')\n",
    "vectors = cvect.fit_transform(new_df['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac0a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45a4fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend(article):\n",
    "    article = article.lower()\n",
    "    article_index = new_df[new_df.title == article].index[0]\n",
    "    distances = similarity[article_index]\n",
    "    article_list = sorted(list(enumerate(distances)), reverse = True, key = lambda x:x[1])[1:8]\n",
    "    for i in article_list:\n",
    "        print(new_df.iloc[i[0]].title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b18929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'building_a_smarter_home_feed_–_pinterest_engineering_–_medium'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['title'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6abeef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_the_interests_platform_–_pinterest_engineering_–_medium\n",
      "chatbots_were_the_next_big_thing:_what_happened?_–_the_startup_–_medium\n",
      "python_for_data_science:_8_concepts_you_may_have_forgotten\n",
      "automated_feature_engineering_in_python_–_towards_data_science\n",
      "machine_learning:_how_to_go_from_zero_to_hero_–_freecodecamp\n",
      "reinforcement_learning_from_scratch_–_insight_data\n",
      "intuitively_understanding_convolutions_for_deep_learning\n"
     ]
    }
   ],
   "source": [
    "recommend('building_a_smarter_home_feed_–_pinterest_engineering_–_medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59d2d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    #file = open(file_name, \"r\")\n",
    "    #filedata = new_df['text']\n",
    "    article = file_name.split(\"\\n\")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26acb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Pinchak | Pinterest engineer, Discovery\n",
      "The home feed should be a reflection of what each user cares about. Content is sourced from inputs such as people and boards the user follows, interests, and recommendations. To ensure we maintain fast, reliable and personalized home feeds, we built the smart feed with the following design values in mind:\n",
      "1. Different sources of Pins should be mixed together at different rates.\n",
      "2. Some Pins should be selectively dropped or deferred until a later time. Some sources may produce Pins of poor quality for a user, so instead of showing everything available immediately, we can be selective about what to show and what to hold back for a future session.\n",
      "3. Pins should be arranged in the order of best-first rather than newest-first. For some sources, newer Pins are intuitively better, while for others, newness is less important.\n",
      "We shifted away from our previously time-ordered home feed system and onto a more flexible one. The core feature of the smart feed architecture is its separation of available, but unseen, content and content that’s already been presented to the user. We leverage knowledge of what the user hasn’t yet seen to our advantage when deciding how the feed evolves over time.\n",
      "Smart feed is a composition of three independent services, each of which has a specific role in the construction of a home feed.\n",
      "The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.\n",
      "Essentially, the worker manages Pins as they become newly available, such as those from the repins of the people the user follows. Pins have varying value to the receiving user, so the worker is tasked with deciding the magnitude of their subjective quality.\n",
      "Incoming Pins are currently obtained from three separate sources: repins made by followed users, related Pins, and Pins from followed interests. Each is scored by the worker and then inserted into a pool for that particular type of pin. Each pool is a priority queue sorted on score and belongs to a single user. Newly added Pins mix with those added before, allowing the highest quality Pins to be accessible over time at the front of the queue.\n",
      "Pools can be implemented in a variety of ways so long as the priority queue requirement is met. We choose to do this by exploiting the key-based sorting of HBase. Each key is a combination of user, score and Pin such that, for any user, we may scan a list of available Pins according to their score. Newly added triples will be inserted at their appropriate location to maintain the score order. This combination of user, score, and Pin into a key value can be used to create a priority queue in other storage systems aside from HBase, a property we may use in the future depending on evolving storage requirements.\n",
      "Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.\n",
      "The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.\n",
      "The content generator is generally free to perform any rearrangements it likes, but is bound to the priority queue nature of the pools. When the generator asks for n pins from a pool, it’ll get the n highest scoring (i.e., best) Pins available. Therefore, the generator doesn’t need to concern itself with finding the best available content, but instead with how the best available content should be presented.\n",
      "In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.\n",
      "The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.\n",
      "The smart feed service relies on the content generator to provide new Pins. If the generator experiences a degradation in performance, the service can gracefully handle the loss of its availability. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. In this instance, the feed will appear to the end user as unchanged from last time. Future feed views will produce chunks as large as, or larger than, the last so that eventually the user will see new Pins.\n",
      "By moving to smart feed, we achieved the goals of a highly flexible architecture and better control over the composition of home feeds. The home feed is now powered by three separate services, each with a well-defined role in its production and distribution. The individual services can be altered or replaced with components that serve the same general purpose. The use of pools to buffer Pins according to their quality allows us a greater amount of control over the composition of home feeds.\n",
      "Continuing with this project, we intend to better model users’ preferences with respect to Pins in their home feeds. Our accuracy of recommendation quality varies considerably over our user base, and we would benefit from using preference information gathered from recent interactions with the home feed. Knowledge of personal preference will also help us order home feeds so the Pins of most value can be discovered with the least amount of effort.\n",
      "If you’re interested in tackling challenges and making improvements like this, join our team!\n",
      "Chris Pinchak is a software engineer at Pinterest.\n",
      "Acknowledgements: This technology was built in collaboration with Dan Feng, Dmitry Chechik, Raghavendra Prabhu, Jeremy Carroll, Xun Liu, Varun Sharma, Joe Lau, Yuchen Liu, Tian-Ying Chang, and Yun Park. This team, as well as people from across the company, helped make this project a reality with their technical insights and invaluable feedback.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering\n",
      "\n",
      "Indexes of top ranked_sentence order are  [(0.07035587589313758, ['Distinct', 'from', 'the', 'smart', 'feed', 'worker,', 'the', 'smart', 'feed', 'content', 'generator', 'is', 'concerned', 'primarily', 'with', 'defining', 'what', '“new”', 'means', 'in', 'the', 'context', 'of', 'a', 'home', 'feed.', 'When', 'a', 'user', 'accesses', 'the', 'home', 'feed,', 'we', 'ask', 'the', 'content', 'generator', 'for', 'new', 'Pins', 'since', 'their', 'last', 'visit.', 'The', 'generator', 'decides', 'the', 'quantity,', 'composition,', 'and', 'arrangement', 'of', 'new', 'Pins', 'to', 'return', 'in', 'response', 'to', 'this', 'request.']), (0.06803332617614993, ['In', 'addition', 'to', 'providing', 'high', 'availability', 'of', 'the', 'home', 'feed,', 'the', 'smart', 'feed', 'service', 'is', 'responsible', 'for', 'combining', 'new', 'Pins', 'returned', 'by', 'the', 'content', 'generator', 'with', 'those', 'that', 'previously', 'appeared', 'in', 'the', 'home', 'feed.', 'We', 'can', 'separate', 'these', 'into', 'the', 'chunk', 'returned', 'by', 'the', 'content', 'generator', 'and', 'the', 'materialized', 'feed', 'managed', 'by', 'the', 'smart', 'feed', 'service.']), (0.06585684921566548, ['The', 'smart', 'feed', 'worker', 'is', 'the', 'first', 'to', 'process', 'Pins', 'and', 'has', 'two', 'primary', 'responsibilities', '—', 'to', 'accept', 'incoming', 'Pins', 'and', 'assign', 'some', 'score', 'proportional', 'to', 'their', 'quality', 'or', 'value', 'to', 'the', 'receiving', 'user,', 'and', 'to', 'remember', 'these', 'scored', 'Pins', 'in', 'some', 'storage', 'for', 'later', 'consumption.']), (0.06047125282541027, ['The', 'content', 'generator', 'assembles', 'available', 'Pins', 'into', 'chunks', 'for', 'consumption', 'by', 'the', 'user', 'as', 'part', 'of', 'their', 'home', 'feed.', 'The', 'generator', 'is', 'free', 'to', 'choose', 'any', 'arrangement', 'based', 'on', 'a', 'variety', 'of', 'input', 'signals,', 'and', 'may', 'elect', 'to', 'use', 'some', 'or', 'all', 'of', 'the', 'Pins', 'available', 'in', 'the', 'pools.', 'Pins', 'that', 'are', 'selected', 'for', 'inclusion', 'in', 'a', 'chunk', 'are', 'thereafter', 'removed', 'from', 'from', 'the', 'pools', 'so', 'they', 'cannot', 'be', 'returned', 'as', 'part', 'of', 'subsequent', 'chunks.']), (0.05584349930996346, ['The', 'materialized', 'feed', 'represents', 'a', 'frozen', 'view', 'of', 'the', 'feed', 'as', 'it', 'was', 'the', 'last', 'time', 'the', 'user', 'viewed', 'it.', 'To', 'the', 'materialized', 'Pins', 'we', 'add', 'the', 'Pins', 'from', 'the', 'content', 'generator', 'in', 'the', 'chunk.', 'The', 'service', 'makes', 'no', 'decisions', 'about', 'order,', 'instead', 'it', 'adds', 'the', 'Pins', 'in', 'exactly', 'the', 'order', 'given', 'by', 'the', 'chunk.', 'Because', 'it', 'has', 'a', 'fairly', 'low', 'rate', 'of', 'reading', 'and', 'writing,', 'the', 'materialized', 'feed', 'is', 'likely', 'to', 'suffer', 'from', 'fewer', 'availability', 'events.', 'In', 'addition,', 'feeds', 'can', 'be', 'trimmed', 'to', 'restrict', 'them', 'to', 'a', 'maximum', 'size.', 'The', 'need', 'for', 'less', 'storage', 'means', 'we', 'can', 'easily', 'increase', 'the', 'availability', 'and', 'reliability', 'of', 'the', 'materialized', 'feed', 'through', 'replication', 'and', 'the', 'use', 'of', 'faster', 'storage', 'hardware.']), (0.04943495385656368, ['The', 'smart', 'feed', 'service', 'relies', 'on', 'the', 'content', 'generator', 'to', 'provide', 'new', 'Pins.', 'If', 'the', 'generator', 'experiences', 'a', 'degradation', 'in', 'performance,', 'the', 'service', 'can', 'gracefully', 'handle', 'the', 'loss', 'of', 'its', 'availability.', 'In', 'the', 'event', 'the', 'content', 'generator', 'encounters', 'an', 'exception', 'while', 'generating', 'a', 'chunk,', 'or', 'if', 'it', 'simply', 'takes', 'too', 'long', 'to', 'produce', 'one,', 'the', 'smart', 'feed', 'service', 'will', 'return', 'the', 'content', 'contained', 'in', 'the', 'materialized', 'feed.', 'In', 'this', 'instance,', 'the', 'feed', 'will', 'appear', 'to', 'the', 'end', 'user', 'as', 'unchanged', 'from', 'last', 'time.', 'Future', 'feed', 'views', 'will', 'produce', 'chunks', 'as', 'large', 'as,', 'or', 'larger', 'than,', 'the', 'last', 'so', 'that', 'eventually', 'the', 'user', 'will', 'see', 'new', 'Pins.']), (0.04929100220823563, ['Chris', 'Pinchak', '|', 'Pinterest', 'engineer,', 'Discovery']), (0.04884016385389834, ['2.', 'Some', 'Pins', 'should', 'be', 'selectively', 'dropped', 'or', 'deferred', 'until', 'a', 'later', 'time.', 'Some', 'sources', 'may', 'produce', 'Pins', 'of', 'poor', 'quality', 'for', 'a', 'user,', 'so', 'instead', 'of', 'showing', 'everything', 'available', 'immediately,', 'we', 'can', 'be', 'selective', 'about', 'what', 'to', 'show', 'and', 'what', 'to', 'hold', 'back', 'for', 'a', 'future', 'session.']), (0.0475486232162542, ['Incoming', 'Pins', 'are', 'currently', 'obtained', 'from', 'three', 'separate', 'sources:', 'repins', 'made', 'by', 'followed', 'users,', 'related', 'Pins,', 'and', 'Pins', 'from', 'followed', 'interests.', 'Each', 'is', 'scored', 'by', 'the', 'worker', 'and', 'then', 'inserted', 'into', 'a', 'pool', 'for', 'that', 'particular', 'type', 'of', 'pin.', 'Each', 'pool', 'is', 'a', 'priority', 'queue', 'sorted', 'on', 'score', 'and', 'belongs', 'to', 'a', 'single', 'user.', 'Newly', 'added', 'Pins', 'mix', 'with', 'those', 'added', 'before,', 'allowing', 'the', 'highest', 'quality', 'Pins', 'to', 'be', 'accessible', 'over', 'time', 'at', 'the', 'front', 'of', 'the', 'queue.']), (0.047333905642470746, ['The', 'home', 'feed', 'should', 'be', 'a', 'reflection', 'of', 'what', 'each', 'user', 'cares', 'about.', 'Content', 'is', 'sourced', 'from', 'inputs', 'such', 'as', 'people', 'and', 'boards', 'the', 'user', 'follows,', 'interests,', 'and', 'recommendations.', 'To', 'ensure', 'we', 'maintain', 'fast,', 'reliable', 'and', 'personalized', 'home', 'feeds,', 'we', 'built', 'the', 'smart', 'feed', 'with', 'the', 'following', 'design', 'values', 'in', 'mind:']), (0.04638295646649186, ['Essentially,', 'the', 'worker', 'manages', 'Pins', 'as', 'they', 'become', 'newly', 'available,', 'such', 'as', 'those', 'from', 'the', 'repins', 'of', 'the', 'people', 'the', 'user', 'follows.', 'Pins', 'have', 'varying', 'value', 'to', 'the', 'receiving', 'user,', 'so', 'the', 'worker', 'is', 'tasked', 'with', 'deciding', 'the', 'magnitude', 'of', 'their', 'subjective', 'quality.']), (0.045744260820342206, ['Continuing', 'with', 'this', 'project,', 'we', 'intend', 'to', 'better', 'model', 'users’', 'preferences', 'with', 'respect', 'to', 'Pins', 'in', 'their', 'home', 'feeds.', 'Our', 'accuracy', 'of', 'recommendation', 'quality', 'varies', 'considerably', 'over', 'our', 'user', 'base,', 'and', 'we', 'would', 'benefit', 'from', 'using', 'preference', 'information', 'gathered', 'from', 'recent', 'interactions', 'with', 'the', 'home', 'feed.', 'Knowledge', 'of', 'personal', 'preference', 'will', 'also', 'help', 'us', 'order', 'home', 'feeds', 'so', 'the', 'Pins', 'of', 'most', 'value', 'can', 'be', 'discovered', 'with', 'the', 'least', 'amount', 'of', 'effort.']), (0.04451904237520356, ['By', 'moving', 'to', 'smart', 'feed,', 'we', 'achieved', 'the', 'goals', 'of', 'a', 'highly', 'flexible', 'architecture', 'and', 'better', 'control', 'over', 'the', 'composition', 'of', 'home', 'feeds.', 'The', 'home', 'feed', 'is', 'now', 'powered', 'by', 'three', 'separate', 'services,', 'each', 'with', 'a', 'well-defined', 'role', 'in', 'its', 'production', 'and', 'distribution.', 'The', 'individual', 'services', 'can', 'be', 'altered', 'or', 'replaced', 'with', 'components', 'that', 'serve', 'the', 'same', 'general', 'purpose.', 'The', 'use', 'of', 'pools', 'to', 'buffer', 'Pins', 'according', 'to', 'their', 'quality', 'allows', 'us', 'a', 'greater', 'amount', 'of', 'control', 'over', 'the', 'composition', 'of', 'home', 'feeds.']), (0.044403984725042873, ['We', 'shifted', 'away', 'from', 'our', 'previously', 'time-ordered', 'home', 'feed', 'system', 'and', 'onto', 'a', 'more', 'flexible', 'one.', 'The', 'core', 'feature', 'of', 'the', 'smart', 'feed', 'architecture', 'is', 'its', 'separation', 'of', 'available,', 'but', 'unseen,', 'content', 'and', 'content', 'that’s', 'already', 'been', 'presented', 'to', 'the', 'user.', 'We', 'leverage', 'knowledge', 'of', 'what', 'the', 'user', 'hasn’t', 'yet', 'seen', 'to', 'our', 'advantage', 'when', 'deciding', 'how', 'the', 'feed', 'evolves', 'over', 'time.']), (0.044129651772061074, ['The', 'content', 'generator', 'is', 'generally', 'free', 'to', 'perform', 'any', 'rearrangements', 'it', 'likes,', 'but', 'is', 'bound', 'to', 'the', 'priority', 'queue', 'nature', 'of', 'the', 'pools.', 'When', 'the', 'generator', 'asks', 'for', 'n', 'pins', 'from', 'a', 'pool,', 'it’ll', 'get', 'the', 'n', 'highest', 'scoring', '(i.e.,', 'best)', 'Pins', 'available.', 'Therefore,', 'the', 'generator', 'doesn’t', 'need', 'to', 'concern', 'itself', 'with', 'finding', 'the', 'best', 'available', 'content,', 'but', 'instead', 'with', 'how', 'the', 'best', 'available', 'content', 'should', 'be', 'presented.']), (0.039712569836571635, ['Smart', 'feed', 'is', 'a', 'composition', 'of', 'three', 'independent', 'services,', 'each', 'of', 'which', 'has', 'a', 'specific', 'role', 'in', 'the', 'construction', 'of', 'a', 'home', 'feed.']), (0.03816094637008598, ['Chris', 'Pinchak', 'is', 'a', 'software', 'engineer', 'at', 'Pinterest.']), (0.03753909409687661, ['3.', 'Pins', 'should', 'be', 'arranged', 'in', 'the', 'order', 'of', 'best-first', 'rather', 'than', 'newest-first.', 'For', 'some', 'sources,', 'newer', 'Pins', 'are', 'intuitively', 'better,', 'while', 'for', 'others,', 'newness', 'is', 'less', 'important.']), (0.028717219526289924, ['Pools', 'can', 'be', 'implemented', 'in', 'a', 'variety', 'of', 'ways', 'so', 'long', 'as', 'the', 'priority', 'queue', 'requirement', 'is', 'met.', 'We', 'choose', 'to', 'do', 'this', 'by', 'exploiting', 'the', 'key-based', 'sorting', 'of', 'HBase.', 'Each', 'key', 'is', 'a', 'combination', 'of', 'user,', 'score', 'and', 'Pin', 'such', 'that,', 'for', 'any', 'user,', 'we', 'may', 'scan', 'a', 'list', 'of', 'available', 'Pins', 'according', 'to', 'their', 'score.', 'Newly', 'added', 'triples', 'will', 'be', 'inserted', 'at', 'their', 'appropriate', 'location', 'to', 'maintain', 'the', 'score', 'order.', 'This', 'combination', 'of', 'user,', 'score,', 'and', 'Pin', 'into', 'a', 'key', 'value', 'can', 'be', 'used', 'to', 'create', 'a', 'priority', 'queue', 'in', 'other', 'storage', 'systems', 'aside', 'from', 'HBase,', 'a', 'property', 'we', 'may', 'use', 'in', 'the', 'future', 'depending', 'on', 'evolving', 'storage', 'requirements.']), (0.02824665173615034, ['1.', 'Different', 'sources', 'of', 'Pins', 'should', 'be', 'mixed', 'together', 'at', 'different', 'rates.']), (0.017503993473322897, ['Inventive', 'engineers', 'building', 'the', 'first', 'visual', 'discovery', 'engine,', '100', 'billion', 'ideas', 'and', 'counting.', 'https://careers.pinterest.com/careers/engineering']), (0.007847593876347553, ['Acknowledgements:', 'This', 'technology', 'was', 'built', 'in', 'collaboration', 'with', 'Dan', 'Feng,', 'Dmitry', 'Chechik,', 'Raghavendra', 'Prabhu,', 'Jeremy', 'Carroll,', 'Xun', 'Liu,', 'Varun', 'Sharma,', 'Joe', 'Lau,', 'Yuchen', 'Liu,', 'Tian-Ying', 'Chang,', 'and', 'Yun', 'Park.', 'This', 'team,', 'as', 'well', 'as', 'people', 'from', 'across', 'the', 'company,', 'helped', 'make', 'this', 'project', 'a', 'reality', 'with', 'their', 'technical', 'insights', 'and', 'invaluable', 'feedback.']), (0.007603101085995281, ['From', 'a', 'quick', 'cheer', 'to', 'a', 'standing', 'ovation,', 'clap', 'to', 'show', 'how', 'much', 'you', 'enjoyed', 'this', 'story.']), (0.0064794816414686825, ['If', 'you’re', 'interested', 'in', 'tackling', 'challenges', 'and', 'making', 'improvements', 'like', 'this,', 'join', 'our', 'team!'])]\n",
      "Summarize Text: \n",
      " Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.. In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.. The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.. The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.. The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.. The smart feed service relies on the content generator to provide new Pins. If the generator experiences a degradation in performance, the service can gracefully handle the loss of its availability. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. In this instance, the feed will appear to the end user as unchanged from last time. Future feed views will produce chunks as large as, or larger than, the last so that eventually the user will see new Pins.. Chris Pinchak | Pinterest engineer, Discovery. 2. Some Pins should be selectively dropped or deferred until a later time. Some sources may produce Pins of poor quality for a user, so instead of showing everything available immediately, we can be selective about what to show and what to hold back for a future session.. Incoming Pins are currently obtained from three separate sources: repins made by followed users, related Pins, and Pins from followed interests. Each is scored by the worker and then inserted into a pool for that particular type of pin. Each pool is a priority queue sorted on score and belongs to a single user. Newly added Pins mix with those added before, allowing the highest quality Pins to be accessible over time at the front of the queue.. The home feed should be a reflection of what each user cares about. Content is sourced from inputs such as people and boards the user follows, interests, and recommendations. To ensure we maintain fast, reliable and personalized home feeds, we built the smart feed with the following design values in mind:\n"
     ]
    }
   ],
   "source": [
    "generate_summary(new_df['text'][40],10)\n",
    "#generate_summary((new_df['text'][i],10) for i in article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c29a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
